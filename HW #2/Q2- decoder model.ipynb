{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2ea7c08013534802882f7b05fb6be82f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9083df27fa042128abafa7d0b441f6c",
              "IPY_MODEL_8ea92696519e4469bff5ca1efcd59b25",
              "IPY_MODEL_1f10be49d42247d5963803a2fd6b4546"
            ],
            "layout": "IPY_MODEL_f5a9e648a0da4e309e5de932aef38884"
          }
        },
        "e9083df27fa042128abafa7d0b441f6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_118c1003fc944a2dbc028e1f0859cceb",
            "placeholder": "​",
            "style": "IPY_MODEL_449a042adde2483d8de429de40fe6684",
            "value": "100%"
          }
        },
        "8ea92696519e4469bff5ca1efcd59b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a22dfc55fdc4406590037c147a9b48e3",
            "max": 244408911,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86ef623447d243118283c1cebbc32e75",
            "value": 244408911
          }
        },
        "1f10be49d42247d5963803a2fd6b4546": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8b8509b530747d1aa103183117436d4",
            "placeholder": "​",
            "style": "IPY_MODEL_b483553b8c8b45379a5348614da5a0af",
            "value": " 233M/233M [00:04&lt;00:00, 54.8MB/s]"
          }
        },
        "f5a9e648a0da4e309e5de932aef38884": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "118c1003fc944a2dbc028e1f0859cceb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "449a042adde2483d8de429de40fe6684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a22dfc55fdc4406590037c147a9b48e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86ef623447d243118283c1cebbc32e75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8b8509b530747d1aa103183117436d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b483553b8c8b45379a5348614da5a0af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6XI2nUHGokQ",
        "outputId": "49ee5d4b-02a3-4b61-bcc8-4a38fe1666f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim #optimization algorithms\n",
        "\n",
        "from torchvision import datasets # contains datasets with almost similar API\n",
        "from torchvision import transforms # contains common image transformations\n",
        "\n",
        "import os\n",
        "import random\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "metadata": {
        "id": "1NkGs_1XGz95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/drive/My Drive/DL/HW2/train'\n",
        "classes = os.walk(PATH).__next__()[1]\n",
        "train_imagepaths= list()\n",
        "test_imagepaths = list()\n",
        "\n",
        "selected_classes = ['n02950826', 'n02963159', 'n04008634', 'n03355925', 'n03980874']\n",
        "selected_train_samples = []\n",
        "selected_test_samples = []\n",
        "\n",
        "for c in classes:\n",
        "  new_path = PATH+\"/\"+c+\"/images/\"\n",
        "  images = os.walk(new_path).__next__()[2]\n",
        "  c_images = []\n",
        "  for img in images:\n",
        "    c_images.append(new_path+ img)\n",
        "  random.shuffle(c_images)\n",
        "  if c in selected_classes:\n",
        "    selected_train_samples.append(c_images[55])\n",
        "    selected_test_samples.append(c_images[5])\n",
        "  train_imagepaths.extend(c_images[50:])\n",
        "  test_imagepaths.extend(c_images[:50])\n",
        "\n",
        "random.shuffle(train_imagepaths)\n",
        "random.shuffle(test_imagepaths)"
      ],
      "metadata": {
        "id": "jT_BDrR6ZjR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_test_samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy93SW67f9eQ",
        "outputId": "804060b8-0192-49a1-ae05-cd703a14e3a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/DL/HW2/train/n02950826/images/n02950826_46.JPEG',\n",
              " '/content/drive/My Drive/DL/HW2/train/n02963159/images/n02963159_201.JPEG',\n",
              " '/content/drive/My Drive/DL/HW2/train/n04008634/images/n04008634_49.JPEG',\n",
              " '/content/drive/My Drive/DL/HW2/train/n03355925/images/n03355925_37.JPEG',\n",
              " '/content/drive/My Drive/DL/HW2/train/n03980874/images/n03980874_178.JPEG']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_imagepaths))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpN5s0_UWqg9",
        "outputId": "bebaac66-bb29-46dc-d07d-801c015f7db7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms.transforms import Grayscale\n",
        "import torch\n",
        "import torchvision as tv\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "transform1 = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224),\n",
        "                      transforms.ToTensor() ])\n",
        "# , transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "\n",
        "def read_image(path):\n",
        "  convert_tensor = transforms.ToTensor()\n",
        "  image = []\n",
        "  img = Image.open(path)\n",
        "  temp = img.copy()\n",
        "  temp = convert_tensor(temp)\n",
        "  if temp.shape[0] == 1:\n",
        "    img = img.convert('RGB')\n",
        "  img = transform1(img)\n",
        "  img = torch.unsqueeze(img, 0)\n",
        " \n",
        "  return img\n"
      ],
      "metadata": {
        "id": "_JWZiibyrHWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_image(img):\n",
        "  transform = transforms.ToPILImage()\n",
        "  img = transform(img)\n",
        "  plt.imshow(img)\n",
        "print(len(selected_train_samples))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFm3PqZnMKQT",
        "outputId": "da1fd4e9-7315-486c-bf0d-f513b6f094f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torchvision.models as models\n",
        "AlexNet = models.alexnet(pretrained=True)\n",
        "AlexNet.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535,
          "referenced_widgets": [
            "2ea7c08013534802882f7b05fb6be82f",
            "e9083df27fa042128abafa7d0b441f6c",
            "8ea92696519e4469bff5ca1efcd59b25",
            "1f10be49d42247d5963803a2fd6b4546",
            "f5a9e648a0da4e309e5de932aef38884",
            "118c1003fc944a2dbc028e1f0859cceb",
            "449a042adde2483d8de429de40fe6684",
            "a22dfc55fdc4406590037c147a9b48e3",
            "86ef623447d243118283c1cebbc32e75",
            "c8b8509b530747d1aa103183117436d4",
            "b483553b8c8b45379a5348614da5a0af"
          ]
        },
        "id": "TGz7hkNXrbVP",
        "outputId": "351e13a3-4a09-4651-8477-0176dd82297a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/233M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ea7c08013534802882f7b05fb6be82f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2_Decoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    self.learning_rate = 0.03\n",
        "    self.test_loss_epoch = []\n",
        "    self.train_loss_epoch = []\n",
        "    super(Conv2_Decoder, self).__init__()\n",
        "\n",
        "    features = list(AlexNet.features)[:6]\n",
        "    self.AlexNet_Conv2 = nn.ModuleList(features).eval()\n",
        "    self.conv1 = nn.Conv2d(in_channels=192, out_channels=256, kernel_size=3, stride=1 , padding=0)\n",
        "    self.conv2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1 , padding=0)\n",
        "    self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1 , padding=0)\n",
        "\n",
        "    self.upconv1 = nn.ConvTranspose2d(in_channels=256, out_channels=256, kernel_size=5, stride=2 ,padding=2)\n",
        "    self.upconv2 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=5, stride=2 ,padding=1)\n",
        "    self.upconv3 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=5, stride=2 ,padding=1)\n",
        "    self.upconv4 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=5, stride=2 ,padding=1)\n",
        "    self.upconv5 = nn.ConvTranspose2d(in_channels=32, out_channels=3, kernel_size=5, stride=2 ,padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    for i,layer in enumerate(self.AlexNet_Conv2):\n",
        "      x = layer(x)\n",
        "      if i == 5:\n",
        "        break\n",
        "    x = F.leaky_relu(self.conv1(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.conv2(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.conv3(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.upconv1(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.upconv2(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.upconv3(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.upconv4(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.upconv5(x), negative_slope=0.2)\n",
        "    x = F.pad(input=x, pad=(0, 1, 1, 0), mode='constant', value=0)\n",
        "    return x\n",
        "  \n",
        "  def calculate_test_loss(self, paths):\n",
        "    self.eval()\n",
        "    loss_function = nn.MSELoss()\n",
        "    test_loss = []\n",
        "    with torch.no_grad():\n",
        "      for i in range(len(paths)):\n",
        "        image = read_image(paths[i])\n",
        "        output = self(image) \n",
        "        loss = loss_function(output, image)\n",
        "        test_loss.append(loss.item())\n",
        "    total_loss = np.mean(test_loss)\n",
        "    print(\"The test Loss is: \", total_loss)\n",
        "    return total_loss\n",
        "\n",
        "  def train_net(self, learning_rate, epochs, train_data):\n",
        "    self.train()\n",
        "    optimizer = optim.Adam(self.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
        "    loss_function = nn.MSELoss()\n",
        "    EPOCHS = epochs\n",
        "    # self.test_loss_epoch.append(self.calculate_test_loss(test_imagepaths))\n",
        "    for epoch in range(EPOCHS):\n",
        "      num = 0\n",
        "      for image_path in train_data:\n",
        "        data = read_image(image_path)\n",
        "        optimizer.zero_grad()\n",
        "        output = self(data)\n",
        "        loss = loss_function(output, data)\n",
        "        self.train_loss_epoch.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        num += 1\n",
        "        \n",
        "        print(\"epoch: \", epoch, \"of batch: \", num, \" batch loss: \", loss)\n",
        "      scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "      # self.test_loss_epoch.append(self.calculate_test_loss(test_imagepaths))\n",
        "  \n",
        "  def test_model(self, image_path):\n",
        "    self.eval()\n",
        "    with torch.no_grad():\n",
        "      image = read_image(image_path)\n",
        "      reconstructed_image = self.forward(image)\n",
        "      return reconstructed_image\n",
        "\n",
        "  def plot_results(self):\n",
        "    plt.plot(np.arange(1,len(self.train_loss_epoch)+1), self.train_loss_epoch)\n",
        "    plt.ylabel('loss of Train')\n",
        "    plt.xlabel(\"number of steps\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    \n",
        "    plt.plot(np.arange(1,len(self.test_loss_epoch)+1), self.test_loss_epoch)\n",
        "    plt.ylabel('loss of test')\n",
        "    plt.xlabel(\"number of epochs\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "conv2_decoder = Conv2_Decoder()\n",
        "conv2_decoder.train_net(0.001, 1, train_imagepaths)\n",
        "# conv2_decoder.train_net(0.001, 1, train_batches)\n",
        "conv2_decoder.plot_results()\n",
        "torch.save(conv2_decoder.state_dict(), '/content/drive/My Drive/DL/HW2/new_conv2_decoder.pt')\n",
        "    "
      ],
      "metadata": {
        "id": "IXsgA54O7_30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv2_decoder = Conv2_Decoder()\n",
        "conv2_decoder.load_state_dict(torch.load(\"/content/drive/My Drive/DL/HW2/conv2_decoder.pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBRV_eHq_4QU",
        "outputId": "153cdd42-5fe2-4b53-f1cb-e7c9b79999c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"path: \", selected_train_samples[0])\n",
        "print(\"original train image: \")\n",
        "img = Image.open(selected_train_samples[0])\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "_94T2fac9YXW",
        "outputId": "da66556a-0267-47e2-c282-18b506592db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "path:  /content/drive/My Drive/DL/HW2/train/n02950826/images/n02950826_295.JPEG\n",
            "original train image: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fda27a9ff50>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"reconstructed train image: \")\n",
        "print(selected_train_samples[0])\n",
        "img = conv2_decoder.test_model(selected_train_samples[0])\n",
        "img = img.squeeze(0)\n",
        "print(img.shape)\n",
        "transform = transforms.ToPILImage()\n",
        "im1 = transform(img)\n",
        "plt.imshow(im1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "-InAA9Ek8dDc",
        "outputId": "4613d1dd-aaf6-488f-c065-15162e1f1265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reconstructed train image: \n",
            "/content/drive/My Drive/DL/HW2/train/n02950826/images/n02950826_295.JPEG\n",
            "torch.Size([3, 224, 224])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fda2794df90>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"path: \", selected_test_samples[0])\n",
        "print(\"original validation image: \")\n",
        "img = Image.open(selected_test_samples[0])\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "vsIGFB7Qe9hq",
        "outputId": "817828a2-b536-4b8a-e214-ce4d17168cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "path:  /content/drive/My Drive/DL/HW2/train/n02950826/images/n02950826_131.JPEG\n",
            "original validation image: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fda27c960d0>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"reconstructed validation image: \")\n",
        "img = conv2_decoder.test_model(selected_test_samples[0])\n",
        "img = img.squeeze(0)\n",
        "print(img.shape)\n",
        "transform = transforms.ToPILImage()\n",
        "im1 = transform(img)\n",
        "plt.imshow(im1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "oHcnslEc9Ilf",
        "outputId": "891f733f-7d9b-463c-c5f4-d03b6a97aa91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reconstructed validation image: \n",
            "torch.Size([3, 224, 224])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fda285c7e10>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv5_Decoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    self.learning_rate = 0.03\n",
        "    self.test_loss_epoch = []\n",
        "    self.train_loss_epoch = []\n",
        "    \n",
        "    super(Conv5_Decoder, self).__init__()\n",
        "\n",
        "    features = list(AlexNet.features)[:13]\n",
        "    self.AlexNet_Conv5 = nn.ModuleList(features).eval()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1 , padding=0)\n",
        "    self.conv2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1 , padding=0)\n",
        "    self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1 , padding=0)\n",
        "\n",
        "    self.upconv1 = nn.ConvTranspose2d(in_channels=256, out_channels=256, kernel_size=5, stride=2 ,padding=2)\n",
        "    self.upconv2 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=5, stride=2 ,padding=1)\n",
        "    self.upconv3 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=5, stride=2 ,padding=1)\n",
        "    self.upconv4 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=5, stride=2 ,padding=1)\n",
        "    self.upconv5 = nn.ConvTranspose2d(in_channels=32, out_channels=3, kernel_size=5, stride=2 ,padding=1)\n",
        "\n",
        "    \n",
        "  def forward(self, x):\n",
        "    for i,layer in enumerate(self.AlexNet_Conv5):\n",
        "      x = layer(x)\n",
        "      if i == 12:\n",
        "        break\n",
        "    x = F.leaky_relu(self.conv1(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.conv2(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.conv3(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.upconv1(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.upconv2(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.upconv3(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.upconv4(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.upconv5(x), negative_slope=0.2)\n",
        "\n",
        "    x = F.pad(input=x, pad=(0, 1, 1, 0), mode='constant', value=0)\n",
        "\n",
        "    return x\n",
        "  def calculate_test_loss(self, paths):\n",
        "    self.eval()\n",
        "    loss_function = nn.MSELoss()\n",
        "    test_loss = []\n",
        "    with torch.no_grad():\n",
        "      for i in range(len(paths)):\n",
        "        image = read_image(paths[i])\n",
        "        output = self(image) \n",
        "        loss = loss_function(output, image)\n",
        "        test_loss.append(loss.item())\n",
        "    total_loss = np.mean(test_loss)\n",
        "    print(\"The test Loss is: \", total_loss)\n",
        "    return total_loss\n",
        "\n",
        "  def train_net(self, learning_rate, epochs, train_data):\n",
        "    self.train()\n",
        "    optimizer = optim.Adam(self.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
        "    loss_function = nn.MSELoss()\n",
        "    EPOCHS = epochs\n",
        "    self.test_loss_epoch.append(self.calculate_test_loss(test_imagepaths))\n",
        "    for epoch in range(EPOCHS):\n",
        "      num = 0\n",
        "      for image_path in train_data:\n",
        "        data = read_image(image_path)\n",
        "        optimizer.zero_grad()\n",
        "        output = self(data)\n",
        "        loss = loss_function(output, data)\n",
        "        self.train_loss_epoch.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        num += 1\n",
        "        print(\"epoch: \", epoch, \"of batch: \", num, \" batch loss: \", loss)\n",
        "      self.test_loss_epoch.append(self.calculate_test_loss(test_imagepaths))\n",
        "  \n",
        "  def test_model(self, image_path):\n",
        "    self.eval()\n",
        "    with torch.no_grad():\n",
        "      image = read_image(image_path)\n",
        "      reconstructed_image = self.forward(image)\n",
        "      return reconstructed_image\n",
        "\n",
        "  def plot_results(self):\n",
        "    plt.plot(np.arange(1,len(self.train_loss_epoch)+1), self.train_loss_epoch)\n",
        "    plt.ylabel('loss of Train')\n",
        "    plt.xlabel(\"number of steps\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    \n",
        "    plt.plot(np.arange(1,len(self.test_loss_epoch)+1), self.test_loss_epoch)\n",
        "    plt.ylabel('loss of test')\n",
        "    plt.xlabel(\"number of epochs\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "conv5_decoder = Conv5_Decoder()\n",
        "conv5_decoder.train_net(0.001, 15, train_imagepaths)\n",
        "conv5_decoder.plot_results()\n",
        "torch.save(conv5_decoder.state_dict(), '/content/drive/My Drive/DL/HW2/conv5_decoder.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i4-zeb-zgdF",
        "outputId": "2e5d9211-f7fd-4729-ee45-42630ab9388f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Fc6_Decoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    self.learning_rate = 0.03\n",
        "    self.test_loss_epoch = []\n",
        "    self.train_loss_epoch = []\n",
        "    super(Fc6_Decoder, self).__init__()\n",
        "    \n",
        "    self.AlexNet_conv = nn.ModuleList(list(AlexNet.features)[:]).eval()\n",
        "    self.avg_pool =  AlexNet.avgpool.eval()\n",
        "    self.AlexNet_FC6 = nn.ModuleList(list(AlexNet.classifier)[:3]).eval()\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=(4096), out_features=4096)\n",
        "    self.fc2 = nn.Linear(in_features=(4096), out_features=4096)\n",
        "    self.fc3 = nn.Linear(in_features=(4096), out_features=4096)\n",
        "\n",
        "    self.upconv1 = nn.ConvTranspose2d(in_channels=256, out_channels=256, kernel_size=4, stride=2 ,padding=0)\n",
        "    self.upconv2 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=5, stride=2 ,padding=0)\n",
        "    self.upconv3 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=5, stride=2 ,padding=0)\n",
        "    self.upconv4 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=5, stride=2 ,padding=0)\n",
        "    self.upconv5 = nn.ConvTranspose2d(in_channels=32, out_channels=3, kernel_size=5, stride=2 ,padding=0)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    for i,layer in enumerate(self.AlexNet_conv):\n",
        "      x = layer(x)\n",
        "    x = self.avg_pool(x)\n",
        "    x = x.view(-1,  1 * 1 * 9216)\n",
        "    x = x.squeeze(0)\n",
        "    for i,layer in enumerate(self.AlexNet_FC6):  \n",
        "      x = layer(x)\n",
        "      if i == 2:\n",
        "        break\n",
        "    x = F.leaky_relu(self.fc1(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.fc2(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.fc3(x), negative_slope=0.2)\n",
        "    #reshaping 4096 --> 4x4x256\n",
        "    x = x.view(1,256, 4 ,4)\n",
        "\n",
        "    x = F.leaky_relu(self.upconv1(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.upconv2(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.upconv3(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.upconv4(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.upconv5(x), negative_slope=0.2)\n",
        "\n",
        "    x = F.pad(input=x, pad=(10, 9, 9, 10), mode='constant', value=0)\n",
        "\n",
        "    return x\n",
        "  \n",
        "  def calculate_test_loss(self, paths):\n",
        "    self.eval()\n",
        "    loss_function = nn.MSELoss()\n",
        "    test_loss = []\n",
        "    with torch.no_grad():\n",
        "      for i in range(len(paths)):\n",
        "        image = read_image(paths[i])\n",
        "        output = self(image) \n",
        "        loss = loss_function(output, image)\n",
        "        test_loss.append(loss.item())\n",
        "    total_loss = np.mean(test_loss)\n",
        "    print(\"The test Loss is: \", total_loss)\n",
        "    return total_loss\n",
        "\n",
        "  def train_net(self, learning_rate, epochs, train_data):\n",
        "    self.train()\n",
        "    optimizer = optim.Adam(self.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
        "    loss_function = nn.MSELoss()\n",
        "    EPOCHS = epochs\n",
        "    self.test_loss_epoch.append(self.calculate_test_loss(test_imagepaths))\n",
        "    for epoch in range(EPOCHS):\n",
        "      num = 0\n",
        "      for image_path in train_data:\n",
        "        data = read_image(image_path)\n",
        "        optimizer.zero_grad()\n",
        "        output = self(data)\n",
        "        loss = loss_function(output, data)\n",
        "        self.train_loss_epoch.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        num += 1\n",
        "        print(\"epoch: \", epoch, \"of batch: \", num, \" batch loss: \", loss)\n",
        "      self.test_loss_epoch.append(self.calculate_test_loss(test_imagepaths))\n",
        "  \n",
        "  def test_model(self, image_path):\n",
        "    self.eval()\n",
        "    with torch.no_grad():\n",
        "      image = read_image(image_path)\n",
        "      reconstructed_image = self.forward(image)\n",
        "      return reconstructed_image\n",
        "\n",
        "  def plot_results(self):\n",
        "    plt.plot(np.arange(1,len(self.train_loss_epoch)+1), self.train_loss_epoch)\n",
        "    plt.ylabel('loss of Train')\n",
        "    plt.xlabel(\"number of steps\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    \n",
        "    plt.plot(np.arange(1,len(self.test_loss_epoch)+1), self.test_loss_epoch)\n",
        "    plt.ylabel('loss of test')\n",
        "    plt.xlabel(\"number of epochs\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "fc6_decoder = Fc6_Decoder()\n",
        "fc6_decoder.train_net(0.001, 1, train_imagepaths)\n",
        "fc6_decoder.plot_results()\n",
        "torch.save(fc6_decoder.state_dict(), '/content/drive/My Drive/DL/HW2/fc6_decoder.pt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vBW5Sh1jzgoW",
        "outputId": "243342a0-b9a7-4d77-ac5d-b8752c8b3582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "epoch:  0 of batch:  4002  batch loss:  tensor(0.1267, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4003  batch loss:  tensor(0.1829, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4004  batch loss:  tensor(0.1297, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4005  batch loss:  tensor(0.0710, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4006  batch loss:  tensor(0.0989, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4007  batch loss:  tensor(0.0558, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4008  batch loss:  tensor(0.0971, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4009  batch loss:  tensor(0.0576, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4010  batch loss:  tensor(0.0739, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4011  batch loss:  tensor(0.0691, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4012  batch loss:  tensor(0.1473, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4013  batch loss:  tensor(0.0438, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4014  batch loss:  tensor(0.0838, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4015  batch loss:  tensor(0.0610, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4016  batch loss:  tensor(0.1157, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4017  batch loss:  tensor(0.1192, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4018  batch loss:  tensor(0.1804, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4019  batch loss:  tensor(0.1090, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4020  batch loss:  tensor(0.1974, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4021  batch loss:  tensor(0.1095, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4022  batch loss:  tensor(0.0715, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4023  batch loss:  tensor(0.0880, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4024  batch loss:  tensor(0.0606, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4025  batch loss:  tensor(0.1803, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4026  batch loss:  tensor(0.0663, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4027  batch loss:  tensor(0.1117, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4028  batch loss:  tensor(0.1598, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4029  batch loss:  tensor(0.0672, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4030  batch loss:  tensor(0.0823, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4031  batch loss:  tensor(0.0671, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4032  batch loss:  tensor(0.1076, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4033  batch loss:  tensor(0.0671, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4034  batch loss:  tensor(0.1411, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4035  batch loss:  tensor(0.0807, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4036  batch loss:  tensor(0.1020, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4037  batch loss:  tensor(0.0859, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4038  batch loss:  tensor(0.1036, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4039  batch loss:  tensor(0.2616, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4040  batch loss:  tensor(0.0973, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4041  batch loss:  tensor(0.0598, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4042  batch loss:  tensor(0.1158, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4043  batch loss:  tensor(0.0542, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4044  batch loss:  tensor(0.0839, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4045  batch loss:  tensor(0.0737, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4046  batch loss:  tensor(0.1079, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4047  batch loss:  tensor(0.1477, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4048  batch loss:  tensor(0.0750, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4049  batch loss:  tensor(0.0787, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4050  batch loss:  tensor(0.1016, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4051  batch loss:  tensor(0.0770, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4052  batch loss:  tensor(0.1993, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4053  batch loss:  tensor(0.1391, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4054  batch loss:  tensor(0.0780, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4055  batch loss:  tensor(0.1326, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4056  batch loss:  tensor(0.0673, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4057  batch loss:  tensor(0.0883, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4058  batch loss:  tensor(0.1104, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4059  batch loss:  tensor(0.1258, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4060  batch loss:  tensor(0.1217, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4061  batch loss:  tensor(0.0856, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4062  batch loss:  tensor(0.2434, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4063  batch loss:  tensor(0.2371, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4064  batch loss:  tensor(0.0588, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4065  batch loss:  tensor(0.0849, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4066  batch loss:  tensor(0.0992, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4067  batch loss:  tensor(0.0521, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4068  batch loss:  tensor(0.1235, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4069  batch loss:  tensor(0.0763, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4070  batch loss:  tensor(0.0589, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4071  batch loss:  tensor(0.1674, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4072  batch loss:  tensor(0.1238, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4073  batch loss:  tensor(0.0994, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4074  batch loss:  tensor(0.1519, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4075  batch loss:  tensor(0.0822, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4076  batch loss:  tensor(0.0796, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4077  batch loss:  tensor(0.1116, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4078  batch loss:  tensor(0.1043, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4079  batch loss:  tensor(0.0716, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4080  batch loss:  tensor(0.0623, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4081  batch loss:  tensor(0.0896, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4082  batch loss:  tensor(0.1276, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4083  batch loss:  tensor(0.0880, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4084  batch loss:  tensor(0.0545, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4085  batch loss:  tensor(0.0810, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4086  batch loss:  tensor(0.0944, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4087  batch loss:  tensor(0.0870, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4088  batch loss:  tensor(0.0902, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4089  batch loss:  tensor(0.0638, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4090  batch loss:  tensor(0.1160, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4091  batch loss:  tensor(0.0662, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4092  batch loss:  tensor(0.1086, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4093  batch loss:  tensor(0.1399, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4094  batch loss:  tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4095  batch loss:  tensor(0.1046, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4096  batch loss:  tensor(0.0917, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4097  batch loss:  tensor(0.1270, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4098  batch loss:  tensor(0.0840, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4099  batch loss:  tensor(0.0678, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4100  batch loss:  tensor(0.0497, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4101  batch loss:  tensor(0.1125, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4102  batch loss:  tensor(0.0845, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4103  batch loss:  tensor(0.0937, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4104  batch loss:  tensor(0.0475, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4105  batch loss:  tensor(0.0917, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4106  batch loss:  tensor(0.1141, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4107  batch loss:  tensor(0.0588, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4108  batch loss:  tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4109  batch loss:  tensor(0.0577, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4110  batch loss:  tensor(0.2080, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4111  batch loss:  tensor(0.0671, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4112  batch loss:  tensor(0.1281, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4113  batch loss:  tensor(0.1080, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4114  batch loss:  tensor(0.0588, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4115  batch loss:  tensor(0.0714, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4116  batch loss:  tensor(0.0622, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4117  batch loss:  tensor(0.0557, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4118  batch loss:  tensor(0.0730, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4119  batch loss:  tensor(0.1472, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4120  batch loss:  tensor(0.0804, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4121  batch loss:  tensor(0.0601, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4122  batch loss:  tensor(0.0973, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4123  batch loss:  tensor(0.1124, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4124  batch loss:  tensor(0.1210, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4125  batch loss:  tensor(0.1063, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4126  batch loss:  tensor(0.1117, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4127  batch loss:  tensor(0.1087, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4128  batch loss:  tensor(0.1716, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4129  batch loss:  tensor(0.2834, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4130  batch loss:  tensor(0.0871, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4131  batch loss:  tensor(0.1329, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4132  batch loss:  tensor(0.1196, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4133  batch loss:  tensor(0.0747, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4134  batch loss:  tensor(0.0731, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4135  batch loss:  tensor(0.1077, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4136  batch loss:  tensor(0.0783, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4137  batch loss:  tensor(0.0933, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4138  batch loss:  tensor(0.0735, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4139  batch loss:  tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4140  batch loss:  tensor(0.0975, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4141  batch loss:  tensor(0.0612, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4142  batch loss:  tensor(0.3326, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4143  batch loss:  tensor(0.0870, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4144  batch loss:  tensor(0.1030, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4145  batch loss:  tensor(0.0668, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4146  batch loss:  tensor(0.1410, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4147  batch loss:  tensor(0.0748, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4148  batch loss:  tensor(0.0716, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4149  batch loss:  tensor(0.0665, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4150  batch loss:  tensor(0.0756, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4151  batch loss:  tensor(0.0661, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4152  batch loss:  tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4153  batch loss:  tensor(0.0933, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4154  batch loss:  tensor(0.0504, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4155  batch loss:  tensor(0.0791, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4156  batch loss:  tensor(0.0783, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4157  batch loss:  tensor(0.0623, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4158  batch loss:  tensor(0.0692, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4159  batch loss:  tensor(0.0720, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4160  batch loss:  tensor(0.0707, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4161  batch loss:  tensor(0.0803, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4162  batch loss:  tensor(0.1452, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4163  batch loss:  tensor(0.0857, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4164  batch loss:  tensor(0.0823, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4165  batch loss:  tensor(0.0653, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4166  batch loss:  tensor(0.1207, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4167  batch loss:  tensor(0.0876, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4168  batch loss:  tensor(0.0878, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4169  batch loss:  tensor(0.1104, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4170  batch loss:  tensor(0.2546, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4171  batch loss:  tensor(0.1176, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4172  batch loss:  tensor(0.0564, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4173  batch loss:  tensor(0.1122, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4174  batch loss:  tensor(0.1048, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4175  batch loss:  tensor(0.1447, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4176  batch loss:  tensor(0.0585, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4177  batch loss:  tensor(0.0549, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4178  batch loss:  tensor(0.0669, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4179  batch loss:  tensor(0.0773, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4180  batch loss:  tensor(0.0967, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4181  batch loss:  tensor(0.1199, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4182  batch loss:  tensor(0.0526, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4183  batch loss:  tensor(0.0757, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4184  batch loss:  tensor(0.1095, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4185  batch loss:  tensor(0.0399, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4186  batch loss:  tensor(0.1146, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4187  batch loss:  tensor(0.0834, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4188  batch loss:  tensor(0.2295, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4189  batch loss:  tensor(0.0986, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4190  batch loss:  tensor(0.0970, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4191  batch loss:  tensor(0.0768, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4192  batch loss:  tensor(0.2796, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4193  batch loss:  tensor(0.1284, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4194  batch loss:  tensor(0.1289, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4195  batch loss:  tensor(0.1205, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4196  batch loss:  tensor(0.0682, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4197  batch loss:  tensor(0.1326, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4198  batch loss:  tensor(0.0647, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4199  batch loss:  tensor(0.0839, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4200  batch loss:  tensor(0.1247, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4201  batch loss:  tensor(0.0940, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4202  batch loss:  tensor(0.2636, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4203  batch loss:  tensor(0.1493, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4204  batch loss:  tensor(0.2291, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4205  batch loss:  tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4206  batch loss:  tensor(0.0665, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4207  batch loss:  tensor(0.2055, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4208  batch loss:  tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4209  batch loss:  tensor(0.0525, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4210  batch loss:  tensor(0.1015, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4211  batch loss:  tensor(0.0960, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4212  batch loss:  tensor(0.0999, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4213  batch loss:  tensor(0.1104, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4214  batch loss:  tensor(0.0919, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4215  batch loss:  tensor(0.0743, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4216  batch loss:  tensor(0.0737, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4217  batch loss:  tensor(0.1003, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4218  batch loss:  tensor(0.0578, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4219  batch loss:  tensor(0.1286, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4220  batch loss:  tensor(0.0473, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4221  batch loss:  tensor(0.0651, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4222  batch loss:  tensor(0.0700, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4223  batch loss:  tensor(0.1164, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4224  batch loss:  tensor(0.0395, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4225  batch loss:  tensor(0.0456, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4226  batch loss:  tensor(0.0956, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4227  batch loss:  tensor(0.1724, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4228  batch loss:  tensor(0.0812, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4229  batch loss:  tensor(0.0933, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4230  batch loss:  tensor(0.0901, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4231  batch loss:  tensor(0.1171, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4232  batch loss:  tensor(0.0472, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4233  batch loss:  tensor(0.1905, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4234  batch loss:  tensor(0.1235, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4235  batch loss:  tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4236  batch loss:  tensor(0.0637, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4237  batch loss:  tensor(0.0486, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4238  batch loss:  tensor(0.1334, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4239  batch loss:  tensor(0.1141, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4240  batch loss:  tensor(0.1064, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4241  batch loss:  tensor(0.1205, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4242  batch loss:  tensor(0.0949, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4243  batch loss:  tensor(0.0520, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4244  batch loss:  tensor(0.1689, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4245  batch loss:  tensor(0.1326, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4246  batch loss:  tensor(0.0692, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4247  batch loss:  tensor(0.1270, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4248  batch loss:  tensor(0.0596, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4249  batch loss:  tensor(0.0577, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4250  batch loss:  tensor(0.0641, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4251  batch loss:  tensor(0.0748, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4252  batch loss:  tensor(0.0616, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4253  batch loss:  tensor(0.0829, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4254  batch loss:  tensor(0.1195, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4255  batch loss:  tensor(0.0541, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4256  batch loss:  tensor(0.1270, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4257  batch loss:  tensor(0.2067, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4258  batch loss:  tensor(0.0794, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4259  batch loss:  tensor(0.3216, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4260  batch loss:  tensor(0.1150, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4261  batch loss:  tensor(0.1196, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4262  batch loss:  tensor(0.1753, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4263  batch loss:  tensor(0.1136, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4264  batch loss:  tensor(0.0948, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4265  batch loss:  tensor(0.1263, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4266  batch loss:  tensor(0.1048, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4267  batch loss:  tensor(0.0733, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4268  batch loss:  tensor(0.0629, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4269  batch loss:  tensor(0.0601, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4270  batch loss:  tensor(0.1870, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4271  batch loss:  tensor(0.0928, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4272  batch loss:  tensor(0.1958, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4273  batch loss:  tensor(0.0643, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4274  batch loss:  tensor(0.1209, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4275  batch loss:  tensor(0.1846, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4276  batch loss:  tensor(0.1072, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4277  batch loss:  tensor(0.0607, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4278  batch loss:  tensor(0.1146, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4279  batch loss:  tensor(0.0870, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4280  batch loss:  tensor(0.1286, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4281  batch loss:  tensor(0.0769, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4282  batch loss:  tensor(0.0854, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4283  batch loss:  tensor(0.1474, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4284  batch loss:  tensor(0.0477, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4285  batch loss:  tensor(0.1566, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4286  batch loss:  tensor(0.0729, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4287  batch loss:  tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4288  batch loss:  tensor(0.0912, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4289  batch loss:  tensor(0.0842, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4290  batch loss:  tensor(0.0848, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4291  batch loss:  tensor(0.0767, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4292  batch loss:  tensor(0.1389, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4293  batch loss:  tensor(0.0825, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4294  batch loss:  tensor(0.0652, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4295  batch loss:  tensor(0.0567, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4296  batch loss:  tensor(0.1111, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4297  batch loss:  tensor(0.0719, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4298  batch loss:  tensor(0.0688, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4299  batch loss:  tensor(0.1555, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4300  batch loss:  tensor(0.0960, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4301  batch loss:  tensor(0.1113, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4302  batch loss:  tensor(0.0653, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4303  batch loss:  tensor(0.0350, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4304  batch loss:  tensor(0.0936, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4305  batch loss:  tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4306  batch loss:  tensor(0.0575, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4307  batch loss:  tensor(0.1042, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4308  batch loss:  tensor(0.0563, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4309  batch loss:  tensor(0.1002, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4310  batch loss:  tensor(0.1420, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4311  batch loss:  tensor(0.1655, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4312  batch loss:  tensor(0.0702, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4313  batch loss:  tensor(0.1250, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4314  batch loss:  tensor(0.0606, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4315  batch loss:  tensor(0.1044, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4316  batch loss:  tensor(0.0830, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4317  batch loss:  tensor(0.0769, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4318  batch loss:  tensor(0.0813, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4319  batch loss:  tensor(0.0816, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4320  batch loss:  tensor(0.0422, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4321  batch loss:  tensor(0.1137, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4322  batch loss:  tensor(0.0751, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4323  batch loss:  tensor(0.1396, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4324  batch loss:  tensor(0.0922, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4325  batch loss:  tensor(0.1841, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4326  batch loss:  tensor(0.0668, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4327  batch loss:  tensor(0.0555, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4328  batch loss:  tensor(0.0597, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4329  batch loss:  tensor(0.0768, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4330  batch loss:  tensor(0.1088, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4331  batch loss:  tensor(0.0763, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4332  batch loss:  tensor(0.1357, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4333  batch loss:  tensor(0.2228, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4334  batch loss:  tensor(0.0787, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4335  batch loss:  tensor(0.0825, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4336  batch loss:  tensor(0.0932, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4337  batch loss:  tensor(0.1215, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4338  batch loss:  tensor(0.0866, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4339  batch loss:  tensor(0.0868, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4340  batch loss:  tensor(0.0995, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4341  batch loss:  tensor(0.1121, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4342  batch loss:  tensor(0.0779, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4343  batch loss:  tensor(0.0624, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4344  batch loss:  tensor(0.0796, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4345  batch loss:  tensor(0.0989, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4346  batch loss:  tensor(0.1088, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4347  batch loss:  tensor(0.0596, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4348  batch loss:  tensor(0.0834, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4349  batch loss:  tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4350  batch loss:  tensor(0.0676, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4351  batch loss:  tensor(0.1438, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4352  batch loss:  tensor(0.1176, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4353  batch loss:  tensor(0.0999, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4354  batch loss:  tensor(0.0486, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4355  batch loss:  tensor(0.0486, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4356  batch loss:  tensor(0.1917, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4357  batch loss:  tensor(0.1179, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4358  batch loss:  tensor(0.0640, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4359  batch loss:  tensor(0.1117, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4360  batch loss:  tensor(0.0794, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4361  batch loss:  tensor(0.0875, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4362  batch loss:  tensor(0.0904, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4363  batch loss:  tensor(0.0585, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4364  batch loss:  tensor(0.1482, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4365  batch loss:  tensor(0.1612, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4366  batch loss:  tensor(0.0751, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4367  batch loss:  tensor(0.0836, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4368  batch loss:  tensor(0.1149, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4369  batch loss:  tensor(0.1732, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4370  batch loss:  tensor(0.1089, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4371  batch loss:  tensor(0.1131, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4372  batch loss:  tensor(0.0855, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4373  batch loss:  tensor(0.0807, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4374  batch loss:  tensor(0.0953, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4375  batch loss:  tensor(0.0629, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4376  batch loss:  tensor(0.0731, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4377  batch loss:  tensor(0.0939, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4378  batch loss:  tensor(0.1191, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4379  batch loss:  tensor(0.0915, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4380  batch loss:  tensor(0.1129, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4381  batch loss:  tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4382  batch loss:  tensor(0.1105, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4383  batch loss:  tensor(0.1130, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4384  batch loss:  tensor(0.0605, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4385  batch loss:  tensor(0.0620, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4386  batch loss:  tensor(0.1004, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4387  batch loss:  tensor(0.0931, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4388  batch loss:  tensor(0.0970, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4389  batch loss:  tensor(0.0514, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4390  batch loss:  tensor(0.1033, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4391  batch loss:  tensor(0.0935, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4392  batch loss:  tensor(0.1342, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4393  batch loss:  tensor(0.1084, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4394  batch loss:  tensor(0.0879, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4395  batch loss:  tensor(0.0861, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4396  batch loss:  tensor(0.0783, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4397  batch loss:  tensor(0.1270, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4398  batch loss:  tensor(0.1595, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4399  batch loss:  tensor(0.0587, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4400  batch loss:  tensor(0.0805, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4401  batch loss:  tensor(0.0976, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4402  batch loss:  tensor(0.2099, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4403  batch loss:  tensor(0.1246, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4404  batch loss:  tensor(0.0832, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4405  batch loss:  tensor(0.2112, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4406  batch loss:  tensor(0.0666, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4407  batch loss:  tensor(0.1757, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4408  batch loss:  tensor(0.0888, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4409  batch loss:  tensor(0.0897, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4410  batch loss:  tensor(0.0552, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4411  batch loss:  tensor(0.0973, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4412  batch loss:  tensor(0.1108, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4413  batch loss:  tensor(0.0988, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4414  batch loss:  tensor(0.0806, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4415  batch loss:  tensor(0.0860, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4416  batch loss:  tensor(0.1828, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4417  batch loss:  tensor(0.1370, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4418  batch loss:  tensor(0.0854, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4419  batch loss:  tensor(0.1282, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4420  batch loss:  tensor(0.0673, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4421  batch loss:  tensor(0.1076, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4422  batch loss:  tensor(0.0602, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4423  batch loss:  tensor(0.0884, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4424  batch loss:  tensor(0.0915, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4425  batch loss:  tensor(0.0445, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4426  batch loss:  tensor(0.0520, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4427  batch loss:  tensor(0.1762, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4428  batch loss:  tensor(0.0711, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4429  batch loss:  tensor(0.0868, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4430  batch loss:  tensor(0.0699, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4431  batch loss:  tensor(0.0674, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4432  batch loss:  tensor(0.0699, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4433  batch loss:  tensor(0.0573, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4434  batch loss:  tensor(0.0827, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4435  batch loss:  tensor(0.1033, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4436  batch loss:  tensor(0.0869, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4437  batch loss:  tensor(0.0660, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4438  batch loss:  tensor(0.0542, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4439  batch loss:  tensor(0.0682, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4440  batch loss:  tensor(0.0732, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4441  batch loss:  tensor(0.0607, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4442  batch loss:  tensor(0.0820, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4443  batch loss:  tensor(0.2004, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4444  batch loss:  tensor(0.1020, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4445  batch loss:  tensor(0.2199, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4446  batch loss:  tensor(0.0968, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4447  batch loss:  tensor(0.0820, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4448  batch loss:  tensor(0.0552, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4449  batch loss:  tensor(0.1078, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4450  batch loss:  tensor(0.0737, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4451  batch loss:  tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4452  batch loss:  tensor(0.0855, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4453  batch loss:  tensor(0.0985, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4454  batch loss:  tensor(0.1099, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4455  batch loss:  tensor(0.1055, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4456  batch loss:  tensor(0.0634, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4457  batch loss:  tensor(0.0494, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4458  batch loss:  tensor(0.0719, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4459  batch loss:  tensor(0.0714, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4460  batch loss:  tensor(0.0502, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4461  batch loss:  tensor(0.1314, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4462  batch loss:  tensor(0.0631, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4463  batch loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4464  batch loss:  tensor(0.0590, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4465  batch loss:  tensor(0.0476, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4466  batch loss:  tensor(0.0794, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4467  batch loss:  tensor(0.1836, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4468  batch loss:  tensor(0.1008, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4469  batch loss:  tensor(0.1700, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4470  batch loss:  tensor(0.1740, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4471  batch loss:  tensor(0.1062, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4472  batch loss:  tensor(0.1261, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4473  batch loss:  tensor(0.0721, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4474  batch loss:  tensor(0.1109, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4475  batch loss:  tensor(0.1582, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4476  batch loss:  tensor(0.1566, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4477  batch loss:  tensor(0.1476, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4478  batch loss:  tensor(0.0549, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4479  batch loss:  tensor(0.1013, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4480  batch loss:  tensor(0.0798, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4481  batch loss:  tensor(0.1141, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4482  batch loss:  tensor(0.0875, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4483  batch loss:  tensor(0.0602, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4484  batch loss:  tensor(0.0769, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4485  batch loss:  tensor(0.0633, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4486  batch loss:  tensor(0.0660, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4487  batch loss:  tensor(0.0517, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4488  batch loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4489  batch loss:  tensor(0.0556, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4490  batch loss:  tensor(0.0910, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4491  batch loss:  tensor(0.1379, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4492  batch loss:  tensor(0.0596, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4493  batch loss:  tensor(0.0711, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4494  batch loss:  tensor(0.1400, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4495  batch loss:  tensor(0.0793, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4496  batch loss:  tensor(0.0654, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4497  batch loss:  tensor(0.1020, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4498  batch loss:  tensor(0.0946, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4499  batch loss:  tensor(0.1015, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4500  batch loss:  tensor(0.0885, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4501  batch loss:  tensor(0.0954, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4502  batch loss:  tensor(0.0734, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4503  batch loss:  tensor(0.0633, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4504  batch loss:  tensor(0.0659, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4505  batch loss:  tensor(0.2137, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4506  batch loss:  tensor(0.0523, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4507  batch loss:  tensor(0.1489, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4508  batch loss:  tensor(0.0854, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4509  batch loss:  tensor(0.1106, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4510  batch loss:  tensor(0.0669, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4511  batch loss:  tensor(0.0885, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4512  batch loss:  tensor(0.1176, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4513  batch loss:  tensor(0.1322, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4514  batch loss:  tensor(0.0671, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4515  batch loss:  tensor(0.0554, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4516  batch loss:  tensor(0.1134, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4517  batch loss:  tensor(0.0711, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4518  batch loss:  tensor(0.0725, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4519  batch loss:  tensor(0.0488, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4520  batch loss:  tensor(0.1137, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4521  batch loss:  tensor(0.0343, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4522  batch loss:  tensor(0.0522, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4523  batch loss:  tensor(0.1194, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4524  batch loss:  tensor(0.0851, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4525  batch loss:  tensor(0.1007, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4526  batch loss:  tensor(0.0697, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4527  batch loss:  tensor(0.0699, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4528  batch loss:  tensor(0.1800, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4529  batch loss:  tensor(0.0647, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4530  batch loss:  tensor(0.1117, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4531  batch loss:  tensor(0.0728, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4532  batch loss:  tensor(0.1107, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4533  batch loss:  tensor(0.0664, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4534  batch loss:  tensor(0.1952, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4535  batch loss:  tensor(0.1254, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4536  batch loss:  tensor(0.0660, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4537  batch loss:  tensor(0.0889, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4538  batch loss:  tensor(0.0539, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4539  batch loss:  tensor(0.0475, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4540  batch loss:  tensor(0.1662, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4541  batch loss:  tensor(0.0627, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4542  batch loss:  tensor(0.1045, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4543  batch loss:  tensor(0.0729, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4544  batch loss:  tensor(0.1076, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4545  batch loss:  tensor(0.0475, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4546  batch loss:  tensor(0.0816, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4547  batch loss:  tensor(0.0590, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4548  batch loss:  tensor(0.0628, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4549  batch loss:  tensor(0.0434, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4550  batch loss:  tensor(0.0847, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4551  batch loss:  tensor(0.2965, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4552  batch loss:  tensor(0.0915, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4553  batch loss:  tensor(0.1090, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4554  batch loss:  tensor(0.1537, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4555  batch loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4556  batch loss:  tensor(0.1280, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4557  batch loss:  tensor(0.1025, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4558  batch loss:  tensor(0.0477, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4559  batch loss:  tensor(0.1531, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4560  batch loss:  tensor(0.0912, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4561  batch loss:  tensor(0.1221, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4562  batch loss:  tensor(0.1001, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4563  batch loss:  tensor(0.0578, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4564  batch loss:  tensor(0.0911, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4565  batch loss:  tensor(0.1502, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4566  batch loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4567  batch loss:  tensor(0.2040, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4568  batch loss:  tensor(0.0877, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4569  batch loss:  tensor(0.0911, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4570  batch loss:  tensor(0.0585, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4571  batch loss:  tensor(0.0969, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4572  batch loss:  tensor(0.0446, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4573  batch loss:  tensor(0.0622, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4574  batch loss:  tensor(0.0559, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4575  batch loss:  tensor(0.1745, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4576  batch loss:  tensor(0.0344, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4577  batch loss:  tensor(0.1812, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4578  batch loss:  tensor(0.1085, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4579  batch loss:  tensor(0.1056, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4580  batch loss:  tensor(0.1358, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4581  batch loss:  tensor(0.0704, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4582  batch loss:  tensor(0.1202, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4583  batch loss:  tensor(0.1234, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4584  batch loss:  tensor(0.2272, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4585  batch loss:  tensor(0.1392, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4586  batch loss:  tensor(0.1363, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4587  batch loss:  tensor(0.1124, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4588  batch loss:  tensor(0.0796, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4589  batch loss:  tensor(0.1266, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4590  batch loss:  tensor(0.0996, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4591  batch loss:  tensor(0.0787, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4592  batch loss:  tensor(0.1186, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4593  batch loss:  tensor(0.0891, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4594  batch loss:  tensor(0.1068, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4595  batch loss:  tensor(0.1034, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4596  batch loss:  tensor(0.1276, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4597  batch loss:  tensor(0.0516, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4598  batch loss:  tensor(0.1461, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4599  batch loss:  tensor(0.0471, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4600  batch loss:  tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4601  batch loss:  tensor(0.0403, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4602  batch loss:  tensor(0.0633, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4603  batch loss:  tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4604  batch loss:  tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4605  batch loss:  tensor(0.1764, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4606  batch loss:  tensor(0.0936, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4607  batch loss:  tensor(0.1456, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4608  batch loss:  tensor(0.0587, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4609  batch loss:  tensor(0.1637, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4610  batch loss:  tensor(0.0974, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4611  batch loss:  tensor(0.0917, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4612  batch loss:  tensor(0.1042, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4613  batch loss:  tensor(0.1313, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4614  batch loss:  tensor(0.0586, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4615  batch loss:  tensor(0.1306, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4616  batch loss:  tensor(0.1149, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4617  batch loss:  tensor(0.0687, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4618  batch loss:  tensor(0.0734, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4619  batch loss:  tensor(0.0908, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4620  batch loss:  tensor(0.1525, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4621  batch loss:  tensor(0.0419, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4622  batch loss:  tensor(0.2099, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4623  batch loss:  tensor(0.0649, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4624  batch loss:  tensor(0.0976, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4625  batch loss:  tensor(0.0595, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4626  batch loss:  tensor(0.1473, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4627  batch loss:  tensor(0.0969, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4628  batch loss:  tensor(0.0681, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4629  batch loss:  tensor(0.1606, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4630  batch loss:  tensor(0.0840, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4631  batch loss:  tensor(0.0927, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4632  batch loss:  tensor(0.0801, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4633  batch loss:  tensor(0.1223, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4634  batch loss:  tensor(0.0521, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4635  batch loss:  tensor(0.0449, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4636  batch loss:  tensor(0.0553, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4637  batch loss:  tensor(0.1026, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4638  batch loss:  tensor(0.1041, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4639  batch loss:  tensor(0.0430, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4640  batch loss:  tensor(0.1280, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4641  batch loss:  tensor(0.0622, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4642  batch loss:  tensor(0.0890, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4643  batch loss:  tensor(0.1073, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4644  batch loss:  tensor(0.0854, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4645  batch loss:  tensor(0.0893, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4646  batch loss:  tensor(0.0482, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4647  batch loss:  tensor(0.0804, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4648  batch loss:  tensor(0.0969, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4649  batch loss:  tensor(0.1268, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4650  batch loss:  tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4651  batch loss:  tensor(0.1185, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4652  batch loss:  tensor(0.0968, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4653  batch loss:  tensor(0.0874, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4654  batch loss:  tensor(0.0858, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4655  batch loss:  tensor(0.0695, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4656  batch loss:  tensor(0.0596, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4657  batch loss:  tensor(0.0884, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4658  batch loss:  tensor(0.0684, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4659  batch loss:  tensor(0.0817, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4660  batch loss:  tensor(0.1472, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4661  batch loss:  tensor(0.0439, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4662  batch loss:  tensor(0.1569, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4663  batch loss:  tensor(0.0766, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4664  batch loss:  tensor(0.0706, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4665  batch loss:  tensor(0.0804, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4666  batch loss:  tensor(0.0862, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4667  batch loss:  tensor(0.0902, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4668  batch loss:  tensor(0.0835, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4669  batch loss:  tensor(0.1447, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4670  batch loss:  tensor(0.1512, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4671  batch loss:  tensor(0.1060, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4672  batch loss:  tensor(0.0763, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4673  batch loss:  tensor(0.1145, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4674  batch loss:  tensor(0.0486, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4675  batch loss:  tensor(0.0873, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4676  batch loss:  tensor(0.0908, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4677  batch loss:  tensor(0.0461, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4678  batch loss:  tensor(0.0736, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4679  batch loss:  tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4680  batch loss:  tensor(0.0955, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4681  batch loss:  tensor(0.0680, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4682  batch loss:  tensor(0.0807, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4683  batch loss:  tensor(0.0999, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4684  batch loss:  tensor(0.0305, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4685  batch loss:  tensor(0.0830, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4686  batch loss:  tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4687  batch loss:  tensor(0.1347, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4688  batch loss:  tensor(0.0602, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4689  batch loss:  tensor(0.0838, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4690  batch loss:  tensor(0.0590, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4691  batch loss:  tensor(0.0907, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4692  batch loss:  tensor(0.0581, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4693  batch loss:  tensor(0.1011, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4694  batch loss:  tensor(0.0383, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4695  batch loss:  tensor(0.0961, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4696  batch loss:  tensor(0.0558, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4697  batch loss:  tensor(0.0619, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4698  batch loss:  tensor(0.1254, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4699  batch loss:  tensor(0.0976, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4700  batch loss:  tensor(0.0839, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4701  batch loss:  tensor(0.1065, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4702  batch loss:  tensor(0.0821, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4703  batch loss:  tensor(0.0769, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4704  batch loss:  tensor(0.1696, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4705  batch loss:  tensor(0.0993, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4706  batch loss:  tensor(0.0687, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4707  batch loss:  tensor(0.0792, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4708  batch loss:  tensor(0.1230, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4709  batch loss:  tensor(0.0724, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4710  batch loss:  tensor(0.0863, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4711  batch loss:  tensor(0.1357, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4712  batch loss:  tensor(0.0773, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4713  batch loss:  tensor(0.1428, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4714  batch loss:  tensor(0.1012, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4715  batch loss:  tensor(0.0833, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4716  batch loss:  tensor(0.0982, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4717  batch loss:  tensor(0.0690, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4718  batch loss:  tensor(0.0805, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4719  batch loss:  tensor(0.0695, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4720  batch loss:  tensor(0.1267, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4721  batch loss:  tensor(0.1565, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4722  batch loss:  tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4723  batch loss:  tensor(0.1776, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4724  batch loss:  tensor(0.1433, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4725  batch loss:  tensor(0.1132, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4726  batch loss:  tensor(0.1333, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4727  batch loss:  tensor(0.0762, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4728  batch loss:  tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4729  batch loss:  tensor(0.0764, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4730  batch loss:  tensor(0.1252, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4731  batch loss:  tensor(0.1171, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4732  batch loss:  tensor(0.0906, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4733  batch loss:  tensor(0.0702, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4734  batch loss:  tensor(0.1075, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4735  batch loss:  tensor(0.1122, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4736  batch loss:  tensor(0.0686, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4737  batch loss:  tensor(0.0641, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4738  batch loss:  tensor(0.1525, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4739  batch loss:  tensor(0.0593, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4740  batch loss:  tensor(0.0613, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4741  batch loss:  tensor(0.0928, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4742  batch loss:  tensor(0.0938, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4743  batch loss:  tensor(0.1220, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4744  batch loss:  tensor(0.0667, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4745  batch loss:  tensor(0.0585, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4746  batch loss:  tensor(0.1341, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4747  batch loss:  tensor(0.0724, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4748  batch loss:  tensor(0.0825, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4749  batch loss:  tensor(0.1529, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4750  batch loss:  tensor(0.0698, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4751  batch loss:  tensor(0.1397, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4752  batch loss:  tensor(0.0896, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4753  batch loss:  tensor(0.0831, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4754  batch loss:  tensor(0.1014, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4755  batch loss:  tensor(0.1952, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4756  batch loss:  tensor(0.0610, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4757  batch loss:  tensor(0.0722, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4758  batch loss:  tensor(0.2462, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4759  batch loss:  tensor(0.2269, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4760  batch loss:  tensor(0.0504, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4761  batch loss:  tensor(0.0614, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4762  batch loss:  tensor(0.1566, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4763  batch loss:  tensor(0.0820, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4764  batch loss:  tensor(0.0658, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4765  batch loss:  tensor(0.0599, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4766  batch loss:  tensor(0.0639, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4767  batch loss:  tensor(0.0549, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4768  batch loss:  tensor(0.0770, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4769  batch loss:  tensor(0.0670, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4770  batch loss:  tensor(0.1901, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4771  batch loss:  tensor(0.1124, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4772  batch loss:  tensor(0.1904, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4773  batch loss:  tensor(0.0551, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4774  batch loss:  tensor(0.1199, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4775  batch loss:  tensor(0.0849, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4776  batch loss:  tensor(0.1912, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4777  batch loss:  tensor(0.0894, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4778  batch loss:  tensor(0.0523, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4779  batch loss:  tensor(0.0414, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4780  batch loss:  tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4781  batch loss:  tensor(0.1144, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4782  batch loss:  tensor(0.1073, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4783  batch loss:  tensor(0.1670, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4784  batch loss:  tensor(0.1007, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4785  batch loss:  tensor(0.1034, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4786  batch loss:  tensor(0.0864, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4787  batch loss:  tensor(0.0881, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4788  batch loss:  tensor(0.0805, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4789  batch loss:  tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4790  batch loss:  tensor(0.0920, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4791  batch loss:  tensor(0.1134, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4792  batch loss:  tensor(0.0399, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4793  batch loss:  tensor(0.1357, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4794  batch loss:  tensor(0.0438, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4795  batch loss:  tensor(0.0803, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4796  batch loss:  tensor(0.1407, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4797  batch loss:  tensor(0.0584, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4798  batch loss:  tensor(0.0759, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4799  batch loss:  tensor(0.0851, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4800  batch loss:  tensor(0.0797, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4801  batch loss:  tensor(0.0921, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4802  batch loss:  tensor(0.0947, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4803  batch loss:  tensor(0.0928, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4804  batch loss:  tensor(0.1404, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4805  batch loss:  tensor(0.1305, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4806  batch loss:  tensor(0.0942, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4807  batch loss:  tensor(0.1236, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4808  batch loss:  tensor(0.1322, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4809  batch loss:  tensor(0.0964, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4810  batch loss:  tensor(0.2336, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4811  batch loss:  tensor(0.0458, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4812  batch loss:  tensor(0.0932, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4813  batch loss:  tensor(0.1195, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4814  batch loss:  tensor(0.0816, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4815  batch loss:  tensor(0.0970, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4816  batch loss:  tensor(0.0358, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4817  batch loss:  tensor(0.0595, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4818  batch loss:  tensor(0.1000, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4819  batch loss:  tensor(0.1480, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4820  batch loss:  tensor(0.0443, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4821  batch loss:  tensor(0.0858, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4822  batch loss:  tensor(0.0739, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4823  batch loss:  tensor(0.0996, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4824  batch loss:  tensor(0.0721, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4825  batch loss:  tensor(0.1172, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4826  batch loss:  tensor(0.0739, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4827  batch loss:  tensor(0.0874, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4828  batch loss:  tensor(0.1058, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4829  batch loss:  tensor(0.0673, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4830  batch loss:  tensor(0.0638, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4831  batch loss:  tensor(0.0695, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4832  batch loss:  tensor(0.1144, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4833  batch loss:  tensor(0.0656, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4834  batch loss:  tensor(0.1140, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4835  batch loss:  tensor(0.0747, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4836  batch loss:  tensor(0.1023, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4837  batch loss:  tensor(0.1678, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4838  batch loss:  tensor(0.0969, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4839  batch loss:  tensor(0.0745, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4840  batch loss:  tensor(0.2060, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4841  batch loss:  tensor(0.1598, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4842  batch loss:  tensor(0.1211, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4843  batch loss:  tensor(0.1505, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4844  batch loss:  tensor(0.0864, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4845  batch loss:  tensor(0.0644, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4846  batch loss:  tensor(0.0763, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4847  batch loss:  tensor(0.0893, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4848  batch loss:  tensor(0.0949, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4849  batch loss:  tensor(0.0826, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4850  batch loss:  tensor(0.0965, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4851  batch loss:  tensor(0.0987, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4852  batch loss:  tensor(0.0703, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4853  batch loss:  tensor(0.0643, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4854  batch loss:  tensor(0.0568, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4855  batch loss:  tensor(0.1849, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4856  batch loss:  tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4857  batch loss:  tensor(0.0652, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4858  batch loss:  tensor(0.1526, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4859  batch loss:  tensor(0.0585, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4860  batch loss:  tensor(0.0765, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4861  batch loss:  tensor(0.1034, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4862  batch loss:  tensor(0.0408, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4863  batch loss:  tensor(0.0557, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4864  batch loss:  tensor(0.0906, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4865  batch loss:  tensor(0.0832, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4866  batch loss:  tensor(0.0812, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4867  batch loss:  tensor(0.0622, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4868  batch loss:  tensor(0.1190, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4869  batch loss:  tensor(0.0761, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4870  batch loss:  tensor(0.1368, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4871  batch loss:  tensor(0.0795, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4872  batch loss:  tensor(0.0976, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4873  batch loss:  tensor(0.1070, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4874  batch loss:  tensor(0.0571, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4875  batch loss:  tensor(0.1476, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4876  batch loss:  tensor(0.1292, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4877  batch loss:  tensor(0.1414, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4878  batch loss:  tensor(0.0791, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4879  batch loss:  tensor(0.0720, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4880  batch loss:  tensor(0.1110, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4881  batch loss:  tensor(0.0562, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4882  batch loss:  tensor(0.1671, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4883  batch loss:  tensor(0.0681, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4884  batch loss:  tensor(0.0822, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4885  batch loss:  tensor(0.2011, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4886  batch loss:  tensor(0.0757, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4887  batch loss:  tensor(0.0685, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4888  batch loss:  tensor(0.1274, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4889  batch loss:  tensor(0.1058, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4890  batch loss:  tensor(0.1279, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4891  batch loss:  tensor(0.1496, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4892  batch loss:  tensor(0.0747, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4893  batch loss:  tensor(0.1141, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4894  batch loss:  tensor(0.1447, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4895  batch loss:  tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4896  batch loss:  tensor(0.1601, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4897  batch loss:  tensor(0.0972, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4898  batch loss:  tensor(0.0861, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4899  batch loss:  tensor(0.1082, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4900  batch loss:  tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4901  batch loss:  tensor(0.0956, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4902  batch loss:  tensor(0.0658, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4903  batch loss:  tensor(0.0750, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4904  batch loss:  tensor(0.0520, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4905  batch loss:  tensor(0.1164, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4906  batch loss:  tensor(0.1091, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4907  batch loss:  tensor(0.1601, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4908  batch loss:  tensor(0.1735, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4909  batch loss:  tensor(0.0989, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4910  batch loss:  tensor(0.1480, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4911  batch loss:  tensor(0.0737, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4912  batch loss:  tensor(0.0955, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4913  batch loss:  tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4914  batch loss:  tensor(0.0899, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4915  batch loss:  tensor(0.2041, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4916  batch loss:  tensor(0.0588, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4917  batch loss:  tensor(0.1460, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4918  batch loss:  tensor(0.0655, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4919  batch loss:  tensor(0.0608, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4920  batch loss:  tensor(0.0969, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4921  batch loss:  tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4922  batch loss:  tensor(0.0539, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4923  batch loss:  tensor(0.0906, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4924  batch loss:  tensor(0.2070, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4925  batch loss:  tensor(0.0626, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4926  batch loss:  tensor(0.0683, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4927  batch loss:  tensor(0.1327, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4928  batch loss:  tensor(0.0850, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4929  batch loss:  tensor(0.0927, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4930  batch loss:  tensor(0.0932, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4931  batch loss:  tensor(0.1114, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4932  batch loss:  tensor(0.1544, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4933  batch loss:  tensor(0.0619, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4934  batch loss:  tensor(0.1001, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4935  batch loss:  tensor(0.1009, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4936  batch loss:  tensor(0.1341, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4937  batch loss:  tensor(0.1068, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4938  batch loss:  tensor(0.0668, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4939  batch loss:  tensor(0.0798, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4940  batch loss:  tensor(0.0777, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4941  batch loss:  tensor(0.1460, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4942  batch loss:  tensor(0.1149, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4943  batch loss:  tensor(0.0839, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4944  batch loss:  tensor(0.0909, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4945  batch loss:  tensor(0.1029, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4946  batch loss:  tensor(0.0568, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4947  batch loss:  tensor(0.0658, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4948  batch loss:  tensor(0.1277, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4949  batch loss:  tensor(0.1060, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4950  batch loss:  tensor(0.0917, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4951  batch loss:  tensor(0.0704, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4952  batch loss:  tensor(0.0709, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4953  batch loss:  tensor(0.0660, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4954  batch loss:  tensor(0.1083, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4955  batch loss:  tensor(0.0872, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4956  batch loss:  tensor(0.1008, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4957  batch loss:  tensor(0.0437, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4958  batch loss:  tensor(0.0662, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4959  batch loss:  tensor(0.0797, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4960  batch loss:  tensor(0.1583, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4961  batch loss:  tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4962  batch loss:  tensor(0.1008, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4963  batch loss:  tensor(0.1104, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4964  batch loss:  tensor(0.1734, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4965  batch loss:  tensor(0.0472, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4966  batch loss:  tensor(0.0590, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4967  batch loss:  tensor(0.1096, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4968  batch loss:  tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4969  batch loss:  tensor(0.1329, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4970  batch loss:  tensor(0.0831, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4971  batch loss:  tensor(0.0563, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4972  batch loss:  tensor(0.1540, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4973  batch loss:  tensor(0.0681, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4974  batch loss:  tensor(0.1169, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4975  batch loss:  tensor(0.1065, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4976  batch loss:  tensor(0.0710, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4977  batch loss:  tensor(0.0750, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4978  batch loss:  tensor(0.0988, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4979  batch loss:  tensor(0.0544, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4980  batch loss:  tensor(0.0921, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4981  batch loss:  tensor(0.1144, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4982  batch loss:  tensor(0.0709, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4983  batch loss:  tensor(0.2280, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4984  batch loss:  tensor(0.1065, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4985  batch loss:  tensor(0.1080, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4986  batch loss:  tensor(0.0505, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4987  batch loss:  tensor(0.1247, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4988  batch loss:  tensor(0.0927, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4989  batch loss:  tensor(0.0623, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4990  batch loss:  tensor(0.0687, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4991  batch loss:  tensor(0.1851, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4992  batch loss:  tensor(0.1329, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4993  batch loss:  tensor(0.0896, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4994  batch loss:  tensor(0.1072, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4995  batch loss:  tensor(0.0566, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4996  batch loss:  tensor(0.0793, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4997  batch loss:  tensor(0.0863, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4998  batch loss:  tensor(0.0818, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  4999  batch loss:  tensor(0.1127, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5000  batch loss:  tensor(0.1113, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5001  batch loss:  tensor(0.1101, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5002  batch loss:  tensor(0.1388, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5003  batch loss:  tensor(0.1138, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5004  batch loss:  tensor(0.0677, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5005  batch loss:  tensor(0.0771, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5006  batch loss:  tensor(0.0916, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5007  batch loss:  tensor(0.0989, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5008  batch loss:  tensor(0.1149, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5009  batch loss:  tensor(0.1660, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5010  batch loss:  tensor(0.1025, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5011  batch loss:  tensor(0.0730, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5012  batch loss:  tensor(0.0581, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5013  batch loss:  tensor(0.0385, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5014  batch loss:  tensor(0.1328, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5015  batch loss:  tensor(0.1037, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5016  batch loss:  tensor(0.0677, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5017  batch loss:  tensor(0.1203, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5018  batch loss:  tensor(0.0519, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5019  batch loss:  tensor(0.1697, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5020  batch loss:  tensor(0.0630, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5021  batch loss:  tensor(0.0972, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5022  batch loss:  tensor(0.0889, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5023  batch loss:  tensor(0.1090, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5024  batch loss:  tensor(0.1060, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5025  batch loss:  tensor(0.0630, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5026  batch loss:  tensor(0.0586, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5027  batch loss:  tensor(0.0637, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5028  batch loss:  tensor(0.1133, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5029  batch loss:  tensor(0.1047, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5030  batch loss:  tensor(0.0378, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5031  batch loss:  tensor(0.0755, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5032  batch loss:  tensor(0.0783, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5033  batch loss:  tensor(0.1275, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5034  batch loss:  tensor(0.0529, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5035  batch loss:  tensor(0.0497, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5036  batch loss:  tensor(0.0963, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5037  batch loss:  tensor(0.0843, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5038  batch loss:  tensor(0.0854, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5039  batch loss:  tensor(0.1842, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5040  batch loss:  tensor(0.1965, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5041  batch loss:  tensor(0.0860, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5042  batch loss:  tensor(0.1459, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5043  batch loss:  tensor(0.1341, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5044  batch loss:  tensor(0.1050, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5045  batch loss:  tensor(0.0591, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5046  batch loss:  tensor(0.1927, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5047  batch loss:  tensor(0.1350, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5048  batch loss:  tensor(0.0843, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5049  batch loss:  tensor(0.0725, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5050  batch loss:  tensor(0.1002, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5051  batch loss:  tensor(0.0857, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5052  batch loss:  tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5053  batch loss:  tensor(0.0679, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5054  batch loss:  tensor(0.0741, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5055  batch loss:  tensor(0.0910, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5056  batch loss:  tensor(0.0692, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5057  batch loss:  tensor(0.1165, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5058  batch loss:  tensor(0.1786, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5059  batch loss:  tensor(0.0793, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5060  batch loss:  tensor(0.1147, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5061  batch loss:  tensor(0.1265, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5062  batch loss:  tensor(0.0754, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5063  batch loss:  tensor(0.1472, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5064  batch loss:  tensor(0.0613, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5065  batch loss:  tensor(0.1404, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5066  batch loss:  tensor(0.0600, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5067  batch loss:  tensor(0.1273, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5068  batch loss:  tensor(0.0845, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5069  batch loss:  tensor(0.0864, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5070  batch loss:  tensor(0.1792, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5071  batch loss:  tensor(0.1305, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5072  batch loss:  tensor(0.1058, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5073  batch loss:  tensor(0.0870, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5074  batch loss:  tensor(0.0663, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5075  batch loss:  tensor(0.0929, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5076  batch loss:  tensor(0.1016, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5077  batch loss:  tensor(0.1594, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5078  batch loss:  tensor(0.0728, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5079  batch loss:  tensor(0.0529, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5080  batch loss:  tensor(0.0484, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5081  batch loss:  tensor(0.0909, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5082  batch loss:  tensor(0.0665, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5083  batch loss:  tensor(0.0794, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5084  batch loss:  tensor(0.1504, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5085  batch loss:  tensor(0.1406, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5086  batch loss:  tensor(0.0730, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5087  batch loss:  tensor(0.0642, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5088  batch loss:  tensor(0.1289, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5089  batch loss:  tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5090  batch loss:  tensor(0.1231, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5091  batch loss:  tensor(0.0713, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5092  batch loss:  tensor(0.0707, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5093  batch loss:  tensor(0.0706, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5094  batch loss:  tensor(0.0454, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5095  batch loss:  tensor(0.0794, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5096  batch loss:  tensor(0.0700, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5097  batch loss:  tensor(0.0488, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5098  batch loss:  tensor(0.1169, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5099  batch loss:  tensor(0.1581, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5100  batch loss:  tensor(0.0699, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5101  batch loss:  tensor(0.1272, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5102  batch loss:  tensor(0.1130, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5103  batch loss:  tensor(0.1015, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5104  batch loss:  tensor(0.0409, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5105  batch loss:  tensor(0.0711, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5106  batch loss:  tensor(0.0927, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5107  batch loss:  tensor(0.0435, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5108  batch loss:  tensor(0.0791, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5109  batch loss:  tensor(0.3053, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5110  batch loss:  tensor(0.0412, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5111  batch loss:  tensor(0.1159, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5112  batch loss:  tensor(0.0572, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5113  batch loss:  tensor(0.1158, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5114  batch loss:  tensor(0.1349, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5115  batch loss:  tensor(0.0849, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5116  batch loss:  tensor(0.0964, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5117  batch loss:  tensor(0.0920, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5118  batch loss:  tensor(0.1232, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5119  batch loss:  tensor(0.1101, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5120  batch loss:  tensor(0.0769, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5121  batch loss:  tensor(0.0927, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5122  batch loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5123  batch loss:  tensor(0.0543, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5124  batch loss:  tensor(0.1226, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5125  batch loss:  tensor(0.0785, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5126  batch loss:  tensor(0.1232, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5127  batch loss:  tensor(0.0672, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5128  batch loss:  tensor(0.1583, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5129  batch loss:  tensor(0.0643, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5130  batch loss:  tensor(0.0948, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5131  batch loss:  tensor(0.0875, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5132  batch loss:  tensor(0.0667, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5133  batch loss:  tensor(0.1244, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5134  batch loss:  tensor(0.0720, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5135  batch loss:  tensor(0.1003, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5136  batch loss:  tensor(0.0686, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5137  batch loss:  tensor(0.0963, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5138  batch loss:  tensor(0.1301, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5139  batch loss:  tensor(0.0984, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5140  batch loss:  tensor(0.1229, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5141  batch loss:  tensor(0.0340, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5142  batch loss:  tensor(0.0936, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5143  batch loss:  tensor(0.0630, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5144  batch loss:  tensor(0.0846, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5145  batch loss:  tensor(0.0834, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5146  batch loss:  tensor(0.1357, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5147  batch loss:  tensor(0.2170, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5148  batch loss:  tensor(0.0801, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5149  batch loss:  tensor(0.0785, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5150  batch loss:  tensor(0.1383, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5151  batch loss:  tensor(0.1072, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5152  batch loss:  tensor(0.0872, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5153  batch loss:  tensor(0.0648, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5154  batch loss:  tensor(0.1304, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5155  batch loss:  tensor(0.1588, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5156  batch loss:  tensor(0.1856, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5157  batch loss:  tensor(0.0884, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5158  batch loss:  tensor(0.1252, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5159  batch loss:  tensor(0.0775, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5160  batch loss:  tensor(0.0748, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5161  batch loss:  tensor(0.0968, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5162  batch loss:  tensor(0.0778, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5163  batch loss:  tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5164  batch loss:  tensor(0.1370, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5165  batch loss:  tensor(0.1300, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5166  batch loss:  tensor(0.0905, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5167  batch loss:  tensor(0.1398, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5168  batch loss:  tensor(0.0750, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5169  batch loss:  tensor(0.0847, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5170  batch loss:  tensor(0.1154, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5171  batch loss:  tensor(0.0733, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5172  batch loss:  tensor(0.0494, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5173  batch loss:  tensor(0.0951, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5174  batch loss:  tensor(0.0803, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5175  batch loss:  tensor(0.1068, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5176  batch loss:  tensor(0.0735, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5177  batch loss:  tensor(0.1299, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5178  batch loss:  tensor(0.1339, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5179  batch loss:  tensor(0.1255, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5180  batch loss:  tensor(0.1241, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5181  batch loss:  tensor(0.1008, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5182  batch loss:  tensor(0.1024, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5183  batch loss:  tensor(0.0754, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5184  batch loss:  tensor(0.0998, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5185  batch loss:  tensor(0.0943, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5186  batch loss:  tensor(0.0897, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5187  batch loss:  tensor(0.0712, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5188  batch loss:  tensor(0.0705, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5189  batch loss:  tensor(0.0509, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5190  batch loss:  tensor(0.1364, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5191  batch loss:  tensor(0.1434, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5192  batch loss:  tensor(0.2414, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5193  batch loss:  tensor(0.0979, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5194  batch loss:  tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5195  batch loss:  tensor(0.0875, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5196  batch loss:  tensor(0.0596, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5197  batch loss:  tensor(0.0845, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5198  batch loss:  tensor(0.0838, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5199  batch loss:  tensor(0.0650, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5200  batch loss:  tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5201  batch loss:  tensor(0.1013, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5202  batch loss:  tensor(0.2072, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5203  batch loss:  tensor(0.0643, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5204  batch loss:  tensor(0.0804, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5205  batch loss:  tensor(0.1279, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5206  batch loss:  tensor(0.1151, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5207  batch loss:  tensor(0.1227, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5208  batch loss:  tensor(0.0763, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5209  batch loss:  tensor(0.0654, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5210  batch loss:  tensor(0.0907, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5211  batch loss:  tensor(0.2328, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5212  batch loss:  tensor(0.1139, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5213  batch loss:  tensor(0.0572, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5214  batch loss:  tensor(0.0981, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5215  batch loss:  tensor(0.1574, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5216  batch loss:  tensor(0.0841, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5217  batch loss:  tensor(0.1099, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5218  batch loss:  tensor(0.0568, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5219  batch loss:  tensor(0.1267, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5220  batch loss:  tensor(0.0833, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5221  batch loss:  tensor(0.0704, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5222  batch loss:  tensor(0.0946, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5223  batch loss:  tensor(0.0790, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5224  batch loss:  tensor(0.0656, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5225  batch loss:  tensor(0.0664, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5226  batch loss:  tensor(0.0592, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5227  batch loss:  tensor(0.1553, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5228  batch loss:  tensor(0.0689, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5229  batch loss:  tensor(0.0709, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5230  batch loss:  tensor(0.0601, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5231  batch loss:  tensor(0.0757, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5232  batch loss:  tensor(0.0860, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5233  batch loss:  tensor(0.1403, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5234  batch loss:  tensor(0.0656, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5235  batch loss:  tensor(0.0501, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5236  batch loss:  tensor(0.1351, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5237  batch loss:  tensor(0.1487, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5238  batch loss:  tensor(0.0736, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5239  batch loss:  tensor(0.0567, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5240  batch loss:  tensor(0.0687, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5241  batch loss:  tensor(0.1016, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5242  batch loss:  tensor(0.0430, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5243  batch loss:  tensor(0.0555, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5244  batch loss:  tensor(0.1160, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5245  batch loss:  tensor(0.1807, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5246  batch loss:  tensor(0.0829, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5247  batch loss:  tensor(0.1528, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5248  batch loss:  tensor(0.0846, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5249  batch loss:  tensor(0.0447, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5250  batch loss:  tensor(0.1009, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5251  batch loss:  tensor(0.1254, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5252  batch loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5253  batch loss:  tensor(0.0973, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5254  batch loss:  tensor(0.0600, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5255  batch loss:  tensor(0.0967, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5256  batch loss:  tensor(0.3068, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5257  batch loss:  tensor(0.2454, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5258  batch loss:  tensor(0.0979, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5259  batch loss:  tensor(0.0580, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5260  batch loss:  tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5261  batch loss:  tensor(0.0574, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5262  batch loss:  tensor(0.1204, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5263  batch loss:  tensor(0.0848, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5264  batch loss:  tensor(0.1202, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5265  batch loss:  tensor(0.0938, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5266  batch loss:  tensor(0.0846, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5267  batch loss:  tensor(0.0692, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5268  batch loss:  tensor(0.0852, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5269  batch loss:  tensor(0.0767, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5270  batch loss:  tensor(0.0561, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5271  batch loss:  tensor(0.0934, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5272  batch loss:  tensor(0.0903, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5273  batch loss:  tensor(0.1402, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5274  batch loss:  tensor(0.0772, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5275  batch loss:  tensor(0.1653, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5276  batch loss:  tensor(0.1235, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5277  batch loss:  tensor(0.0874, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5278  batch loss:  tensor(0.2857, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5279  batch loss:  tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5280  batch loss:  tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5281  batch loss:  tensor(0.0616, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5282  batch loss:  tensor(0.0548, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5283  batch loss:  tensor(0.0941, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5284  batch loss:  tensor(0.0661, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5285  batch loss:  tensor(0.0895, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5286  batch loss:  tensor(0.0560, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5287  batch loss:  tensor(0.0823, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5288  batch loss:  tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5289  batch loss:  tensor(0.0785, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5290  batch loss:  tensor(0.0997, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5291  batch loss:  tensor(0.1322, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5292  batch loss:  tensor(0.0694, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5293  batch loss:  tensor(0.0910, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5294  batch loss:  tensor(0.0817, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5295  batch loss:  tensor(0.1083, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5296  batch loss:  tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5297  batch loss:  tensor(0.1667, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5298  batch loss:  tensor(0.0856, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5299  batch loss:  tensor(0.1248, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5300  batch loss:  tensor(0.0972, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5301  batch loss:  tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5302  batch loss:  tensor(0.1011, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5303  batch loss:  tensor(0.1129, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5304  batch loss:  tensor(0.0847, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5305  batch loss:  tensor(0.1944, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5306  batch loss:  tensor(0.0886, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5307  batch loss:  tensor(0.0784, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5308  batch loss:  tensor(0.1400, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5309  batch loss:  tensor(0.0810, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5310  batch loss:  tensor(0.0812, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5311  batch loss:  tensor(0.0616, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5312  batch loss:  tensor(0.0416, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5313  batch loss:  tensor(0.1109, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5314  batch loss:  tensor(0.0558, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5315  batch loss:  tensor(0.0578, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5316  batch loss:  tensor(0.1210, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5317  batch loss:  tensor(0.0916, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5318  batch loss:  tensor(0.1513, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5319  batch loss:  tensor(0.0449, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5320  batch loss:  tensor(0.0932, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5321  batch loss:  tensor(0.1520, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5322  batch loss:  tensor(0.0765, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5323  batch loss:  tensor(0.0380, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5324  batch loss:  tensor(0.1078, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5325  batch loss:  tensor(0.0845, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5326  batch loss:  tensor(0.0646, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5327  batch loss:  tensor(0.1126, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5328  batch loss:  tensor(0.1035, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5329  batch loss:  tensor(0.0598, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5330  batch loss:  tensor(0.0805, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5331  batch loss:  tensor(0.0687, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5332  batch loss:  tensor(0.0818, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5333  batch loss:  tensor(0.0774, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5334  batch loss:  tensor(0.0985, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5335  batch loss:  tensor(0.0821, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5336  batch loss:  tensor(0.0966, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5337  batch loss:  tensor(0.0859, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5338  batch loss:  tensor(0.0844, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5339  batch loss:  tensor(0.1940, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5340  batch loss:  tensor(0.1027, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5341  batch loss:  tensor(0.0915, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5342  batch loss:  tensor(0.1215, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5343  batch loss:  tensor(0.0809, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5344  batch loss:  tensor(0.1133, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5345  batch loss:  tensor(0.1201, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5346  batch loss:  tensor(0.0792, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5347  batch loss:  tensor(0.0558, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5348  batch loss:  tensor(0.0692, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5349  batch loss:  tensor(0.0847, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5350  batch loss:  tensor(0.0551, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5351  batch loss:  tensor(0.1029, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5352  batch loss:  tensor(0.0860, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5353  batch loss:  tensor(0.0790, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5354  batch loss:  tensor(0.1095, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5355  batch loss:  tensor(0.0531, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5356  batch loss:  tensor(0.0660, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5357  batch loss:  tensor(0.0783, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5358  batch loss:  tensor(0.0652, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5359  batch loss:  tensor(0.0878, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5360  batch loss:  tensor(0.1328, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5361  batch loss:  tensor(0.0474, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5362  batch loss:  tensor(0.0763, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5363  batch loss:  tensor(0.0366, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5364  batch loss:  tensor(0.0563, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5365  batch loss:  tensor(0.2051, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5366  batch loss:  tensor(0.1132, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5367  batch loss:  tensor(0.0567, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5368  batch loss:  tensor(0.1895, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5369  batch loss:  tensor(0.0701, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5370  batch loss:  tensor(0.1456, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5371  batch loss:  tensor(0.0527, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5372  batch loss:  tensor(0.0775, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5373  batch loss:  tensor(0.0612, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5374  batch loss:  tensor(0.1152, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5375  batch loss:  tensor(0.0964, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5376  batch loss:  tensor(0.0969, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5377  batch loss:  tensor(0.1065, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5378  batch loss:  tensor(0.1697, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5379  batch loss:  tensor(0.0873, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5380  batch loss:  tensor(0.0844, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5381  batch loss:  tensor(0.0693, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5382  batch loss:  tensor(0.3352, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5383  batch loss:  tensor(0.0947, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5384  batch loss:  tensor(0.0532, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5385  batch loss:  tensor(0.1453, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5386  batch loss:  tensor(0.0763, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5387  batch loss:  tensor(0.1206, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5388  batch loss:  tensor(0.0717, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5389  batch loss:  tensor(0.1028, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5390  batch loss:  tensor(0.0508, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5391  batch loss:  tensor(0.0719, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5392  batch loss:  tensor(0.1169, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5393  batch loss:  tensor(0.0592, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5394  batch loss:  tensor(0.0491, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5395  batch loss:  tensor(0.0791, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5396  batch loss:  tensor(0.1002, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5397  batch loss:  tensor(0.0485, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5398  batch loss:  tensor(0.1640, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5399  batch loss:  tensor(0.0848, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5400  batch loss:  tensor(0.1734, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5401  batch loss:  tensor(0.1287, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5402  batch loss:  tensor(0.2011, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5403  batch loss:  tensor(0.0848, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5404  batch loss:  tensor(0.2708, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5405  batch loss:  tensor(0.1938, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5406  batch loss:  tensor(0.1915, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5407  batch loss:  tensor(0.0517, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5408  batch loss:  tensor(0.1204, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5409  batch loss:  tensor(0.0772, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5410  batch loss:  tensor(0.0975, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5411  batch loss:  tensor(0.1312, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5412  batch loss:  tensor(0.1376, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5413  batch loss:  tensor(0.1006, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5414  batch loss:  tensor(0.0721, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5415  batch loss:  tensor(0.0809, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5416  batch loss:  tensor(0.0643, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5417  batch loss:  tensor(0.1021, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5418  batch loss:  tensor(0.0926, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5419  batch loss:  tensor(0.0513, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5420  batch loss:  tensor(0.1113, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5421  batch loss:  tensor(0.0898, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5422  batch loss:  tensor(0.0748, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5423  batch loss:  tensor(0.0690, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5424  batch loss:  tensor(0.1670, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5425  batch loss:  tensor(0.0766, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5426  batch loss:  tensor(0.1442, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5427  batch loss:  tensor(0.0919, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5428  batch loss:  tensor(0.0948, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5429  batch loss:  tensor(0.2263, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5430  batch loss:  tensor(0.1426, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5431  batch loss:  tensor(0.1095, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5432  batch loss:  tensor(0.0979, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5433  batch loss:  tensor(0.1046, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5434  batch loss:  tensor(0.0966, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5435  batch loss:  tensor(0.0678, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5436  batch loss:  tensor(0.1623, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5437  batch loss:  tensor(0.0709, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5438  batch loss:  tensor(0.0843, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5439  batch loss:  tensor(0.0622, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5440  batch loss:  tensor(0.0896, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5441  batch loss:  tensor(0.0881, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5442  batch loss:  tensor(0.2705, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5443  batch loss:  tensor(0.0599, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5444  batch loss:  tensor(0.0998, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5445  batch loss:  tensor(0.0803, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5446  batch loss:  tensor(0.1929, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5447  batch loss:  tensor(0.0963, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5448  batch loss:  tensor(0.1120, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5449  batch loss:  tensor(0.0711, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5450  batch loss:  tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5451  batch loss:  tensor(0.1234, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5452  batch loss:  tensor(0.0988, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5453  batch loss:  tensor(0.0746, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5454  batch loss:  tensor(0.1037, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5455  batch loss:  tensor(0.0604, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5456  batch loss:  tensor(0.0871, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5457  batch loss:  tensor(0.0662, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5458  batch loss:  tensor(0.0654, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5459  batch loss:  tensor(0.0669, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5460  batch loss:  tensor(0.0999, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5461  batch loss:  tensor(0.0561, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5462  batch loss:  tensor(0.1278, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5463  batch loss:  tensor(0.0780, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5464  batch loss:  tensor(0.0859, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5465  batch loss:  tensor(0.0966, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5466  batch loss:  tensor(0.0520, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5467  batch loss:  tensor(0.0985, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5468  batch loss:  tensor(0.0628, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5469  batch loss:  tensor(0.1079, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5470  batch loss:  tensor(0.0712, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5471  batch loss:  tensor(0.0668, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5472  batch loss:  tensor(0.0503, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5473  batch loss:  tensor(0.1166, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5474  batch loss:  tensor(0.1782, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5475  batch loss:  tensor(0.0619, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5476  batch loss:  tensor(0.3167, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5477  batch loss:  tensor(0.1517, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5478  batch loss:  tensor(0.0622, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5479  batch loss:  tensor(0.1152, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5480  batch loss:  tensor(0.0426, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5481  batch loss:  tensor(0.0678, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5482  batch loss:  tensor(0.1137, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5483  batch loss:  tensor(0.0625, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5484  batch loss:  tensor(0.0774, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5485  batch loss:  tensor(0.0593, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5486  batch loss:  tensor(0.1616, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5487  batch loss:  tensor(0.0953, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5488  batch loss:  tensor(0.1001, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5489  batch loss:  tensor(0.0802, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5490  batch loss:  tensor(0.1274, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5491  batch loss:  tensor(0.0843, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5492  batch loss:  tensor(0.0810, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5493  batch loss:  tensor(0.0690, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5494  batch loss:  tensor(0.0934, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5495  batch loss:  tensor(0.0770, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5496  batch loss:  tensor(0.0971, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5497  batch loss:  tensor(0.1541, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5498  batch loss:  tensor(0.0686, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5499  batch loss:  tensor(0.0653, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5500  batch loss:  tensor(0.0711, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5501  batch loss:  tensor(0.1531, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5502  batch loss:  tensor(0.0700, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5503  batch loss:  tensor(0.0874, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5504  batch loss:  tensor(0.0553, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5505  batch loss:  tensor(0.1233, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5506  batch loss:  tensor(0.1008, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5507  batch loss:  tensor(0.1185, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5508  batch loss:  tensor(0.0679, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5509  batch loss:  tensor(0.0778, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5510  batch loss:  tensor(0.0858, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5511  batch loss:  tensor(0.1045, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5512  batch loss:  tensor(0.0816, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5513  batch loss:  tensor(0.0707, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5514  batch loss:  tensor(0.0479, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5515  batch loss:  tensor(0.0654, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5516  batch loss:  tensor(0.1106, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5517  batch loss:  tensor(0.0828, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5518  batch loss:  tensor(0.0682, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5519  batch loss:  tensor(0.0622, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5520  batch loss:  tensor(0.0791, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5521  batch loss:  tensor(0.0875, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5522  batch loss:  tensor(0.0791, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5523  batch loss:  tensor(0.1913, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5524  batch loss:  tensor(0.1712, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5525  batch loss:  tensor(0.1022, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5526  batch loss:  tensor(0.2180, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5527  batch loss:  tensor(0.1281, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5528  batch loss:  tensor(0.0584, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5529  batch loss:  tensor(0.0705, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5530  batch loss:  tensor(0.1971, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5531  batch loss:  tensor(0.0561, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5532  batch loss:  tensor(0.0701, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5533  batch loss:  tensor(0.0758, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5534  batch loss:  tensor(0.1417, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5535  batch loss:  tensor(0.0513, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5536  batch loss:  tensor(0.1388, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5537  batch loss:  tensor(0.1163, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5538  batch loss:  tensor(0.1149, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5539  batch loss:  tensor(0.0855, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5540  batch loss:  tensor(0.1131, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5541  batch loss:  tensor(0.0801, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5542  batch loss:  tensor(0.0394, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5543  batch loss:  tensor(0.1631, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5544  batch loss:  tensor(0.0786, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5545  batch loss:  tensor(0.1018, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5546  batch loss:  tensor(0.1681, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5547  batch loss:  tensor(0.1121, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5548  batch loss:  tensor(0.1590, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5549  batch loss:  tensor(0.1123, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5550  batch loss:  tensor(0.1257, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5551  batch loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5552  batch loss:  tensor(0.0796, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5553  batch loss:  tensor(0.0691, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5554  batch loss:  tensor(0.1076, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5555  batch loss:  tensor(0.0926, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5556  batch loss:  tensor(0.0733, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5557  batch loss:  tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5558  batch loss:  tensor(0.0598, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5559  batch loss:  tensor(0.0953, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5560  batch loss:  tensor(0.0826, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5561  batch loss:  tensor(0.0841, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5562  batch loss:  tensor(0.0741, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5563  batch loss:  tensor(0.0590, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5564  batch loss:  tensor(0.0608, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5565  batch loss:  tensor(0.1171, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5566  batch loss:  tensor(0.1070, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5567  batch loss:  tensor(0.0598, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5568  batch loss:  tensor(0.0929, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5569  batch loss:  tensor(0.1082, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5570  batch loss:  tensor(0.0810, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5571  batch loss:  tensor(0.0751, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5572  batch loss:  tensor(0.1278, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5573  batch loss:  tensor(0.2465, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5574  batch loss:  tensor(0.1140, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5575  batch loss:  tensor(0.1272, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5576  batch loss:  tensor(0.1083, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5577  batch loss:  tensor(0.0960, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5578  batch loss:  tensor(0.1358, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5579  batch loss:  tensor(0.0590, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5580  batch loss:  tensor(0.0859, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5581  batch loss:  tensor(0.0904, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5582  batch loss:  tensor(0.1726, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5583  batch loss:  tensor(0.0728, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5584  batch loss:  tensor(0.1126, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5585  batch loss:  tensor(0.1223, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5586  batch loss:  tensor(0.1159, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5587  batch loss:  tensor(0.1466, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5588  batch loss:  tensor(0.0966, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5589  batch loss:  tensor(0.0745, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5590  batch loss:  tensor(0.1023, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5591  batch loss:  tensor(0.0754, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5592  batch loss:  tensor(0.0951, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5593  batch loss:  tensor(0.1483, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5594  batch loss:  tensor(0.0680, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5595  batch loss:  tensor(0.0776, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5596  batch loss:  tensor(0.0820, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5597  batch loss:  tensor(0.0781, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5598  batch loss:  tensor(0.0709, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5599  batch loss:  tensor(0.0675, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5600  batch loss:  tensor(0.1020, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5601  batch loss:  tensor(0.0845, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5602  batch loss:  tensor(0.0517, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5603  batch loss:  tensor(0.1154, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5604  batch loss:  tensor(0.0917, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5605  batch loss:  tensor(0.0788, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5606  batch loss:  tensor(0.0617, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5607  batch loss:  tensor(0.1152, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5608  batch loss:  tensor(0.0681, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5609  batch loss:  tensor(0.0994, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5610  batch loss:  tensor(0.0566, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5611  batch loss:  tensor(0.0509, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5612  batch loss:  tensor(0.0935, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5613  batch loss:  tensor(0.1057, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5614  batch loss:  tensor(0.0758, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5615  batch loss:  tensor(0.2235, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5616  batch loss:  tensor(0.0575, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5617  batch loss:  tensor(0.0813, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5618  batch loss:  tensor(0.1064, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5619  batch loss:  tensor(0.1513, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5620  batch loss:  tensor(0.1556, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5621  batch loss:  tensor(0.1368, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5622  batch loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5623  batch loss:  tensor(0.0681, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5624  batch loss:  tensor(0.1081, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5625  batch loss:  tensor(0.1007, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5626  batch loss:  tensor(0.1237, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5627  batch loss:  tensor(0.0525, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5628  batch loss:  tensor(0.0527, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5629  batch loss:  tensor(0.0549, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5630  batch loss:  tensor(0.0719, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5631  batch loss:  tensor(0.1412, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5632  batch loss:  tensor(0.0980, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5633  batch loss:  tensor(0.1637, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5634  batch loss:  tensor(0.2081, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5635  batch loss:  tensor(0.0588, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5636  batch loss:  tensor(0.0503, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5637  batch loss:  tensor(0.0937, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5638  batch loss:  tensor(0.0491, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5639  batch loss:  tensor(0.1129, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5640  batch loss:  tensor(0.1270, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5641  batch loss:  tensor(0.1109, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5642  batch loss:  tensor(0.1138, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5643  batch loss:  tensor(0.0721, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5644  batch loss:  tensor(0.0773, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5645  batch loss:  tensor(0.1137, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5646  batch loss:  tensor(0.1242, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5647  batch loss:  tensor(0.0878, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5648  batch loss:  tensor(0.0635, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5649  batch loss:  tensor(0.0895, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5650  batch loss:  tensor(0.2058, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5651  batch loss:  tensor(0.0528, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5652  batch loss:  tensor(0.0675, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5653  batch loss:  tensor(0.0559, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5654  batch loss:  tensor(0.1672, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5655  batch loss:  tensor(0.0798, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5656  batch loss:  tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5657  batch loss:  tensor(0.0816, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5658  batch loss:  tensor(0.0787, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5659  batch loss:  tensor(0.0716, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5660  batch loss:  tensor(0.0397, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5661  batch loss:  tensor(0.0607, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5662  batch loss:  tensor(0.0771, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5663  batch loss:  tensor(0.1364, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5664  batch loss:  tensor(0.1283, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5665  batch loss:  tensor(0.0808, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5666  batch loss:  tensor(0.1999, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5667  batch loss:  tensor(0.0713, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5668  batch loss:  tensor(0.0937, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5669  batch loss:  tensor(0.0901, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5670  batch loss:  tensor(0.0966, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5671  batch loss:  tensor(0.0513, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5672  batch loss:  tensor(0.0691, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5673  batch loss:  tensor(0.0390, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5674  batch loss:  tensor(0.0777, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5675  batch loss:  tensor(0.1028, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5676  batch loss:  tensor(0.1248, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5677  batch loss:  tensor(0.2403, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5678  batch loss:  tensor(0.1406, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5679  batch loss:  tensor(0.1292, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5680  batch loss:  tensor(0.1366, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5681  batch loss:  tensor(0.1099, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5682  batch loss:  tensor(0.1006, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5683  batch loss:  tensor(0.1286, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5684  batch loss:  tensor(0.1841, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5685  batch loss:  tensor(0.1125, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5686  batch loss:  tensor(0.1057, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5687  batch loss:  tensor(0.0272, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5688  batch loss:  tensor(0.0576, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5689  batch loss:  tensor(0.0721, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5690  batch loss:  tensor(0.1875, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5691  batch loss:  tensor(0.0958, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5692  batch loss:  tensor(0.0624, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5693  batch loss:  tensor(0.0929, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5694  batch loss:  tensor(0.1574, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5695  batch loss:  tensor(0.0706, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5696  batch loss:  tensor(0.0874, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5697  batch loss:  tensor(0.0877, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5698  batch loss:  tensor(0.0658, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5699  batch loss:  tensor(0.1069, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5700  batch loss:  tensor(0.1843, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5701  batch loss:  tensor(0.0652, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5702  batch loss:  tensor(0.0908, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5703  batch loss:  tensor(0.1360, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5704  batch loss:  tensor(0.0865, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5705  batch loss:  tensor(0.0852, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5706  batch loss:  tensor(0.0901, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5707  batch loss:  tensor(0.1779, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5708  batch loss:  tensor(0.0729, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5709  batch loss:  tensor(0.0785, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5710  batch loss:  tensor(0.0784, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5711  batch loss:  tensor(0.0970, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5712  batch loss:  tensor(0.0952, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5713  batch loss:  tensor(0.0591, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5714  batch loss:  tensor(0.1502, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5715  batch loss:  tensor(0.0954, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5716  batch loss:  tensor(0.0599, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5717  batch loss:  tensor(0.1697, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5718  batch loss:  tensor(0.0972, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5719  batch loss:  tensor(0.0742, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5720  batch loss:  tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5721  batch loss:  tensor(0.0988, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5722  batch loss:  tensor(0.3337, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5723  batch loss:  tensor(0.3047, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5724  batch loss:  tensor(0.1281, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5725  batch loss:  tensor(0.0792, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5726  batch loss:  tensor(0.0616, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5727  batch loss:  tensor(0.0626, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5728  batch loss:  tensor(0.0948, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5729  batch loss:  tensor(0.1198, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5730  batch loss:  tensor(0.0465, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5731  batch loss:  tensor(0.1274, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5732  batch loss:  tensor(0.1161, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5733  batch loss:  tensor(0.1455, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5734  batch loss:  tensor(0.1020, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5735  batch loss:  tensor(0.0761, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5736  batch loss:  tensor(0.0760, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5737  batch loss:  tensor(0.1143, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5738  batch loss:  tensor(0.0975, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5739  batch loss:  tensor(0.1591, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5740  batch loss:  tensor(0.0711, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5741  batch loss:  tensor(0.1153, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5742  batch loss:  tensor(0.0855, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5743  batch loss:  tensor(0.1088, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5744  batch loss:  tensor(0.1316, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5745  batch loss:  tensor(0.1512, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5746  batch loss:  tensor(0.1398, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5747  batch loss:  tensor(0.0615, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5748  batch loss:  tensor(0.0523, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5749  batch loss:  tensor(0.0852, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5750  batch loss:  tensor(0.0868, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5751  batch loss:  tensor(0.2140, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5752  batch loss:  tensor(0.1707, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5753  batch loss:  tensor(0.0922, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5754  batch loss:  tensor(0.0772, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5755  batch loss:  tensor(0.0617, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5756  batch loss:  tensor(0.1654, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5757  batch loss:  tensor(0.0976, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5758  batch loss:  tensor(0.1641, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5759  batch loss:  tensor(0.1100, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5760  batch loss:  tensor(0.0806, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5761  batch loss:  tensor(0.0822, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5762  batch loss:  tensor(0.0838, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5763  batch loss:  tensor(0.1007, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5764  batch loss:  tensor(0.0920, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5765  batch loss:  tensor(0.0890, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5766  batch loss:  tensor(0.1016, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5767  batch loss:  tensor(0.0849, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5768  batch loss:  tensor(0.0739, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5769  batch loss:  tensor(0.0727, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5770  batch loss:  tensor(0.0829, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5771  batch loss:  tensor(0.1000, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5772  batch loss:  tensor(0.1532, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5773  batch loss:  tensor(0.0561, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5774  batch loss:  tensor(0.0616, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5775  batch loss:  tensor(0.0666, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5776  batch loss:  tensor(0.0706, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5777  batch loss:  tensor(0.0961, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5778  batch loss:  tensor(0.0748, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5779  batch loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5780  batch loss:  tensor(0.2102, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5781  batch loss:  tensor(0.1450, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5782  batch loss:  tensor(0.0970, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5783  batch loss:  tensor(0.1032, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5784  batch loss:  tensor(0.1023, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5785  batch loss:  tensor(0.1385, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5786  batch loss:  tensor(0.1045, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5787  batch loss:  tensor(0.0806, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5788  batch loss:  tensor(0.0857, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5789  batch loss:  tensor(0.2909, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5790  batch loss:  tensor(0.1014, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5791  batch loss:  tensor(0.0588, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5792  batch loss:  tensor(0.0746, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5793  batch loss:  tensor(0.1317, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5794  batch loss:  tensor(0.0884, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5795  batch loss:  tensor(0.1106, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5796  batch loss:  tensor(0.1194, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5797  batch loss:  tensor(0.1152, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5798  batch loss:  tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5799  batch loss:  tensor(0.0539, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5800  batch loss:  tensor(0.1654, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5801  batch loss:  tensor(0.0837, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5802  batch loss:  tensor(0.0610, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5803  batch loss:  tensor(0.1599, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5804  batch loss:  tensor(0.0946, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5805  batch loss:  tensor(0.2369, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5806  batch loss:  tensor(0.0858, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5807  batch loss:  tensor(0.1133, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5808  batch loss:  tensor(0.1096, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5809  batch loss:  tensor(0.0574, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5810  batch loss:  tensor(0.1237, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5811  batch loss:  tensor(0.0787, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5812  batch loss:  tensor(0.1665, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5813  batch loss:  tensor(0.1898, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5814  batch loss:  tensor(0.0766, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5815  batch loss:  tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5816  batch loss:  tensor(0.1130, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5817  batch loss:  tensor(0.1041, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5818  batch loss:  tensor(0.1764, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5819  batch loss:  tensor(0.0846, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5820  batch loss:  tensor(0.1164, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5821  batch loss:  tensor(0.1196, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5822  batch loss:  tensor(0.0680, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5823  batch loss:  tensor(0.0954, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5824  batch loss:  tensor(0.1110, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5825  batch loss:  tensor(0.1068, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5826  batch loss:  tensor(0.0863, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5827  batch loss:  tensor(0.1139, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5828  batch loss:  tensor(0.0759, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5829  batch loss:  tensor(0.0925, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5830  batch loss:  tensor(0.1112, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5831  batch loss:  tensor(0.1257, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5832  batch loss:  tensor(0.2678, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5833  batch loss:  tensor(0.0409, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5834  batch loss:  tensor(0.0640, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5835  batch loss:  tensor(0.0589, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5836  batch loss:  tensor(0.0695, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5837  batch loss:  tensor(0.1400, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5838  batch loss:  tensor(0.0603, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5839  batch loss:  tensor(0.1166, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5840  batch loss:  tensor(0.0491, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5841  batch loss:  tensor(0.0937, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5842  batch loss:  tensor(0.0814, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5843  batch loss:  tensor(0.0557, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5844  batch loss:  tensor(0.1500, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5845  batch loss:  tensor(0.0665, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5846  batch loss:  tensor(0.1811, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5847  batch loss:  tensor(0.0827, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5848  batch loss:  tensor(0.1658, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5849  batch loss:  tensor(0.1507, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5850  batch loss:  tensor(0.0716, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5851  batch loss:  tensor(0.0588, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5852  batch loss:  tensor(0.0532, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5853  batch loss:  tensor(0.0616, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5854  batch loss:  tensor(0.0667, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5855  batch loss:  tensor(0.0979, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5856  batch loss:  tensor(0.0938, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5857  batch loss:  tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5858  batch loss:  tensor(0.0630, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5859  batch loss:  tensor(0.1341, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5860  batch loss:  tensor(0.0739, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5861  batch loss:  tensor(0.1072, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5862  batch loss:  tensor(0.0815, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5863  batch loss:  tensor(0.1972, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5864  batch loss:  tensor(0.1313, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5865  batch loss:  tensor(0.0865, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5866  batch loss:  tensor(0.1097, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5867  batch loss:  tensor(0.1613, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5868  batch loss:  tensor(0.0805, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5869  batch loss:  tensor(0.0737, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5870  batch loss:  tensor(0.1251, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5871  batch loss:  tensor(0.1183, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5872  batch loss:  tensor(0.0816, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5873  batch loss:  tensor(0.1108, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5874  batch loss:  tensor(0.1397, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5875  batch loss:  tensor(0.0736, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5876  batch loss:  tensor(0.0967, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5877  batch loss:  tensor(0.1434, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5878  batch loss:  tensor(0.0958, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5879  batch loss:  tensor(0.0987, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5880  batch loss:  tensor(0.0895, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5881  batch loss:  tensor(0.1557, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5882  batch loss:  tensor(0.1343, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5883  batch loss:  tensor(0.1538, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5884  batch loss:  tensor(0.0594, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5885  batch loss:  tensor(0.0863, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5886  batch loss:  tensor(0.0924, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5887  batch loss:  tensor(0.1126, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5888  batch loss:  tensor(0.0884, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5889  batch loss:  tensor(0.1408, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5890  batch loss:  tensor(0.0942, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5891  batch loss:  tensor(0.1000, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5892  batch loss:  tensor(0.1571, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5893  batch loss:  tensor(0.0714, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5894  batch loss:  tensor(0.0746, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5895  batch loss:  tensor(0.0856, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5896  batch loss:  tensor(0.2261, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5897  batch loss:  tensor(0.1572, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5898  batch loss:  tensor(0.1072, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5899  batch loss:  tensor(0.0360, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5900  batch loss:  tensor(0.1017, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5901  batch loss:  tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5902  batch loss:  tensor(0.0615, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5903  batch loss:  tensor(0.1295, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5904  batch loss:  tensor(0.1119, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5905  batch loss:  tensor(0.0761, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5906  batch loss:  tensor(0.1549, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5907  batch loss:  tensor(0.0514, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5908  batch loss:  tensor(0.1115, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5909  batch loss:  tensor(0.1121, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5910  batch loss:  tensor(0.1163, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5911  batch loss:  tensor(0.1473, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5912  batch loss:  tensor(0.1037, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5913  batch loss:  tensor(0.0876, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5914  batch loss:  tensor(0.1180, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5915  batch loss:  tensor(0.0775, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5916  batch loss:  tensor(0.0628, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5917  batch loss:  tensor(0.0700, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5918  batch loss:  tensor(0.1068, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5919  batch loss:  tensor(0.0921, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5920  batch loss:  tensor(0.0520, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5921  batch loss:  tensor(0.0654, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5922  batch loss:  tensor(0.0484, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5923  batch loss:  tensor(0.1208, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5924  batch loss:  tensor(0.0426, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5925  batch loss:  tensor(0.0792, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5926  batch loss:  tensor(0.0724, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5927  batch loss:  tensor(0.0345, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5928  batch loss:  tensor(0.0705, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5929  batch loss:  tensor(0.0945, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5930  batch loss:  tensor(0.0956, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5931  batch loss:  tensor(0.0649, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5932  batch loss:  tensor(0.1829, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5933  batch loss:  tensor(0.0604, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5934  batch loss:  tensor(0.0512, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5935  batch loss:  tensor(0.0896, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5936  batch loss:  tensor(0.1205, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5937  batch loss:  tensor(0.1162, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5938  batch loss:  tensor(0.1210, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5939  batch loss:  tensor(0.1627, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5940  batch loss:  tensor(0.1206, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5941  batch loss:  tensor(0.1224, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5942  batch loss:  tensor(0.1127, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5943  batch loss:  tensor(0.1142, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5944  batch loss:  tensor(0.0534, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5945  batch loss:  tensor(0.1992, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5946  batch loss:  tensor(0.0660, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5947  batch loss:  tensor(0.1096, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5948  batch loss:  tensor(0.1207, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5949  batch loss:  tensor(0.0653, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5950  batch loss:  tensor(0.0624, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5951  batch loss:  tensor(0.0578, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5952  batch loss:  tensor(0.0766, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5953  batch loss:  tensor(0.0850, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5954  batch loss:  tensor(0.0815, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5955  batch loss:  tensor(0.0595, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5956  batch loss:  tensor(0.1178, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5957  batch loss:  tensor(0.0847, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5958  batch loss:  tensor(0.2229, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5959  batch loss:  tensor(0.0662, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5960  batch loss:  tensor(0.1111, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5961  batch loss:  tensor(0.0755, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5962  batch loss:  tensor(0.1500, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5963  batch loss:  tensor(0.1084, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5964  batch loss:  tensor(0.1155, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5965  batch loss:  tensor(0.1024, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5966  batch loss:  tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5967  batch loss:  tensor(0.0965, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5968  batch loss:  tensor(0.0993, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5969  batch loss:  tensor(0.0788, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5970  batch loss:  tensor(0.0522, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5971  batch loss:  tensor(0.1551, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5972  batch loss:  tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5973  batch loss:  tensor(0.0588, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5974  batch loss:  tensor(0.0979, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5975  batch loss:  tensor(0.1215, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5976  batch loss:  tensor(0.0654, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5977  batch loss:  tensor(0.1265, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5978  batch loss:  tensor(0.0865, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5979  batch loss:  tensor(0.2220, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5980  batch loss:  tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5981  batch loss:  tensor(0.2291, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5982  batch loss:  tensor(0.0576, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5983  batch loss:  tensor(0.0534, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5984  batch loss:  tensor(0.0839, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5985  batch loss:  tensor(0.0716, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5986  batch loss:  tensor(0.0744, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5987  batch loss:  tensor(0.1038, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5988  batch loss:  tensor(0.1777, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5989  batch loss:  tensor(0.0500, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5990  batch loss:  tensor(0.0339, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5991  batch loss:  tensor(0.0751, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5992  batch loss:  tensor(0.1226, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5993  batch loss:  tensor(0.0882, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5994  batch loss:  tensor(0.0534, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5995  batch loss:  tensor(0.0859, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5996  batch loss:  tensor(0.1653, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5997  batch loss:  tensor(0.0684, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5998  batch loss:  tensor(0.1059, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  5999  batch loss:  tensor(0.1116, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6000  batch loss:  tensor(0.0515, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6001  batch loss:  tensor(0.0851, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6002  batch loss:  tensor(0.0538, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6003  batch loss:  tensor(0.0773, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6004  batch loss:  tensor(0.0720, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6005  batch loss:  tensor(0.0772, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6006  batch loss:  tensor(0.1164, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6007  batch loss:  tensor(0.0894, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6008  batch loss:  tensor(0.1303, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6009  batch loss:  tensor(0.1445, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6010  batch loss:  tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6011  batch loss:  tensor(0.1113, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6012  batch loss:  tensor(0.1149, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6013  batch loss:  tensor(0.1205, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6014  batch loss:  tensor(0.0671, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6015  batch loss:  tensor(0.0856, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6016  batch loss:  tensor(0.0928, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6017  batch loss:  tensor(0.2139, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6018  batch loss:  tensor(0.0684, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6019  batch loss:  tensor(0.0556, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6020  batch loss:  tensor(0.0703, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6021  batch loss:  tensor(0.1022, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6022  batch loss:  tensor(0.1009, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6023  batch loss:  tensor(0.0539, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6024  batch loss:  tensor(0.1223, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6025  batch loss:  tensor(0.1176, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6026  batch loss:  tensor(0.0839, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6027  batch loss:  tensor(0.0848, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6028  batch loss:  tensor(0.0672, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6029  batch loss:  tensor(0.0922, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6030  batch loss:  tensor(0.0524, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6031  batch loss:  tensor(0.0830, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6032  batch loss:  tensor(0.1588, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6033  batch loss:  tensor(0.1573, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6034  batch loss:  tensor(0.0833, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6035  batch loss:  tensor(0.0518, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6036  batch loss:  tensor(0.0855, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6037  batch loss:  tensor(0.1232, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6038  batch loss:  tensor(0.0984, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6039  batch loss:  tensor(0.0797, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6040  batch loss:  tensor(0.0859, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6041  batch loss:  tensor(0.1028, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6042  batch loss:  tensor(0.0997, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6043  batch loss:  tensor(0.0739, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6044  batch loss:  tensor(0.1511, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6045  batch loss:  tensor(0.0834, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6046  batch loss:  tensor(0.1075, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6047  batch loss:  tensor(0.0591, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6048  batch loss:  tensor(0.1839, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6049  batch loss:  tensor(0.0767, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6050  batch loss:  tensor(0.0716, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6051  batch loss:  tensor(0.0730, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6052  batch loss:  tensor(0.0667, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6053  batch loss:  tensor(0.1086, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6054  batch loss:  tensor(0.1003, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6055  batch loss:  tensor(0.1151, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6056  batch loss:  tensor(0.1109, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6057  batch loss:  tensor(0.0605, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6058  batch loss:  tensor(0.1336, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6059  batch loss:  tensor(0.0937, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6060  batch loss:  tensor(0.0434, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6061  batch loss:  tensor(0.1140, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6062  batch loss:  tensor(0.1057, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6063  batch loss:  tensor(0.0795, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6064  batch loss:  tensor(0.1368, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6065  batch loss:  tensor(0.1118, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6066  batch loss:  tensor(0.1073, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6067  batch loss:  tensor(0.0754, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6068  batch loss:  tensor(0.1364, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6069  batch loss:  tensor(0.1012, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6070  batch loss:  tensor(0.0670, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6071  batch loss:  tensor(0.0763, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6072  batch loss:  tensor(0.1291, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6073  batch loss:  tensor(0.0860, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6074  batch loss:  tensor(0.1554, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6075  batch loss:  tensor(0.0821, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6076  batch loss:  tensor(0.0786, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6077  batch loss:  tensor(0.0671, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6078  batch loss:  tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6079  batch loss:  tensor(0.0651, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6080  batch loss:  tensor(0.0878, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6081  batch loss:  tensor(0.0591, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6082  batch loss:  tensor(0.0757, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6083  batch loss:  tensor(0.0613, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6084  batch loss:  tensor(0.0795, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6085  batch loss:  tensor(0.1106, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6086  batch loss:  tensor(0.0863, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6087  batch loss:  tensor(0.0574, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6088  batch loss:  tensor(0.0870, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6089  batch loss:  tensor(0.0970, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6090  batch loss:  tensor(0.1494, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6091  batch loss:  tensor(0.0899, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6092  batch loss:  tensor(0.1320, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6093  batch loss:  tensor(0.1341, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6094  batch loss:  tensor(0.0664, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6095  batch loss:  tensor(0.0827, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6096  batch loss:  tensor(0.0806, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6097  batch loss:  tensor(0.0820, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6098  batch loss:  tensor(0.1042, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6099  batch loss:  tensor(0.1222, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6100  batch loss:  tensor(0.1045, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6101  batch loss:  tensor(0.1328, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6102  batch loss:  tensor(0.0314, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6103  batch loss:  tensor(0.0492, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6104  batch loss:  tensor(0.0671, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6105  batch loss:  tensor(0.0895, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6106  batch loss:  tensor(0.1026, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6107  batch loss:  tensor(0.1034, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6108  batch loss:  tensor(0.0948, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6109  batch loss:  tensor(0.0480, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6110  batch loss:  tensor(0.1449, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6111  batch loss:  tensor(0.0951, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6112  batch loss:  tensor(0.1890, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6113  batch loss:  tensor(0.1336, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6114  batch loss:  tensor(0.0657, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6115  batch loss:  tensor(0.0689, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6116  batch loss:  tensor(0.2147, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6117  batch loss:  tensor(0.1309, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6118  batch loss:  tensor(0.2662, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6119  batch loss:  tensor(0.0724, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6120  batch loss:  tensor(0.1350, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6121  batch loss:  tensor(0.1454, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6122  batch loss:  tensor(0.0534, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6123  batch loss:  tensor(0.1094, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6124  batch loss:  tensor(0.0773, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6125  batch loss:  tensor(0.0831, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6126  batch loss:  tensor(0.2656, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6127  batch loss:  tensor(0.0612, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6128  batch loss:  tensor(0.0592, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6129  batch loss:  tensor(0.0954, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6130  batch loss:  tensor(0.0931, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6131  batch loss:  tensor(0.0812, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6132  batch loss:  tensor(0.0915, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6133  batch loss:  tensor(0.0896, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6134  batch loss:  tensor(0.0603, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6135  batch loss:  tensor(0.0981, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6136  batch loss:  tensor(0.0960, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6137  batch loss:  tensor(0.1077, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6138  batch loss:  tensor(0.0786, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6139  batch loss:  tensor(0.1137, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6140  batch loss:  tensor(0.0401, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6141  batch loss:  tensor(0.1128, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6142  batch loss:  tensor(0.2385, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6143  batch loss:  tensor(0.1803, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6144  batch loss:  tensor(0.0935, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6145  batch loss:  tensor(0.1225, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6146  batch loss:  tensor(0.1865, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6147  batch loss:  tensor(0.0918, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6148  batch loss:  tensor(0.0853, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6149  batch loss:  tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6150  batch loss:  tensor(0.0911, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6151  batch loss:  tensor(0.0802, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6152  batch loss:  tensor(0.1019, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6153  batch loss:  tensor(0.0885, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6154  batch loss:  tensor(0.0440, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6155  batch loss:  tensor(0.1462, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6156  batch loss:  tensor(0.0459, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6157  batch loss:  tensor(0.1139, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6158  batch loss:  tensor(0.0488, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6159  batch loss:  tensor(0.0956, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6160  batch loss:  tensor(0.1368, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6161  batch loss:  tensor(0.0703, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6162  batch loss:  tensor(0.0590, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6163  batch loss:  tensor(0.0907, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6164  batch loss:  tensor(0.0590, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6165  batch loss:  tensor(0.0562, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6166  batch loss:  tensor(0.1655, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6167  batch loss:  tensor(0.0547, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6168  batch loss:  tensor(0.0653, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6169  batch loss:  tensor(0.1207, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6170  batch loss:  tensor(0.0982, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6171  batch loss:  tensor(0.1663, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6172  batch loss:  tensor(0.1152, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6173  batch loss:  tensor(0.0724, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6174  batch loss:  tensor(0.1885, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6175  batch loss:  tensor(0.1346, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6176  batch loss:  tensor(0.1204, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6177  batch loss:  tensor(0.0746, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6178  batch loss:  tensor(0.0634, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6179  batch loss:  tensor(0.0456, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6180  batch loss:  tensor(0.1184, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6181  batch loss:  tensor(0.0624, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6182  batch loss:  tensor(0.1170, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6183  batch loss:  tensor(0.1142, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6184  batch loss:  tensor(0.1137, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6185  batch loss:  tensor(0.1387, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6186  batch loss:  tensor(0.1865, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6187  batch loss:  tensor(0.1219, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6188  batch loss:  tensor(0.0718, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6189  batch loss:  tensor(0.0795, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6190  batch loss:  tensor(0.0725, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6191  batch loss:  tensor(0.0832, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6192  batch loss:  tensor(0.1047, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6193  batch loss:  tensor(0.1598, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6194  batch loss:  tensor(0.1640, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6195  batch loss:  tensor(0.1440, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6196  batch loss:  tensor(0.0826, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6197  batch loss:  tensor(0.1105, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6198  batch loss:  tensor(0.1713, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6199  batch loss:  tensor(0.1184, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6200  batch loss:  tensor(0.0753, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6201  batch loss:  tensor(0.1134, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6202  batch loss:  tensor(0.0468, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6203  batch loss:  tensor(0.1203, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6204  batch loss:  tensor(0.1572, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6205  batch loss:  tensor(0.0949, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6206  batch loss:  tensor(0.0582, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6207  batch loss:  tensor(0.1159, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6208  batch loss:  tensor(0.1779, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6209  batch loss:  tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6210  batch loss:  tensor(0.0585, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6211  batch loss:  tensor(0.0604, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6212  batch loss:  tensor(0.0897, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6213  batch loss:  tensor(0.1422, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6214  batch loss:  tensor(0.1011, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6215  batch loss:  tensor(0.0472, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6216  batch loss:  tensor(0.1347, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6217  batch loss:  tensor(0.0721, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6218  batch loss:  tensor(0.1038, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6219  batch loss:  tensor(0.1192, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6220  batch loss:  tensor(0.0875, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6221  batch loss:  tensor(0.0746, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6222  batch loss:  tensor(0.1139, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6223  batch loss:  tensor(0.0598, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6224  batch loss:  tensor(0.1102, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6225  batch loss:  tensor(0.0898, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6226  batch loss:  tensor(0.0771, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6227  batch loss:  tensor(0.1549, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6228  batch loss:  tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6229  batch loss:  tensor(0.0812, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6230  batch loss:  tensor(0.1291, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6231  batch loss:  tensor(0.0436, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6232  batch loss:  tensor(0.1521, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6233  batch loss:  tensor(0.0716, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6234  batch loss:  tensor(0.1230, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6235  batch loss:  tensor(0.1839, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6236  batch loss:  tensor(0.0925, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6237  batch loss:  tensor(0.0686, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6238  batch loss:  tensor(0.1780, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6239  batch loss:  tensor(0.1031, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6240  batch loss:  tensor(0.1029, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6241  batch loss:  tensor(0.0685, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6242  batch loss:  tensor(0.0883, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6243  batch loss:  tensor(0.0780, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6244  batch loss:  tensor(0.0872, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6245  batch loss:  tensor(0.0710, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6246  batch loss:  tensor(0.0821, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6247  batch loss:  tensor(0.0762, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6248  batch loss:  tensor(0.1196, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6249  batch loss:  tensor(0.0978, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6250  batch loss:  tensor(0.1352, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6251  batch loss:  tensor(0.0870, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6252  batch loss:  tensor(0.0989, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6253  batch loss:  tensor(0.1235, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6254  batch loss:  tensor(0.0543, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6255  batch loss:  tensor(0.1305, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6256  batch loss:  tensor(0.1137, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6257  batch loss:  tensor(0.0731, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6258  batch loss:  tensor(0.1055, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6259  batch loss:  tensor(0.1012, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6260  batch loss:  tensor(0.0930, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6261  batch loss:  tensor(0.1546, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6262  batch loss:  tensor(0.0766, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6263  batch loss:  tensor(0.0424, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6264  batch loss:  tensor(0.0815, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6265  batch loss:  tensor(0.0666, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6266  batch loss:  tensor(0.0896, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6267  batch loss:  tensor(0.1049, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6268  batch loss:  tensor(0.2064, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6269  batch loss:  tensor(0.1613, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6270  batch loss:  tensor(0.1046, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6271  batch loss:  tensor(0.1126, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6272  batch loss:  tensor(0.1178, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6273  batch loss:  tensor(0.0580, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6274  batch loss:  tensor(0.0885, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6275  batch loss:  tensor(0.0671, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6276  batch loss:  tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6277  batch loss:  tensor(0.0538, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6278  batch loss:  tensor(0.0951, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6279  batch loss:  tensor(0.0993, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6280  batch loss:  tensor(0.1185, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6281  batch loss:  tensor(0.0830, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6282  batch loss:  tensor(0.0767, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6283  batch loss:  tensor(0.0969, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6284  batch loss:  tensor(0.0774, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6285  batch loss:  tensor(0.0871, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6286  batch loss:  tensor(0.0761, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6287  batch loss:  tensor(0.0680, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6288  batch loss:  tensor(0.0608, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6289  batch loss:  tensor(0.0962, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6290  batch loss:  tensor(0.1266, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6291  batch loss:  tensor(0.0627, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6292  batch loss:  tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6293  batch loss:  tensor(0.1490, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6294  batch loss:  tensor(0.0780, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6295  batch loss:  tensor(0.1626, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6296  batch loss:  tensor(0.0680, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6297  batch loss:  tensor(0.0945, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6298  batch loss:  tensor(0.0694, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6299  batch loss:  tensor(0.0798, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6300  batch loss:  tensor(0.0884, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6301  batch loss:  tensor(0.0684, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6302  batch loss:  tensor(0.1088, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6303  batch loss:  tensor(0.1500, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6304  batch loss:  tensor(0.1457, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6305  batch loss:  tensor(0.0661, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6306  batch loss:  tensor(0.1834, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6307  batch loss:  tensor(0.0709, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6308  batch loss:  tensor(0.0807, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6309  batch loss:  tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6310  batch loss:  tensor(0.1053, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6311  batch loss:  tensor(0.0661, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6312  batch loss:  tensor(0.0480, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6313  batch loss:  tensor(0.1153, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6314  batch loss:  tensor(0.0638, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6315  batch loss:  tensor(0.0637, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6316  batch loss:  tensor(0.1738, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6317  batch loss:  tensor(0.1075, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6318  batch loss:  tensor(0.0556, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6319  batch loss:  tensor(0.1008, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6320  batch loss:  tensor(0.0926, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6321  batch loss:  tensor(0.0679, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6322  batch loss:  tensor(0.1158, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6323  batch loss:  tensor(0.1938, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6324  batch loss:  tensor(0.0636, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6325  batch loss:  tensor(0.1536, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6326  batch loss:  tensor(0.0636, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6327  batch loss:  tensor(0.0888, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6328  batch loss:  tensor(0.0873, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6329  batch loss:  tensor(0.0757, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6330  batch loss:  tensor(0.1431, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6331  batch loss:  tensor(0.1616, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6332  batch loss:  tensor(0.0748, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6333  batch loss:  tensor(0.0959, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6334  batch loss:  tensor(0.1860, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6335  batch loss:  tensor(0.0606, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6336  batch loss:  tensor(0.1448, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6337  batch loss:  tensor(0.0835, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6338  batch loss:  tensor(0.0556, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6339  batch loss:  tensor(0.1809, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6340  batch loss:  tensor(0.1167, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6341  batch loss:  tensor(0.0575, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6342  batch loss:  tensor(0.1123, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6343  batch loss:  tensor(0.1391, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6344  batch loss:  tensor(0.0889, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6345  batch loss:  tensor(0.1295, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6346  batch loss:  tensor(0.1259, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6347  batch loss:  tensor(0.1118, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6348  batch loss:  tensor(0.1080, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6349  batch loss:  tensor(0.1231, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6350  batch loss:  tensor(0.1752, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6351  batch loss:  tensor(0.1920, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6352  batch loss:  tensor(0.0968, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6353  batch loss:  tensor(0.1571, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6354  batch loss:  tensor(0.0769, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6355  batch loss:  tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6356  batch loss:  tensor(0.0842, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6357  batch loss:  tensor(0.1472, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6358  batch loss:  tensor(0.2151, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6359  batch loss:  tensor(0.0530, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6360  batch loss:  tensor(0.1174, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6361  batch loss:  tensor(0.0727, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6362  batch loss:  tensor(0.0518, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6363  batch loss:  tensor(0.1557, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6364  batch loss:  tensor(0.1847, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6365  batch loss:  tensor(0.0769, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6366  batch loss:  tensor(0.0851, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6367  batch loss:  tensor(0.1163, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6368  batch loss:  tensor(0.0738, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6369  batch loss:  tensor(0.1063, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6370  batch loss:  tensor(0.0711, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6371  batch loss:  tensor(0.1554, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6372  batch loss:  tensor(0.0938, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6373  batch loss:  tensor(0.1123, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6374  batch loss:  tensor(0.1035, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6375  batch loss:  tensor(0.0827, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6376  batch loss:  tensor(0.0702, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6377  batch loss:  tensor(0.0770, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6378  batch loss:  tensor(0.0995, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6379  batch loss:  tensor(0.0588, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6380  batch loss:  tensor(0.0499, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6381  batch loss:  tensor(0.0587, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6382  batch loss:  tensor(0.1617, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6383  batch loss:  tensor(0.1271, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6384  batch loss:  tensor(0.1267, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6385  batch loss:  tensor(0.0820, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6386  batch loss:  tensor(0.1093, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6387  batch loss:  tensor(0.0693, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6388  batch loss:  tensor(0.0682, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6389  batch loss:  tensor(0.1806, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6390  batch loss:  tensor(0.0561, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6391  batch loss:  tensor(0.0996, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6392  batch loss:  tensor(0.0861, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6393  batch loss:  tensor(0.1067, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6394  batch loss:  tensor(0.0796, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6395  batch loss:  tensor(0.0398, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6396  batch loss:  tensor(0.1102, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6397  batch loss:  tensor(0.1411, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6398  batch loss:  tensor(0.1435, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6399  batch loss:  tensor(0.0593, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6400  batch loss:  tensor(0.1230, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6401  batch loss:  tensor(0.0863, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6402  batch loss:  tensor(0.1029, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6403  batch loss:  tensor(0.0591, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6404  batch loss:  tensor(0.1968, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6405  batch loss:  tensor(0.1369, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6406  batch loss:  tensor(0.0843, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6407  batch loss:  tensor(0.1018, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6408  batch loss:  tensor(0.0763, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6409  batch loss:  tensor(0.1594, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6410  batch loss:  tensor(0.0966, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6411  batch loss:  tensor(0.1230, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6412  batch loss:  tensor(0.0992, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6413  batch loss:  tensor(0.0682, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6414  batch loss:  tensor(0.0959, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6415  batch loss:  tensor(0.0802, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6416  batch loss:  tensor(0.0850, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6417  batch loss:  tensor(0.1222, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6418  batch loss:  tensor(0.0798, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6419  batch loss:  tensor(0.1006, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6420  batch loss:  tensor(0.1019, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6421  batch loss:  tensor(0.1074, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6422  batch loss:  tensor(0.0875, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6423  batch loss:  tensor(0.0890, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6424  batch loss:  tensor(0.1296, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6425  batch loss:  tensor(0.0818, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6426  batch loss:  tensor(0.1354, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6427  batch loss:  tensor(0.1134, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6428  batch loss:  tensor(0.1354, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6429  batch loss:  tensor(0.0648, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6430  batch loss:  tensor(0.1450, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6431  batch loss:  tensor(0.3543, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6432  batch loss:  tensor(0.1122, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6433  batch loss:  tensor(0.0673, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6434  batch loss:  tensor(0.1247, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6435  batch loss:  tensor(0.0865, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6436  batch loss:  tensor(0.1014, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6437  batch loss:  tensor(0.1051, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6438  batch loss:  tensor(0.0987, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6439  batch loss:  tensor(0.1308, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6440  batch loss:  tensor(0.0635, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6441  batch loss:  tensor(0.0872, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6442  batch loss:  tensor(0.1176, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6443  batch loss:  tensor(0.0795, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6444  batch loss:  tensor(0.1139, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6445  batch loss:  tensor(0.0866, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6446  batch loss:  tensor(0.0998, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6447  batch loss:  tensor(0.0500, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6448  batch loss:  tensor(0.1096, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6449  batch loss:  tensor(0.0793, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6450  batch loss:  tensor(0.1148, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6451  batch loss:  tensor(0.1118, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6452  batch loss:  tensor(0.1232, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6453  batch loss:  tensor(0.0653, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6454  batch loss:  tensor(0.0841, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6455  batch loss:  tensor(0.0660, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6456  batch loss:  tensor(0.0736, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6457  batch loss:  tensor(0.1652, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6458  batch loss:  tensor(0.0814, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6459  batch loss:  tensor(0.1066, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6460  batch loss:  tensor(0.0916, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6461  batch loss:  tensor(0.0731, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6462  batch loss:  tensor(0.0724, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6463  batch loss:  tensor(0.0977, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6464  batch loss:  tensor(0.0699, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6465  batch loss:  tensor(0.0702, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6466  batch loss:  tensor(0.1475, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6467  batch loss:  tensor(0.0632, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6468  batch loss:  tensor(0.0488, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6469  batch loss:  tensor(0.1887, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6470  batch loss:  tensor(0.0731, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6471  batch loss:  tensor(0.1037, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6472  batch loss:  tensor(0.0624, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6473  batch loss:  tensor(0.1399, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6474  batch loss:  tensor(0.0731, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6475  batch loss:  tensor(0.1064, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6476  batch loss:  tensor(0.0737, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6477  batch loss:  tensor(0.0635, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6478  batch loss:  tensor(0.1195, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6479  batch loss:  tensor(0.0660, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6480  batch loss:  tensor(0.2199, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6481  batch loss:  tensor(0.0725, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6482  batch loss:  tensor(0.1048, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6483  batch loss:  tensor(0.0546, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6484  batch loss:  tensor(0.0483, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6485  batch loss:  tensor(0.1211, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6486  batch loss:  tensor(0.1123, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6487  batch loss:  tensor(0.0923, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6488  batch loss:  tensor(0.0881, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6489  batch loss:  tensor(0.1096, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6490  batch loss:  tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6491  batch loss:  tensor(0.0592, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6492  batch loss:  tensor(0.0599, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6493  batch loss:  tensor(0.1187, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6494  batch loss:  tensor(0.0934, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6495  batch loss:  tensor(0.0780, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6496  batch loss:  tensor(0.1581, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6497  batch loss:  tensor(0.1008, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6498  batch loss:  tensor(0.0766, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6499  batch loss:  tensor(0.0549, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6500  batch loss:  tensor(0.0870, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6501  batch loss:  tensor(0.0634, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6502  batch loss:  tensor(0.2029, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6503  batch loss:  tensor(0.1075, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6504  batch loss:  tensor(0.2087, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6505  batch loss:  tensor(0.0473, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6506  batch loss:  tensor(0.0610, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6507  batch loss:  tensor(0.0659, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6508  batch loss:  tensor(0.0892, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6509  batch loss:  tensor(0.1249, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6510  batch loss:  tensor(0.0911, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6511  batch loss:  tensor(0.1258, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6512  batch loss:  tensor(0.1020, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6513  batch loss:  tensor(0.1087, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6514  batch loss:  tensor(0.0959, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6515  batch loss:  tensor(0.1731, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6516  batch loss:  tensor(0.0630, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6517  batch loss:  tensor(0.1058, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6518  batch loss:  tensor(0.0582, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6519  batch loss:  tensor(0.1166, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6520  batch loss:  tensor(0.1002, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6521  batch loss:  tensor(0.0972, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6522  batch loss:  tensor(0.1225, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6523  batch loss:  tensor(0.0596, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6524  batch loss:  tensor(0.0727, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6525  batch loss:  tensor(0.1549, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6526  batch loss:  tensor(0.0868, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6527  batch loss:  tensor(0.0639, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6528  batch loss:  tensor(0.1117, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6529  batch loss:  tensor(0.1259, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6530  batch loss:  tensor(0.0695, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6531  batch loss:  tensor(0.1228, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6532  batch loss:  tensor(0.1061, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6533  batch loss:  tensor(0.1186, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6534  batch loss:  tensor(0.1096, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6535  batch loss:  tensor(0.0699, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6536  batch loss:  tensor(0.0650, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6537  batch loss:  tensor(0.1341, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6538  batch loss:  tensor(0.1496, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6539  batch loss:  tensor(0.1281, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6540  batch loss:  tensor(0.1018, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6541  batch loss:  tensor(0.1173, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6542  batch loss:  tensor(0.1393, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6543  batch loss:  tensor(0.0849, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6544  batch loss:  tensor(0.0931, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6545  batch loss:  tensor(0.1215, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6546  batch loss:  tensor(0.1007, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6547  batch loss:  tensor(0.1106, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6548  batch loss:  tensor(0.0897, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6549  batch loss:  tensor(0.1041, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6550  batch loss:  tensor(0.0991, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6551  batch loss:  tensor(0.0776, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6552  batch loss:  tensor(0.1861, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6553  batch loss:  tensor(0.2334, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6554  batch loss:  tensor(0.1015, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6555  batch loss:  tensor(0.1618, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6556  batch loss:  tensor(0.1738, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6557  batch loss:  tensor(0.1109, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6558  batch loss:  tensor(0.0669, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6559  batch loss:  tensor(0.0804, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6560  batch loss:  tensor(0.1618, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6561  batch loss:  tensor(0.0715, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6562  batch loss:  tensor(0.1058, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6563  batch loss:  tensor(0.0741, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6564  batch loss:  tensor(0.0705, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6565  batch loss:  tensor(0.0664, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6566  batch loss:  tensor(0.2266, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6567  batch loss:  tensor(0.0372, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6568  batch loss:  tensor(0.1332, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6569  batch loss:  tensor(0.1259, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6570  batch loss:  tensor(0.0628, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6571  batch loss:  tensor(0.1001, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6572  batch loss:  tensor(0.0494, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6573  batch loss:  tensor(0.1394, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6574  batch loss:  tensor(0.0631, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6575  batch loss:  tensor(0.1226, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6576  batch loss:  tensor(0.1508, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6577  batch loss:  tensor(0.0867, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6578  batch loss:  tensor(0.0807, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6579  batch loss:  tensor(0.0918, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6580  batch loss:  tensor(0.0707, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6581  batch loss:  tensor(0.1217, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6582  batch loss:  tensor(0.1432, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6583  batch loss:  tensor(0.1250, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6584  batch loss:  tensor(0.1156, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6585  batch loss:  tensor(0.2635, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6586  batch loss:  tensor(0.0884, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6587  batch loss:  tensor(0.0790, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6588  batch loss:  tensor(0.0996, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6589  batch loss:  tensor(0.0680, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6590  batch loss:  tensor(0.1095, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6591  batch loss:  tensor(0.0598, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6592  batch loss:  tensor(0.1078, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6593  batch loss:  tensor(0.0725, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6594  batch loss:  tensor(0.0798, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6595  batch loss:  tensor(0.1028, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6596  batch loss:  tensor(0.0577, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6597  batch loss:  tensor(0.0759, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6598  batch loss:  tensor(0.0948, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6599  batch loss:  tensor(0.1573, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6600  batch loss:  tensor(0.1198, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6601  batch loss:  tensor(0.0846, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6602  batch loss:  tensor(0.0444, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6603  batch loss:  tensor(0.0924, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6604  batch loss:  tensor(0.1297, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6605  batch loss:  tensor(0.0643, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6606  batch loss:  tensor(0.0609, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6607  batch loss:  tensor(0.1046, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6608  batch loss:  tensor(0.0938, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6609  batch loss:  tensor(0.1495, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6610  batch loss:  tensor(0.0894, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6611  batch loss:  tensor(0.0873, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6612  batch loss:  tensor(0.0967, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6613  batch loss:  tensor(0.0716, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6614  batch loss:  tensor(0.0727, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6615  batch loss:  tensor(0.0890, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6616  batch loss:  tensor(0.0870, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6617  batch loss:  tensor(0.2471, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6618  batch loss:  tensor(0.0921, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6619  batch loss:  tensor(0.0480, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6620  batch loss:  tensor(0.1036, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6621  batch loss:  tensor(0.0749, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6622  batch loss:  tensor(0.0478, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6623  batch loss:  tensor(0.0685, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6624  batch loss:  tensor(0.0744, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6625  batch loss:  tensor(0.0702, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6626  batch loss:  tensor(0.1094, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6627  batch loss:  tensor(0.0760, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6628  batch loss:  tensor(0.0712, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6629  batch loss:  tensor(0.0836, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6630  batch loss:  tensor(0.0914, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6631  batch loss:  tensor(0.1576, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6632  batch loss:  tensor(0.0751, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6633  batch loss:  tensor(0.1302, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6634  batch loss:  tensor(0.1107, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6635  batch loss:  tensor(0.0633, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6636  batch loss:  tensor(0.0546, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6637  batch loss:  tensor(0.0812, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6638  batch loss:  tensor(0.1004, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6639  batch loss:  tensor(0.1070, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6640  batch loss:  tensor(0.1479, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6641  batch loss:  tensor(0.0857, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6642  batch loss:  tensor(0.0986, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6643  batch loss:  tensor(0.1133, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6644  batch loss:  tensor(0.1120, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6645  batch loss:  tensor(0.0914, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6646  batch loss:  tensor(0.1184, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6647  batch loss:  tensor(0.0603, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6648  batch loss:  tensor(0.0961, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6649  batch loss:  tensor(0.0648, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6650  batch loss:  tensor(0.0950, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6651  batch loss:  tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6652  batch loss:  tensor(0.1136, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6653  batch loss:  tensor(0.1084, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6654  batch loss:  tensor(0.0401, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6655  batch loss:  tensor(0.0881, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6656  batch loss:  tensor(0.1195, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6657  batch loss:  tensor(0.1753, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6658  batch loss:  tensor(0.1047, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6659  batch loss:  tensor(0.1736, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6660  batch loss:  tensor(0.0742, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6661  batch loss:  tensor(0.1034, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6662  batch loss:  tensor(0.1893, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6663  batch loss:  tensor(0.0771, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6664  batch loss:  tensor(0.0896, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6665  batch loss:  tensor(0.0685, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6666  batch loss:  tensor(0.1261, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6667  batch loss:  tensor(0.0797, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6668  batch loss:  tensor(0.1802, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6669  batch loss:  tensor(0.1579, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6670  batch loss:  tensor(0.0567, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6671  batch loss:  tensor(0.0614, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6672  batch loss:  tensor(0.0934, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6673  batch loss:  tensor(0.1237, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6674  batch loss:  tensor(0.0753, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6675  batch loss:  tensor(0.0674, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6676  batch loss:  tensor(0.0771, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6677  batch loss:  tensor(0.1434, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6678  batch loss:  tensor(0.0932, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6679  batch loss:  tensor(0.1048, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6680  batch loss:  tensor(0.1101, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6681  batch loss:  tensor(0.1098, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6682  batch loss:  tensor(0.0853, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6683  batch loss:  tensor(0.0748, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6684  batch loss:  tensor(0.1027, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6685  batch loss:  tensor(0.1462, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6686  batch loss:  tensor(0.0640, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6687  batch loss:  tensor(0.0588, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6688  batch loss:  tensor(0.0769, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6689  batch loss:  tensor(0.1049, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6690  batch loss:  tensor(0.0553, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6691  batch loss:  tensor(0.0680, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6692  batch loss:  tensor(0.0756, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6693  batch loss:  tensor(0.0734, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6694  batch loss:  tensor(0.0643, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6695  batch loss:  tensor(0.0768, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6696  batch loss:  tensor(0.0646, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6697  batch loss:  tensor(0.0509, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6698  batch loss:  tensor(0.0586, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6699  batch loss:  tensor(0.0588, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6700  batch loss:  tensor(0.1045, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6701  batch loss:  tensor(0.1157, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6702  batch loss:  tensor(0.0649, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6703  batch loss:  tensor(0.0909, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6704  batch loss:  tensor(0.0908, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6705  batch loss:  tensor(0.0668, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6706  batch loss:  tensor(0.1549, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6707  batch loss:  tensor(0.1101, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6708  batch loss:  tensor(0.0478, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6709  batch loss:  tensor(0.0604, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6710  batch loss:  tensor(0.0575, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6711  batch loss:  tensor(0.1370, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6712  batch loss:  tensor(0.0568, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6713  batch loss:  tensor(0.1853, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6714  batch loss:  tensor(0.0479, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6715  batch loss:  tensor(0.0748, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6716  batch loss:  tensor(0.0766, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6717  batch loss:  tensor(0.0532, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6718  batch loss:  tensor(0.0410, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6719  batch loss:  tensor(0.1878, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6720  batch loss:  tensor(0.0843, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6721  batch loss:  tensor(0.0394, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6722  batch loss:  tensor(0.1005, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6723  batch loss:  tensor(0.0886, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6724  batch loss:  tensor(0.0842, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6725  batch loss:  tensor(0.1106, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6726  batch loss:  tensor(0.0924, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6727  batch loss:  tensor(0.0472, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6728  batch loss:  tensor(0.1074, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6729  batch loss:  tensor(0.1283, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6730  batch loss:  tensor(0.0472, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6731  batch loss:  tensor(0.1570, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6732  batch loss:  tensor(0.1238, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6733  batch loss:  tensor(0.0826, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6734  batch loss:  tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6735  batch loss:  tensor(0.0492, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6736  batch loss:  tensor(0.0670, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6737  batch loss:  tensor(0.0966, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6738  batch loss:  tensor(0.0739, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6739  batch loss:  tensor(0.0507, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6740  batch loss:  tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6741  batch loss:  tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6742  batch loss:  tensor(0.0862, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6743  batch loss:  tensor(0.0883, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6744  batch loss:  tensor(0.1236, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6745  batch loss:  tensor(0.0921, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6746  batch loss:  tensor(0.1100, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6747  batch loss:  tensor(0.0689, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6748  batch loss:  tensor(0.0933, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6749  batch loss:  tensor(0.0891, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6750  batch loss:  tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6751  batch loss:  tensor(0.0652, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6752  batch loss:  tensor(0.0967, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6753  batch loss:  tensor(0.0877, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6754  batch loss:  tensor(0.0769, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6755  batch loss:  tensor(0.0827, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6756  batch loss:  tensor(0.0807, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6757  batch loss:  tensor(0.1089, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6758  batch loss:  tensor(0.1125, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6759  batch loss:  tensor(0.0734, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6760  batch loss:  tensor(0.1309, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6761  batch loss:  tensor(0.0788, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6762  batch loss:  tensor(0.1249, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6763  batch loss:  tensor(0.0956, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6764  batch loss:  tensor(0.1305, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6765  batch loss:  tensor(0.2174, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6766  batch loss:  tensor(0.0847, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6767  batch loss:  tensor(0.1396, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6768  batch loss:  tensor(0.0716, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6769  batch loss:  tensor(0.1053, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6770  batch loss:  tensor(0.0867, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6771  batch loss:  tensor(0.1854, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6772  batch loss:  tensor(0.0695, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6773  batch loss:  tensor(0.1379, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6774  batch loss:  tensor(0.1205, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6775  batch loss:  tensor(0.0931, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6776  batch loss:  tensor(0.0719, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6777  batch loss:  tensor(0.0758, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6778  batch loss:  tensor(0.0689, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6779  batch loss:  tensor(0.0950, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6780  batch loss:  tensor(0.1558, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6781  batch loss:  tensor(0.0810, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6782  batch loss:  tensor(0.0793, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6783  batch loss:  tensor(0.1172, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6784  batch loss:  tensor(0.0815, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6785  batch loss:  tensor(0.0862, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6786  batch loss:  tensor(0.0626, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6787  batch loss:  tensor(0.0959, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6788  batch loss:  tensor(0.1529, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6789  batch loss:  tensor(0.0739, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6790  batch loss:  tensor(0.0876, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6791  batch loss:  tensor(0.1038, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6792  batch loss:  tensor(0.1214, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6793  batch loss:  tensor(0.0867, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6794  batch loss:  tensor(0.0515, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6795  batch loss:  tensor(0.1247, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6796  batch loss:  tensor(0.0767, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6797  batch loss:  tensor(0.0879, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6798  batch loss:  tensor(0.0868, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6799  batch loss:  tensor(0.0812, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6800  batch loss:  tensor(0.0706, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6801  batch loss:  tensor(0.1569, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6802  batch loss:  tensor(0.0978, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6803  batch loss:  tensor(0.0545, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6804  batch loss:  tensor(0.0995, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6805  batch loss:  tensor(0.1398, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6806  batch loss:  tensor(0.1451, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6807  batch loss:  tensor(0.2787, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6808  batch loss:  tensor(0.0985, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6809  batch loss:  tensor(0.1607, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6810  batch loss:  tensor(0.0472, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6811  batch loss:  tensor(0.0550, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6812  batch loss:  tensor(0.1499, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6813  batch loss:  tensor(0.0966, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6814  batch loss:  tensor(0.1359, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6815  batch loss:  tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6816  batch loss:  tensor(0.0891, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6817  batch loss:  tensor(0.0779, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6818  batch loss:  tensor(0.1070, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6819  batch loss:  tensor(0.1891, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6820  batch loss:  tensor(0.2072, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6821  batch loss:  tensor(0.1310, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6822  batch loss:  tensor(0.0978, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6823  batch loss:  tensor(0.0495, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6824  batch loss:  tensor(0.0933, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6825  batch loss:  tensor(0.0566, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6826  batch loss:  tensor(0.0691, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6827  batch loss:  tensor(0.1022, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6828  batch loss:  tensor(0.2013, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6829  batch loss:  tensor(0.0801, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6830  batch loss:  tensor(0.0955, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6831  batch loss:  tensor(0.1560, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6832  batch loss:  tensor(0.0986, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6833  batch loss:  tensor(0.1586, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6834  batch loss:  tensor(0.0972, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6835  batch loss:  tensor(0.1106, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6836  batch loss:  tensor(0.1613, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6837  batch loss:  tensor(0.1207, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6838  batch loss:  tensor(0.0808, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6839  batch loss:  tensor(0.0565, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6840  batch loss:  tensor(0.0495, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6841  batch loss:  tensor(0.0457, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6842  batch loss:  tensor(0.1492, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6843  batch loss:  tensor(0.1093, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6844  batch loss:  tensor(0.0714, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6845  batch loss:  tensor(0.1606, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6846  batch loss:  tensor(0.0765, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6847  batch loss:  tensor(0.1431, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6848  batch loss:  tensor(0.1443, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6849  batch loss:  tensor(0.2455, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6850  batch loss:  tensor(0.0939, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6851  batch loss:  tensor(0.0719, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6852  batch loss:  tensor(0.0655, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6853  batch loss:  tensor(0.1597, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6854  batch loss:  tensor(0.0827, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6855  batch loss:  tensor(0.1642, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6856  batch loss:  tensor(0.0775, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6857  batch loss:  tensor(0.1145, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6858  batch loss:  tensor(0.2065, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6859  batch loss:  tensor(0.0672, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6860  batch loss:  tensor(0.1707, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6861  batch loss:  tensor(0.0550, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6862  batch loss:  tensor(0.0653, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6863  batch loss:  tensor(0.0911, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6864  batch loss:  tensor(0.0901, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6865  batch loss:  tensor(0.0814, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6866  batch loss:  tensor(0.1168, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6867  batch loss:  tensor(0.1152, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6868  batch loss:  tensor(0.1307, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6869  batch loss:  tensor(0.0468, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6870  batch loss:  tensor(0.2612, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6871  batch loss:  tensor(0.0599, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6872  batch loss:  tensor(0.0903, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6873  batch loss:  tensor(0.0548, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6874  batch loss:  tensor(0.2555, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6875  batch loss:  tensor(0.2026, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6876  batch loss:  tensor(0.1513, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6877  batch loss:  tensor(0.0766, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6878  batch loss:  tensor(0.0812, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6879  batch loss:  tensor(0.1565, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6880  batch loss:  tensor(0.0995, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6881  batch loss:  tensor(0.0645, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6882  batch loss:  tensor(0.1182, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6883  batch loss:  tensor(0.0918, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6884  batch loss:  tensor(0.0703, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6885  batch loss:  tensor(0.0624, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6886  batch loss:  tensor(0.1376, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6887  batch loss:  tensor(0.0803, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6888  batch loss:  tensor(0.0774, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6889  batch loss:  tensor(0.1094, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6890  batch loss:  tensor(0.1917, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6891  batch loss:  tensor(0.0561, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6892  batch loss:  tensor(0.0617, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6893  batch loss:  tensor(0.0566, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6894  batch loss:  tensor(0.0748, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6895  batch loss:  tensor(0.0974, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6896  batch loss:  tensor(0.0854, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6897  batch loss:  tensor(0.0646, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6898  batch loss:  tensor(0.0642, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6899  batch loss:  tensor(0.0573, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6900  batch loss:  tensor(0.1254, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6901  batch loss:  tensor(0.1068, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6902  batch loss:  tensor(0.0861, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6903  batch loss:  tensor(0.1382, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6904  batch loss:  tensor(0.0479, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6905  batch loss:  tensor(0.0815, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6906  batch loss:  tensor(0.0895, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6907  batch loss:  tensor(0.0676, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6908  batch loss:  tensor(0.0741, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6909  batch loss:  tensor(0.1280, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6910  batch loss:  tensor(0.1351, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6911  batch loss:  tensor(0.1359, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6912  batch loss:  tensor(0.0851, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6913  batch loss:  tensor(0.1462, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6914  batch loss:  tensor(0.0369, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6915  batch loss:  tensor(0.1049, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6916  batch loss:  tensor(0.1116, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6917  batch loss:  tensor(0.0492, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6918  batch loss:  tensor(0.0858, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6919  batch loss:  tensor(0.0809, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6920  batch loss:  tensor(0.1197, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6921  batch loss:  tensor(0.0472, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6922  batch loss:  tensor(0.1319, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6923  batch loss:  tensor(0.0490, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6924  batch loss:  tensor(0.0892, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6925  batch loss:  tensor(0.1079, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6926  batch loss:  tensor(0.1285, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6927  batch loss:  tensor(0.0505, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6928  batch loss:  tensor(0.0919, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6929  batch loss:  tensor(0.0556, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6930  batch loss:  tensor(0.3435, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6931  batch loss:  tensor(0.0983, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6932  batch loss:  tensor(0.0711, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6933  batch loss:  tensor(0.0651, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6934  batch loss:  tensor(0.0807, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6935  batch loss:  tensor(0.0705, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6936  batch loss:  tensor(0.0844, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6937  batch loss:  tensor(0.0739, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6938  batch loss:  tensor(0.0784, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6939  batch loss:  tensor(0.1198, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6940  batch loss:  tensor(0.1924, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6941  batch loss:  tensor(0.1216, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6942  batch loss:  tensor(0.0420, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6943  batch loss:  tensor(0.0409, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6944  batch loss:  tensor(0.0661, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6945  batch loss:  tensor(0.0718, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6946  batch loss:  tensor(0.2083, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6947  batch loss:  tensor(0.0827, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6948  batch loss:  tensor(0.0897, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6949  batch loss:  tensor(0.1035, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6950  batch loss:  tensor(0.0678, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6951  batch loss:  tensor(0.2184, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6952  batch loss:  tensor(0.0620, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6953  batch loss:  tensor(0.0815, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6954  batch loss:  tensor(0.0709, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6955  batch loss:  tensor(0.0918, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6956  batch loss:  tensor(0.0821, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6957  batch loss:  tensor(0.1267, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6958  batch loss:  tensor(0.1027, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6959  batch loss:  tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6960  batch loss:  tensor(0.1811, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6961  batch loss:  tensor(0.0776, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6962  batch loss:  tensor(0.0878, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6963  batch loss:  tensor(0.0679, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6964  batch loss:  tensor(0.0549, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6965  batch loss:  tensor(0.0849, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6966  batch loss:  tensor(0.0998, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6967  batch loss:  tensor(0.0792, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6968  batch loss:  tensor(0.0457, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6969  batch loss:  tensor(0.0854, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6970  batch loss:  tensor(0.3534, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6971  batch loss:  tensor(0.0834, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6972  batch loss:  tensor(0.1342, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6973  batch loss:  tensor(0.0700, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6974  batch loss:  tensor(0.1062, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6975  batch loss:  tensor(0.0586, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6976  batch loss:  tensor(0.1319, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6977  batch loss:  tensor(0.0971, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6978  batch loss:  tensor(0.0698, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6979  batch loss:  tensor(0.3286, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6980  batch loss:  tensor(0.0491, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6981  batch loss:  tensor(0.1287, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6982  batch loss:  tensor(0.0903, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6983  batch loss:  tensor(0.0625, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6984  batch loss:  tensor(0.0748, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6985  batch loss:  tensor(0.3020, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6986  batch loss:  tensor(0.0990, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6987  batch loss:  tensor(0.0424, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6988  batch loss:  tensor(0.1268, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6989  batch loss:  tensor(0.0672, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6990  batch loss:  tensor(0.1428, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6991  batch loss:  tensor(0.1260, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6992  batch loss:  tensor(0.0681, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6993  batch loss:  tensor(0.0698, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6994  batch loss:  tensor(0.0892, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6995  batch loss:  tensor(0.0401, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6996  batch loss:  tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6997  batch loss:  tensor(0.1077, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6998  batch loss:  tensor(0.1168, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  6999  batch loss:  tensor(0.1074, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7000  batch loss:  tensor(0.0994, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7001  batch loss:  tensor(0.1800, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7002  batch loss:  tensor(0.2811, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7003  batch loss:  tensor(0.1700, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7004  batch loss:  tensor(0.1105, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7005  batch loss:  tensor(0.0914, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7006  batch loss:  tensor(0.1500, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7007  batch loss:  tensor(0.0760, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7008  batch loss:  tensor(0.1058, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7009  batch loss:  tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7010  batch loss:  tensor(0.0733, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7011  batch loss:  tensor(0.1178, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7012  batch loss:  tensor(0.0433, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7013  batch loss:  tensor(0.0614, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7014  batch loss:  tensor(0.1000, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7015  batch loss:  tensor(0.0832, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7016  batch loss:  tensor(0.1231, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7017  batch loss:  tensor(0.0987, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7018  batch loss:  tensor(0.2068, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7019  batch loss:  tensor(0.1136, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7020  batch loss:  tensor(0.1726, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7021  batch loss:  tensor(0.0587, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7022  batch loss:  tensor(0.0572, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7023  batch loss:  tensor(0.0504, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7024  batch loss:  tensor(0.1033, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7025  batch loss:  tensor(0.0587, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7026  batch loss:  tensor(0.0635, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7027  batch loss:  tensor(0.0806, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7028  batch loss:  tensor(0.0891, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7029  batch loss:  tensor(0.0635, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7030  batch loss:  tensor(0.0637, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7031  batch loss:  tensor(0.0566, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7032  batch loss:  tensor(0.0685, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7033  batch loss:  tensor(0.0909, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7034  batch loss:  tensor(0.0768, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7035  batch loss:  tensor(0.0768, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7036  batch loss:  tensor(0.1004, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7037  batch loss:  tensor(0.2526, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7038  batch loss:  tensor(0.0643, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7039  batch loss:  tensor(0.0771, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7040  batch loss:  tensor(0.1207, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7041  batch loss:  tensor(0.0808, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7042  batch loss:  tensor(0.1137, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7043  batch loss:  tensor(0.0539, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7044  batch loss:  tensor(0.1144, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7045  batch loss:  tensor(0.0638, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7046  batch loss:  tensor(0.0596, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7047  batch loss:  tensor(0.0656, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7048  batch loss:  tensor(0.1121, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7049  batch loss:  tensor(0.0878, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7050  batch loss:  tensor(0.2083, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7051  batch loss:  tensor(0.0931, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7052  batch loss:  tensor(0.0688, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7053  batch loss:  tensor(0.1308, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7054  batch loss:  tensor(0.1355, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7055  batch loss:  tensor(0.1915, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7056  batch loss:  tensor(0.1554, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7057  batch loss:  tensor(0.0852, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7058  batch loss:  tensor(0.0762, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7059  batch loss:  tensor(0.1090, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7060  batch loss:  tensor(0.1018, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7061  batch loss:  tensor(0.0789, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7062  batch loss:  tensor(0.1155, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7063  batch loss:  tensor(0.0940, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7064  batch loss:  tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7065  batch loss:  tensor(0.0810, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7066  batch loss:  tensor(0.1623, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7067  batch loss:  tensor(0.1359, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7068  batch loss:  tensor(0.0674, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7069  batch loss:  tensor(0.1149, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7070  batch loss:  tensor(0.1292, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7071  batch loss:  tensor(0.0818, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7072  batch loss:  tensor(0.0650, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7073  batch loss:  tensor(0.0651, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7074  batch loss:  tensor(0.0984, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7075  batch loss:  tensor(0.0641, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7076  batch loss:  tensor(0.0968, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7077  batch loss:  tensor(0.0866, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7078  batch loss:  tensor(0.0955, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7079  batch loss:  tensor(0.0663, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7080  batch loss:  tensor(0.0892, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7081  batch loss:  tensor(0.0663, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7082  batch loss:  tensor(0.1271, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7083  batch loss:  tensor(0.0961, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7084  batch loss:  tensor(0.0990, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7085  batch loss:  tensor(0.1055, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7086  batch loss:  tensor(0.1186, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7087  batch loss:  tensor(0.0749, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7088  batch loss:  tensor(0.1084, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7089  batch loss:  tensor(0.1129, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7090  batch loss:  tensor(0.0728, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7091  batch loss:  tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7092  batch loss:  tensor(0.1520, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7093  batch loss:  tensor(0.1919, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7094  batch loss:  tensor(0.0950, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7095  batch loss:  tensor(0.1209, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7096  batch loss:  tensor(0.0588, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7097  batch loss:  tensor(0.1127, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7098  batch loss:  tensor(0.0755, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7099  batch loss:  tensor(0.1118, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7100  batch loss:  tensor(0.0906, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7101  batch loss:  tensor(0.0681, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7102  batch loss:  tensor(0.1381, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7103  batch loss:  tensor(0.1534, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7104  batch loss:  tensor(0.1403, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7105  batch loss:  tensor(0.0864, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7106  batch loss:  tensor(0.0853, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7107  batch loss:  tensor(0.0947, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7108  batch loss:  tensor(0.1014, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7109  batch loss:  tensor(0.0705, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7110  batch loss:  tensor(0.0673, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7111  batch loss:  tensor(0.0583, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7112  batch loss:  tensor(0.0535, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7113  batch loss:  tensor(0.1672, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7114  batch loss:  tensor(0.0948, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7115  batch loss:  tensor(0.1114, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7116  batch loss:  tensor(0.1300, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7117  batch loss:  tensor(0.0621, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7118  batch loss:  tensor(0.1221, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7119  batch loss:  tensor(0.0687, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7120  batch loss:  tensor(0.0571, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7121  batch loss:  tensor(0.1083, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7122  batch loss:  tensor(0.0885, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7123  batch loss:  tensor(0.0639, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7124  batch loss:  tensor(0.1509, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7125  batch loss:  tensor(0.0505, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7126  batch loss:  tensor(0.0788, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7127  batch loss:  tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7128  batch loss:  tensor(0.0670, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7129  batch loss:  tensor(0.0552, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7130  batch loss:  tensor(0.0586, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7131  batch loss:  tensor(0.0858, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7132  batch loss:  tensor(0.0520, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7133  batch loss:  tensor(0.0732, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7134  batch loss:  tensor(0.1267, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7135  batch loss:  tensor(0.2734, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7136  batch loss:  tensor(0.0686, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7137  batch loss:  tensor(0.1112, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7138  batch loss:  tensor(0.0915, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7139  batch loss:  tensor(0.0952, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7140  batch loss:  tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7141  batch loss:  tensor(0.1239, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7142  batch loss:  tensor(0.0683, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7143  batch loss:  tensor(0.1464, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7144  batch loss:  tensor(0.0684, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7145  batch loss:  tensor(0.0780, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7146  batch loss:  tensor(0.0670, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7147  batch loss:  tensor(0.0786, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7148  batch loss:  tensor(0.1026, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7149  batch loss:  tensor(0.0816, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7150  batch loss:  tensor(0.1551, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7151  batch loss:  tensor(0.1245, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7152  batch loss:  tensor(0.1003, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7153  batch loss:  tensor(0.1446, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7154  batch loss:  tensor(0.1050, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7155  batch loss:  tensor(0.0621, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7156  batch loss:  tensor(0.1116, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7157  batch loss:  tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7158  batch loss:  tensor(0.1043, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7159  batch loss:  tensor(0.1580, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7160  batch loss:  tensor(0.0745, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7161  batch loss:  tensor(0.0954, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7162  batch loss:  tensor(0.0681, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7163  batch loss:  tensor(0.1045, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7164  batch loss:  tensor(0.0867, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7165  batch loss:  tensor(0.0730, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7166  batch loss:  tensor(0.0743, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7167  batch loss:  tensor(0.0818, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7168  batch loss:  tensor(0.0892, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7169  batch loss:  tensor(0.1028, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7170  batch loss:  tensor(0.0617, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7171  batch loss:  tensor(0.0548, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7172  batch loss:  tensor(0.0618, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7173  batch loss:  tensor(0.1167, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7174  batch loss:  tensor(0.0617, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7175  batch loss:  tensor(0.1103, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7176  batch loss:  tensor(0.1327, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7177  batch loss:  tensor(0.0554, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7178  batch loss:  tensor(0.0921, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7179  batch loss:  tensor(0.0635, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7180  batch loss:  tensor(0.0839, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7181  batch loss:  tensor(0.1537, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7182  batch loss:  tensor(0.0787, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7183  batch loss:  tensor(0.1161, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7184  batch loss:  tensor(0.0575, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7185  batch loss:  tensor(0.0713, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7186  batch loss:  tensor(0.0853, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7187  batch loss:  tensor(0.0835, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7188  batch loss:  tensor(0.0959, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7189  batch loss:  tensor(0.0807, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7190  batch loss:  tensor(0.1138, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7191  batch loss:  tensor(0.0481, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7192  batch loss:  tensor(0.0907, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7193  batch loss:  tensor(0.0740, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7194  batch loss:  tensor(0.0537, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7195  batch loss:  tensor(0.1811, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7196  batch loss:  tensor(0.0776, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7197  batch loss:  tensor(0.0722, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7198  batch loss:  tensor(0.1256, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7199  batch loss:  tensor(0.0942, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7200  batch loss:  tensor(0.0716, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7201  batch loss:  tensor(0.0915, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7202  batch loss:  tensor(0.0767, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7203  batch loss:  tensor(0.1643, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7204  batch loss:  tensor(0.0513, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7205  batch loss:  tensor(0.1230, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7206  batch loss:  tensor(0.1566, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7207  batch loss:  tensor(0.0904, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7208  batch loss:  tensor(0.1342, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7209  batch loss:  tensor(0.0597, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7210  batch loss:  tensor(0.0779, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7211  batch loss:  tensor(0.0640, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7212  batch loss:  tensor(0.1536, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7213  batch loss:  tensor(0.0930, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7214  batch loss:  tensor(0.0798, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7215  batch loss:  tensor(0.0833, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7216  batch loss:  tensor(0.1245, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7217  batch loss:  tensor(0.1185, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7218  batch loss:  tensor(0.0579, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7219  batch loss:  tensor(0.0575, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7220  batch loss:  tensor(0.0936, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7221  batch loss:  tensor(0.1078, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7222  batch loss:  tensor(0.0683, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7223  batch loss:  tensor(0.0886, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7224  batch loss:  tensor(0.0587, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7225  batch loss:  tensor(0.0854, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7226  batch loss:  tensor(0.0802, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7227  batch loss:  tensor(0.1252, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7228  batch loss:  tensor(0.1218, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7229  batch loss:  tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7230  batch loss:  tensor(0.0698, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7231  batch loss:  tensor(0.0730, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7232  batch loss:  tensor(0.0582, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7233  batch loss:  tensor(0.0452, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7234  batch loss:  tensor(0.1319, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7235  batch loss:  tensor(0.0786, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7236  batch loss:  tensor(0.1346, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7237  batch loss:  tensor(0.0432, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7238  batch loss:  tensor(0.0941, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7239  batch loss:  tensor(0.0430, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7240  batch loss:  tensor(0.1663, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7241  batch loss:  tensor(0.1817, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7242  batch loss:  tensor(0.2135, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7243  batch loss:  tensor(0.1378, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7244  batch loss:  tensor(0.1194, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7245  batch loss:  tensor(0.0492, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7246  batch loss:  tensor(0.0761, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7247  batch loss:  tensor(0.0485, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7248  batch loss:  tensor(0.0911, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7249  batch loss:  tensor(0.1009, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7250  batch loss:  tensor(0.1361, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7251  batch loss:  tensor(0.0550, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7252  batch loss:  tensor(0.0565, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7253  batch loss:  tensor(0.0685, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7254  batch loss:  tensor(0.0724, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7255  batch loss:  tensor(0.0642, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7256  batch loss:  tensor(0.0865, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7257  batch loss:  tensor(0.0539, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7258  batch loss:  tensor(0.0611, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7259  batch loss:  tensor(0.1024, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7260  batch loss:  tensor(0.1124, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7261  batch loss:  tensor(0.0613, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7262  batch loss:  tensor(0.0940, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7263  batch loss:  tensor(0.1191, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7264  batch loss:  tensor(0.0720, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7265  batch loss:  tensor(0.0704, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7266  batch loss:  tensor(0.1309, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7267  batch loss:  tensor(0.0486, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7268  batch loss:  tensor(0.0883, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7269  batch loss:  tensor(0.0887, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7270  batch loss:  tensor(0.0746, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7271  batch loss:  tensor(0.0755, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7272  batch loss:  tensor(0.0631, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7273  batch loss:  tensor(0.0594, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7274  batch loss:  tensor(0.0757, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7275  batch loss:  tensor(0.0820, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7276  batch loss:  tensor(0.0877, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7277  batch loss:  tensor(0.1145, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7278  batch loss:  tensor(0.0793, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7279  batch loss:  tensor(0.1845, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7280  batch loss:  tensor(0.0969, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7281  batch loss:  tensor(0.0759, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7282  batch loss:  tensor(0.0517, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7283  batch loss:  tensor(0.0790, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7284  batch loss:  tensor(0.1051, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7285  batch loss:  tensor(0.1078, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7286  batch loss:  tensor(0.0483, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7287  batch loss:  tensor(0.1111, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7288  batch loss:  tensor(0.0959, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7289  batch loss:  tensor(0.0639, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7290  batch loss:  tensor(0.0666, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7291  batch loss:  tensor(0.0560, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7292  batch loss:  tensor(0.0820, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7293  batch loss:  tensor(0.0445, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7294  batch loss:  tensor(0.0896, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7295  batch loss:  tensor(0.0360, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7296  batch loss:  tensor(0.0656, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7297  batch loss:  tensor(0.0401, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7298  batch loss:  tensor(0.0862, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7299  batch loss:  tensor(0.0454, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7300  batch loss:  tensor(0.1349, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7301  batch loss:  tensor(0.0814, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7302  batch loss:  tensor(0.0899, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7303  batch loss:  tensor(0.0801, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7304  batch loss:  tensor(0.2055, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7305  batch loss:  tensor(0.3101, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7306  batch loss:  tensor(0.0813, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7307  batch loss:  tensor(0.0479, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7308  batch loss:  tensor(0.1064, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7309  batch loss:  tensor(0.1218, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7310  batch loss:  tensor(0.0769, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7311  batch loss:  tensor(0.0637, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7312  batch loss:  tensor(0.0906, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7313  batch loss:  tensor(0.0769, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7314  batch loss:  tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7315  batch loss:  tensor(0.0706, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7316  batch loss:  tensor(0.0599, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7317  batch loss:  tensor(0.0635, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7318  batch loss:  tensor(0.0949, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7319  batch loss:  tensor(0.1116, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7320  batch loss:  tensor(0.0749, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7321  batch loss:  tensor(0.0617, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7322  batch loss:  tensor(0.0529, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7323  batch loss:  tensor(0.1226, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7324  batch loss:  tensor(0.1425, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7325  batch loss:  tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7326  batch loss:  tensor(0.1700, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7327  batch loss:  tensor(0.1277, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7328  batch loss:  tensor(0.1083, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7329  batch loss:  tensor(0.1238, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7330  batch loss:  tensor(0.0801, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7331  batch loss:  tensor(0.1048, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7332  batch loss:  tensor(0.0704, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7333  batch loss:  tensor(0.0625, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7334  batch loss:  tensor(0.0644, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7335  batch loss:  tensor(0.0764, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7336  batch loss:  tensor(0.0682, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7337  batch loss:  tensor(0.0822, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7338  batch loss:  tensor(0.1160, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7339  batch loss:  tensor(0.1254, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7340  batch loss:  tensor(0.1651, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7341  batch loss:  tensor(0.2350, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7342  batch loss:  tensor(0.1174, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7343  batch loss:  tensor(0.0956, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7344  batch loss:  tensor(0.1201, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7345  batch loss:  tensor(0.0977, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7346  batch loss:  tensor(0.0647, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7347  batch loss:  tensor(0.1553, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7348  batch loss:  tensor(0.1061, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7349  batch loss:  tensor(0.1203, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7350  batch loss:  tensor(0.0952, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7351  batch loss:  tensor(0.1151, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7352  batch loss:  tensor(0.1123, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7353  batch loss:  tensor(0.0761, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7354  batch loss:  tensor(0.1848, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7355  batch loss:  tensor(0.2551, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7356  batch loss:  tensor(0.1007, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7357  batch loss:  tensor(0.1077, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7358  batch loss:  tensor(0.1802, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7359  batch loss:  tensor(0.0594, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7360  batch loss:  tensor(0.1872, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7361  batch loss:  tensor(0.1301, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7362  batch loss:  tensor(0.0874, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7363  batch loss:  tensor(0.0932, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7364  batch loss:  tensor(0.1360, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7365  batch loss:  tensor(0.0591, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7366  batch loss:  tensor(0.0821, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7367  batch loss:  tensor(0.1564, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7368  batch loss:  tensor(0.1707, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7369  batch loss:  tensor(0.1165, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7370  batch loss:  tensor(0.0921, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7371  batch loss:  tensor(0.0668, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7372  batch loss:  tensor(0.1397, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7373  batch loss:  tensor(0.0667, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7374  batch loss:  tensor(0.1627, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7375  batch loss:  tensor(0.0727, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7376  batch loss:  tensor(0.0835, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7377  batch loss:  tensor(0.0785, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7378  batch loss:  tensor(0.0786, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7379  batch loss:  tensor(0.0654, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7380  batch loss:  tensor(0.2129, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7381  batch loss:  tensor(0.0566, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7382  batch loss:  tensor(0.0554, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7383  batch loss:  tensor(0.1236, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7384  batch loss:  tensor(0.1014, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7385  batch loss:  tensor(0.0620, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7386  batch loss:  tensor(0.1280, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7387  batch loss:  tensor(0.0996, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7388  batch loss:  tensor(0.1019, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7389  batch loss:  tensor(0.0829, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7390  batch loss:  tensor(0.0598, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7391  batch loss:  tensor(0.0541, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7392  batch loss:  tensor(0.0873, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7393  batch loss:  tensor(0.0623, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7394  batch loss:  tensor(0.1127, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7395  batch loss:  tensor(0.2190, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7396  batch loss:  tensor(0.0469, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7397  batch loss:  tensor(0.0490, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7398  batch loss:  tensor(0.1973, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7399  batch loss:  tensor(0.0841, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7400  batch loss:  tensor(0.0654, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7401  batch loss:  tensor(0.0846, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7402  batch loss:  tensor(0.0655, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7403  batch loss:  tensor(0.0691, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7404  batch loss:  tensor(0.1328, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7405  batch loss:  tensor(0.0903, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7406  batch loss:  tensor(0.0591, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7407  batch loss:  tensor(0.0967, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7408  batch loss:  tensor(0.0629, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7409  batch loss:  tensor(0.1139, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7410  batch loss:  tensor(0.0851, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7411  batch loss:  tensor(0.0452, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7412  batch loss:  tensor(0.1140, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7413  batch loss:  tensor(0.1453, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7414  batch loss:  tensor(0.0888, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7415  batch loss:  tensor(0.0545, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7416  batch loss:  tensor(0.0598, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7417  batch loss:  tensor(0.2351, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7418  batch loss:  tensor(0.0630, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7419  batch loss:  tensor(0.0991, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7420  batch loss:  tensor(0.0471, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7421  batch loss:  tensor(0.1064, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7422  batch loss:  tensor(0.0423, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7423  batch loss:  tensor(0.0507, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7424  batch loss:  tensor(0.1063, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7425  batch loss:  tensor(0.0994, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7426  batch loss:  tensor(0.1259, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7427  batch loss:  tensor(0.1509, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7428  batch loss:  tensor(0.0751, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7429  batch loss:  tensor(0.0842, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7430  batch loss:  tensor(0.0963, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7431  batch loss:  tensor(0.1064, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7432  batch loss:  tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7433  batch loss:  tensor(0.0500, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7434  batch loss:  tensor(0.1335, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7435  batch loss:  tensor(0.0846, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7436  batch loss:  tensor(0.0546, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7437  batch loss:  tensor(0.0699, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7438  batch loss:  tensor(0.1345, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7439  batch loss:  tensor(0.1537, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7440  batch loss:  tensor(0.0692, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7441  batch loss:  tensor(0.0813, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7442  batch loss:  tensor(0.1725, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7443  batch loss:  tensor(0.0962, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7444  batch loss:  tensor(0.1079, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7445  batch loss:  tensor(0.0894, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7446  batch loss:  tensor(0.0924, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7447  batch loss:  tensor(0.0691, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7448  batch loss:  tensor(0.0607, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7449  batch loss:  tensor(0.0676, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7450  batch loss:  tensor(0.0509, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7451  batch loss:  tensor(0.0936, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7452  batch loss:  tensor(0.0534, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7453  batch loss:  tensor(0.0765, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7454  batch loss:  tensor(0.0943, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7455  batch loss:  tensor(0.0777, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7456  batch loss:  tensor(0.1043, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7457  batch loss:  tensor(0.0990, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7458  batch loss:  tensor(0.0974, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7459  batch loss:  tensor(0.0737, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7460  batch loss:  tensor(0.1019, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7461  batch loss:  tensor(0.1113, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7462  batch loss:  tensor(0.1373, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7463  batch loss:  tensor(0.1565, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7464  batch loss:  tensor(0.1134, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7465  batch loss:  tensor(0.0648, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7466  batch loss:  tensor(0.0766, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7467  batch loss:  tensor(0.0966, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7468  batch loss:  tensor(0.0985, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7469  batch loss:  tensor(0.0623, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7470  batch loss:  tensor(0.0902, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7471  batch loss:  tensor(0.1317, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7472  batch loss:  tensor(0.0710, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7473  batch loss:  tensor(0.1232, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7474  batch loss:  tensor(0.0888, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7475  batch loss:  tensor(0.0849, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7476  batch loss:  tensor(0.1001, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7477  batch loss:  tensor(0.1439, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7478  batch loss:  tensor(0.1386, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7479  batch loss:  tensor(0.1742, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7480  batch loss:  tensor(0.0877, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7481  batch loss:  tensor(0.0728, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7482  batch loss:  tensor(0.0535, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7483  batch loss:  tensor(0.0873, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7484  batch loss:  tensor(0.0779, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7485  batch loss:  tensor(0.0854, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7486  batch loss:  tensor(0.1082, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7487  batch loss:  tensor(0.0898, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7488  batch loss:  tensor(0.0813, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7489  batch loss:  tensor(0.0614, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7490  batch loss:  tensor(0.0599, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7491  batch loss:  tensor(0.0652, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7492  batch loss:  tensor(0.1079, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7493  batch loss:  tensor(0.1958, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7494  batch loss:  tensor(0.1160, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7495  batch loss:  tensor(0.0744, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7496  batch loss:  tensor(0.0906, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7497  batch loss:  tensor(0.0966, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7498  batch loss:  tensor(0.0368, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7499  batch loss:  tensor(0.0602, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7500  batch loss:  tensor(0.0943, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7501  batch loss:  tensor(0.0847, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7502  batch loss:  tensor(0.0890, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7503  batch loss:  tensor(0.1141, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7504  batch loss:  tensor(0.0871, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7505  batch loss:  tensor(0.1714, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7506  batch loss:  tensor(0.0803, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7507  batch loss:  tensor(0.1879, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7508  batch loss:  tensor(0.0865, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7509  batch loss:  tensor(0.1195, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7510  batch loss:  tensor(0.0856, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7511  batch loss:  tensor(0.0432, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7512  batch loss:  tensor(0.0847, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7513  batch loss:  tensor(0.0981, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7514  batch loss:  tensor(0.0801, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7515  batch loss:  tensor(0.0689, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7516  batch loss:  tensor(0.0990, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7517  batch loss:  tensor(0.1047, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7518  batch loss:  tensor(0.1359, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7519  batch loss:  tensor(0.1040, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7520  batch loss:  tensor(0.1093, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7521  batch loss:  tensor(0.0768, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7522  batch loss:  tensor(0.0700, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7523  batch loss:  tensor(0.0993, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7524  batch loss:  tensor(0.0579, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7525  batch loss:  tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7526  batch loss:  tensor(0.1124, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7527  batch loss:  tensor(0.0571, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7528  batch loss:  tensor(0.0791, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7529  batch loss:  tensor(0.0856, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7530  batch loss:  tensor(0.1113, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7531  batch loss:  tensor(0.0381, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7532  batch loss:  tensor(0.0501, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7533  batch loss:  tensor(0.0977, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7534  batch loss:  tensor(0.0777, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7535  batch loss:  tensor(0.1396, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7536  batch loss:  tensor(0.0774, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7537  batch loss:  tensor(0.0462, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7538  batch loss:  tensor(0.1642, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7539  batch loss:  tensor(0.1134, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7540  batch loss:  tensor(0.0804, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7541  batch loss:  tensor(0.0853, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7542  batch loss:  tensor(0.0915, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7543  batch loss:  tensor(0.1873, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7544  batch loss:  tensor(0.1007, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7545  batch loss:  tensor(0.1068, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7546  batch loss:  tensor(0.1943, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7547  batch loss:  tensor(0.1084, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7548  batch loss:  tensor(0.0896, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7549  batch loss:  tensor(0.0573, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7550  batch loss:  tensor(0.0450, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7551  batch loss:  tensor(0.0668, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7552  batch loss:  tensor(0.1618, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7553  batch loss:  tensor(0.1018, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7554  batch loss:  tensor(0.0872, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7555  batch loss:  tensor(0.0979, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7556  batch loss:  tensor(0.0871, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7557  batch loss:  tensor(0.0674, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7558  batch loss:  tensor(0.1020, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7559  batch loss:  tensor(0.1151, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7560  batch loss:  tensor(0.0632, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7561  batch loss:  tensor(0.0512, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7562  batch loss:  tensor(0.1157, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7563  batch loss:  tensor(0.0661, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7564  batch loss:  tensor(0.0636, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7565  batch loss:  tensor(0.0679, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7566  batch loss:  tensor(0.0662, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7567  batch loss:  tensor(0.1217, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7568  batch loss:  tensor(0.0878, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7569  batch loss:  tensor(0.1262, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7570  batch loss:  tensor(0.1055, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7571  batch loss:  tensor(0.0895, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7572  batch loss:  tensor(0.0961, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7573  batch loss:  tensor(0.0477, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7574  batch loss:  tensor(0.1144, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7575  batch loss:  tensor(0.0998, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7576  batch loss:  tensor(0.0915, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7577  batch loss:  tensor(0.0706, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7578  batch loss:  tensor(0.0378, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7579  batch loss:  tensor(0.0753, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7580  batch loss:  tensor(0.0822, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7581  batch loss:  tensor(0.0933, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7582  batch loss:  tensor(0.0602, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7583  batch loss:  tensor(0.1316, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7584  batch loss:  tensor(0.1331, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7585  batch loss:  tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7586  batch loss:  tensor(0.0894, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7587  batch loss:  tensor(0.0798, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7588  batch loss:  tensor(0.0748, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7589  batch loss:  tensor(0.0641, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7590  batch loss:  tensor(0.0693, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7591  batch loss:  tensor(0.1306, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7592  batch loss:  tensor(0.0601, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7593  batch loss:  tensor(0.0928, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7594  batch loss:  tensor(0.0806, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7595  batch loss:  tensor(0.1924, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7596  batch loss:  tensor(0.0683, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7597  batch loss:  tensor(0.0987, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7598  batch loss:  tensor(0.1529, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7599  batch loss:  tensor(0.0395, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7600  batch loss:  tensor(0.1589, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7601  batch loss:  tensor(0.1049, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7602  batch loss:  tensor(0.0607, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7603  batch loss:  tensor(0.3465, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7604  batch loss:  tensor(0.1956, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7605  batch loss:  tensor(0.1397, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7606  batch loss:  tensor(0.1095, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7607  batch loss:  tensor(0.0583, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7608  batch loss:  tensor(0.0582, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7609  batch loss:  tensor(0.0731, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7610  batch loss:  tensor(0.0822, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7611  batch loss:  tensor(0.1216, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7612  batch loss:  tensor(0.2066, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7613  batch loss:  tensor(0.1011, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7614  batch loss:  tensor(0.0855, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7615  batch loss:  tensor(0.0687, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7616  batch loss:  tensor(0.1151, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7617  batch loss:  tensor(0.0799, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7618  batch loss:  tensor(0.0643, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7619  batch loss:  tensor(0.0699, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7620  batch loss:  tensor(0.0855, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7621  batch loss:  tensor(0.0773, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7622  batch loss:  tensor(0.0854, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7623  batch loss:  tensor(0.1007, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7624  batch loss:  tensor(0.1127, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7625  batch loss:  tensor(0.1293, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7626  batch loss:  tensor(0.0608, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7627  batch loss:  tensor(0.1354, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7628  batch loss:  tensor(0.0809, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7629  batch loss:  tensor(0.1924, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7630  batch loss:  tensor(0.0432, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7631  batch loss:  tensor(0.0500, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7632  batch loss:  tensor(0.0558, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7633  batch loss:  tensor(0.0971, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7634  batch loss:  tensor(0.0825, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7635  batch loss:  tensor(0.0943, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7636  batch loss:  tensor(0.1082, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7637  batch loss:  tensor(0.0615, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7638  batch loss:  tensor(0.1641, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7639  batch loss:  tensor(0.1521, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7640  batch loss:  tensor(0.0763, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7641  batch loss:  tensor(0.0520, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7642  batch loss:  tensor(0.0471, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7643  batch loss:  tensor(0.0642, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7644  batch loss:  tensor(0.0880, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7645  batch loss:  tensor(0.1381, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7646  batch loss:  tensor(0.0894, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7647  batch loss:  tensor(0.1282, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7648  batch loss:  tensor(0.1225, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7649  batch loss:  tensor(0.0699, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7650  batch loss:  tensor(0.0624, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7651  batch loss:  tensor(0.1039, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7652  batch loss:  tensor(0.0538, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7653  batch loss:  tensor(0.1599, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7654  batch loss:  tensor(0.0645, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7655  batch loss:  tensor(0.0820, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7656  batch loss:  tensor(0.1017, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7657  batch loss:  tensor(0.0970, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7658  batch loss:  tensor(0.0884, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7659  batch loss:  tensor(0.0758, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7660  batch loss:  tensor(0.0896, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7661  batch loss:  tensor(0.0927, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7662  batch loss:  tensor(0.0732, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7663  batch loss:  tensor(0.0976, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7664  batch loss:  tensor(0.0589, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7665  batch loss:  tensor(0.0959, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7666  batch loss:  tensor(0.0690, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7667  batch loss:  tensor(0.0637, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7668  batch loss:  tensor(0.1907, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7669  batch loss:  tensor(0.1599, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7670  batch loss:  tensor(0.1211, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7671  batch loss:  tensor(0.0873, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7672  batch loss:  tensor(0.0625, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7673  batch loss:  tensor(0.0616, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7674  batch loss:  tensor(0.0829, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7675  batch loss:  tensor(0.1384, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7676  batch loss:  tensor(0.0803, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7677  batch loss:  tensor(0.1102, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7678  batch loss:  tensor(0.1722, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7679  batch loss:  tensor(0.1160, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7680  batch loss:  tensor(0.0454, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7681  batch loss:  tensor(0.0544, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7682  batch loss:  tensor(0.1359, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7683  batch loss:  tensor(0.0609, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7684  batch loss:  tensor(0.1182, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7685  batch loss:  tensor(0.0702, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7686  batch loss:  tensor(0.0643, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7687  batch loss:  tensor(0.1017, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7688  batch loss:  tensor(0.0844, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7689  batch loss:  tensor(0.1638, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7690  batch loss:  tensor(0.0644, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7691  batch loss:  tensor(0.1360, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7692  batch loss:  tensor(0.1278, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7693  batch loss:  tensor(0.1378, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7694  batch loss:  tensor(0.1086, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7695  batch loss:  tensor(0.0490, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7696  batch loss:  tensor(0.0896, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7697  batch loss:  tensor(0.0975, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7698  batch loss:  tensor(0.0490, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7699  batch loss:  tensor(0.0867, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7700  batch loss:  tensor(0.0816, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7701  batch loss:  tensor(0.1018, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7702  batch loss:  tensor(0.1105, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7703  batch loss:  tensor(0.1486, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7704  batch loss:  tensor(0.1306, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7705  batch loss:  tensor(0.0934, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7706  batch loss:  tensor(0.0772, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7707  batch loss:  tensor(0.0767, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7708  batch loss:  tensor(0.0903, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7709  batch loss:  tensor(0.0961, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7710  batch loss:  tensor(0.0813, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7711  batch loss:  tensor(0.0773, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7712  batch loss:  tensor(0.1131, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7713  batch loss:  tensor(0.0582, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7714  batch loss:  tensor(0.0718, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7715  batch loss:  tensor(0.0610, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7716  batch loss:  tensor(0.0849, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7717  batch loss:  tensor(0.0806, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7718  batch loss:  tensor(0.1302, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7719  batch loss:  tensor(0.0790, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7720  batch loss:  tensor(0.1451, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7721  batch loss:  tensor(0.0920, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7722  batch loss:  tensor(0.0769, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7723  batch loss:  tensor(0.2494, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7724  batch loss:  tensor(0.1130, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7725  batch loss:  tensor(0.1288, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7726  batch loss:  tensor(0.2059, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7727  batch loss:  tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7728  batch loss:  tensor(0.0480, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7729  batch loss:  tensor(0.0813, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7730  batch loss:  tensor(0.0788, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7731  batch loss:  tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7732  batch loss:  tensor(0.0854, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7733  batch loss:  tensor(0.1305, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7734  batch loss:  tensor(0.1221, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7735  batch loss:  tensor(0.0902, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7736  batch loss:  tensor(0.0994, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7737  batch loss:  tensor(0.0508, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7738  batch loss:  tensor(0.1534, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7739  batch loss:  tensor(0.1813, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7740  batch loss:  tensor(0.1013, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7741  batch loss:  tensor(0.0739, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7742  batch loss:  tensor(0.1096, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7743  batch loss:  tensor(0.1150, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7744  batch loss:  tensor(0.1603, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7745  batch loss:  tensor(0.0870, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7746  batch loss:  tensor(0.0814, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7747  batch loss:  tensor(0.0755, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7748  batch loss:  tensor(0.0737, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7749  batch loss:  tensor(0.0719, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7750  batch loss:  tensor(0.0875, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7751  batch loss:  tensor(0.0546, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7752  batch loss:  tensor(0.0648, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7753  batch loss:  tensor(0.1289, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7754  batch loss:  tensor(0.0648, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7755  batch loss:  tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7756  batch loss:  tensor(0.0959, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7757  batch loss:  tensor(0.0506, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7758  batch loss:  tensor(0.0963, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7759  batch loss:  tensor(0.1206, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7760  batch loss:  tensor(0.0828, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7761  batch loss:  tensor(0.0676, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7762  batch loss:  tensor(0.1793, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7763  batch loss:  tensor(0.0610, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7764  batch loss:  tensor(0.0622, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7765  batch loss:  tensor(0.0798, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7766  batch loss:  tensor(0.0613, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7767  batch loss:  tensor(0.1054, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7768  batch loss:  tensor(0.0882, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7769  batch loss:  tensor(0.1279, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7770  batch loss:  tensor(0.0544, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7771  batch loss:  tensor(0.1122, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7772  batch loss:  tensor(0.0431, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7773  batch loss:  tensor(0.1008, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7774  batch loss:  tensor(0.0895, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7775  batch loss:  tensor(0.1113, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7776  batch loss:  tensor(0.0759, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7777  batch loss:  tensor(0.1250, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7778  batch loss:  tensor(0.1013, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7779  batch loss:  tensor(0.1173, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7780  batch loss:  tensor(0.1719, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7781  batch loss:  tensor(0.0410, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7782  batch loss:  tensor(0.1076, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7783  batch loss:  tensor(0.0578, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7784  batch loss:  tensor(0.0759, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7785  batch loss:  tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7786  batch loss:  tensor(0.0577, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7787  batch loss:  tensor(0.0794, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7788  batch loss:  tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7789  batch loss:  tensor(0.0692, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7790  batch loss:  tensor(0.0986, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7791  batch loss:  tensor(0.0858, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7792  batch loss:  tensor(0.0738, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7793  batch loss:  tensor(0.0869, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7794  batch loss:  tensor(0.0522, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7795  batch loss:  tensor(0.0859, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7796  batch loss:  tensor(0.0771, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7797  batch loss:  tensor(0.0536, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7798  batch loss:  tensor(0.0645, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7799  batch loss:  tensor(0.0707, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7800  batch loss:  tensor(0.0858, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7801  batch loss:  tensor(0.0934, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7802  batch loss:  tensor(0.0825, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7803  batch loss:  tensor(0.1445, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7804  batch loss:  tensor(0.1609, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7805  batch loss:  tensor(0.0767, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7806  batch loss:  tensor(0.1445, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7807  batch loss:  tensor(0.0878, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7808  batch loss:  tensor(0.0777, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7809  batch loss:  tensor(0.0894, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7810  batch loss:  tensor(0.0917, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7811  batch loss:  tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7812  batch loss:  tensor(0.0829, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7813  batch loss:  tensor(0.0579, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7814  batch loss:  tensor(0.0462, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7815  batch loss:  tensor(0.0548, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7816  batch loss:  tensor(0.1269, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7817  batch loss:  tensor(0.0662, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7818  batch loss:  tensor(0.0729, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7819  batch loss:  tensor(0.0831, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7820  batch loss:  tensor(0.0437, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7821  batch loss:  tensor(0.0717, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7822  batch loss:  tensor(0.0845, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7823  batch loss:  tensor(0.0910, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7824  batch loss:  tensor(0.0820, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7825  batch loss:  tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7826  batch loss:  tensor(0.0689, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7827  batch loss:  tensor(0.0420, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7828  batch loss:  tensor(0.1654, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7829  batch loss:  tensor(0.0732, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7830  batch loss:  tensor(0.1045, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7831  batch loss:  tensor(0.0649, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7832  batch loss:  tensor(0.1781, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7833  batch loss:  tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7834  batch loss:  tensor(0.1375, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7835  batch loss:  tensor(0.0975, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7836  batch loss:  tensor(0.0931, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7837  batch loss:  tensor(0.0762, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7838  batch loss:  tensor(0.1040, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7839  batch loss:  tensor(0.1187, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7840  batch loss:  tensor(0.0413, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7841  batch loss:  tensor(0.1295, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7842  batch loss:  tensor(0.1260, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7843  batch loss:  tensor(0.3041, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7844  batch loss:  tensor(0.0872, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7845  batch loss:  tensor(0.0632, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7846  batch loss:  tensor(0.1332, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7847  batch loss:  tensor(0.1006, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7848  batch loss:  tensor(0.0730, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7849  batch loss:  tensor(0.0840, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7850  batch loss:  tensor(0.1052, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7851  batch loss:  tensor(0.0828, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7852  batch loss:  tensor(0.0718, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7853  batch loss:  tensor(0.0711, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7854  batch loss:  tensor(0.1233, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7855  batch loss:  tensor(0.1489, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7856  batch loss:  tensor(0.1114, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7857  batch loss:  tensor(0.1147, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7858  batch loss:  tensor(0.0699, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7859  batch loss:  tensor(0.0773, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7860  batch loss:  tensor(0.0778, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7861  batch loss:  tensor(0.1218, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7862  batch loss:  tensor(0.0491, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7863  batch loss:  tensor(0.1481, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7864  batch loss:  tensor(0.1370, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7865  batch loss:  tensor(0.1460, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7866  batch loss:  tensor(0.1100, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7867  batch loss:  tensor(0.0524, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7868  batch loss:  tensor(0.1265, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7869  batch loss:  tensor(0.1573, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7870  batch loss:  tensor(0.0687, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7871  batch loss:  tensor(0.1178, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7872  batch loss:  tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7873  batch loss:  tensor(0.1524, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7874  batch loss:  tensor(0.0760, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7875  batch loss:  tensor(0.0750, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7876  batch loss:  tensor(0.0876, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7877  batch loss:  tensor(0.0777, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7878  batch loss:  tensor(0.0363, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7879  batch loss:  tensor(0.1337, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7880  batch loss:  tensor(0.0598, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7881  batch loss:  tensor(0.0687, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7882  batch loss:  tensor(0.0852, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7883  batch loss:  tensor(0.0537, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7884  batch loss:  tensor(0.1178, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7885  batch loss:  tensor(0.0784, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7886  batch loss:  tensor(0.0922, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7887  batch loss:  tensor(0.0627, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7888  batch loss:  tensor(0.0783, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7889  batch loss:  tensor(0.1340, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7890  batch loss:  tensor(0.1586, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7891  batch loss:  tensor(0.0681, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7892  batch loss:  tensor(0.0813, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7893  batch loss:  tensor(0.0802, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7894  batch loss:  tensor(0.1444, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7895  batch loss:  tensor(0.0488, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7896  batch loss:  tensor(0.0673, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7897  batch loss:  tensor(0.0867, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7898  batch loss:  tensor(0.1091, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7899  batch loss:  tensor(0.0690, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7900  batch loss:  tensor(0.0647, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7901  batch loss:  tensor(0.1206, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7902  batch loss:  tensor(0.1184, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7903  batch loss:  tensor(0.1175, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7904  batch loss:  tensor(0.0707, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7905  batch loss:  tensor(0.0865, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7906  batch loss:  tensor(0.0539, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7907  batch loss:  tensor(0.1429, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7908  batch loss:  tensor(0.0702, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7909  batch loss:  tensor(0.0625, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7910  batch loss:  tensor(0.1385, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7911  batch loss:  tensor(0.1497, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7912  batch loss:  tensor(0.0934, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7913  batch loss:  tensor(0.1293, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7914  batch loss:  tensor(0.0968, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7915  batch loss:  tensor(0.1078, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7916  batch loss:  tensor(0.0777, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7917  batch loss:  tensor(0.1184, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7918  batch loss:  tensor(0.1007, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7919  batch loss:  tensor(0.0666, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7920  batch loss:  tensor(0.0858, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7921  batch loss:  tensor(0.0938, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7922  batch loss:  tensor(0.1779, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7923  batch loss:  tensor(0.0951, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7924  batch loss:  tensor(0.0759, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7925  batch loss:  tensor(0.1175, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7926  batch loss:  tensor(0.0786, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7927  batch loss:  tensor(0.0941, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7928  batch loss:  tensor(0.0722, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7929  batch loss:  tensor(0.0926, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7930  batch loss:  tensor(0.0575, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7931  batch loss:  tensor(0.0952, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7932  batch loss:  tensor(0.1006, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7933  batch loss:  tensor(0.0701, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7934  batch loss:  tensor(0.1082, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7935  batch loss:  tensor(0.3157, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7936  batch loss:  tensor(0.1312, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7937  batch loss:  tensor(0.1929, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7938  batch loss:  tensor(0.0798, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7939  batch loss:  tensor(0.0775, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7940  batch loss:  tensor(0.1026, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7941  batch loss:  tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7942  batch loss:  tensor(0.1788, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7943  batch loss:  tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7944  batch loss:  tensor(0.0711, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7945  batch loss:  tensor(0.0965, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7946  batch loss:  tensor(0.0665, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7947  batch loss:  tensor(0.0923, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7948  batch loss:  tensor(0.0966, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7949  batch loss:  tensor(0.1063, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7950  batch loss:  tensor(0.0921, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7951  batch loss:  tensor(0.0669, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7952  batch loss:  tensor(0.0695, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7953  batch loss:  tensor(0.1053, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7954  batch loss:  tensor(0.1150, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7955  batch loss:  tensor(0.0764, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7956  batch loss:  tensor(0.0978, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7957  batch loss:  tensor(0.0703, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7958  batch loss:  tensor(0.0757, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7959  batch loss:  tensor(0.0918, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7960  batch loss:  tensor(0.1251, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7961  batch loss:  tensor(0.1499, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7962  batch loss:  tensor(0.1440, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7963  batch loss:  tensor(0.0445, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7964  batch loss:  tensor(0.1218, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7965  batch loss:  tensor(0.0862, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7966  batch loss:  tensor(0.0714, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7967  batch loss:  tensor(0.0894, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7968  batch loss:  tensor(0.0974, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7969  batch loss:  tensor(0.2520, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7970  batch loss:  tensor(0.1073, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7971  batch loss:  tensor(0.0614, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7972  batch loss:  tensor(0.0702, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7973  batch loss:  tensor(0.0656, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7974  batch loss:  tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7975  batch loss:  tensor(0.0919, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7976  batch loss:  tensor(0.0921, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7977  batch loss:  tensor(0.0518, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7978  batch loss:  tensor(0.0880, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7979  batch loss:  tensor(0.0767, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7980  batch loss:  tensor(0.0826, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7981  batch loss:  tensor(0.0803, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7982  batch loss:  tensor(0.0789, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7983  batch loss:  tensor(0.0601, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7984  batch loss:  tensor(0.1094, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7985  batch loss:  tensor(0.0526, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7986  batch loss:  tensor(0.0688, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7987  batch loss:  tensor(0.1090, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7988  batch loss:  tensor(0.0899, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7989  batch loss:  tensor(0.0676, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7990  batch loss:  tensor(0.0641, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7991  batch loss:  tensor(0.0803, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7992  batch loss:  tensor(0.0779, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7993  batch loss:  tensor(0.1075, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7994  batch loss:  tensor(0.0727, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7995  batch loss:  tensor(0.0839, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7996  batch loss:  tensor(0.1383, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7997  batch loss:  tensor(0.0594, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7998  batch loss:  tensor(0.1000, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  7999  batch loss:  tensor(0.0429, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8000  batch loss:  tensor(0.0858, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8001  batch loss:  tensor(0.0753, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8002  batch loss:  tensor(0.1037, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8003  batch loss:  tensor(0.0769, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8004  batch loss:  tensor(0.1262, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8005  batch loss:  tensor(0.1219, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8006  batch loss:  tensor(0.1028, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8007  batch loss:  tensor(0.0996, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8008  batch loss:  tensor(0.0658, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8009  batch loss:  tensor(0.0604, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8010  batch loss:  tensor(0.0630, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8011  batch loss:  tensor(0.0694, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8012  batch loss:  tensor(0.0789, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8013  batch loss:  tensor(0.0625, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8014  batch loss:  tensor(0.0464, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8015  batch loss:  tensor(0.0855, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8016  batch loss:  tensor(0.0997, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8017  batch loss:  tensor(0.0758, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8018  batch loss:  tensor(0.1602, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8019  batch loss:  tensor(0.0618, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8020  batch loss:  tensor(0.0801, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8021  batch loss:  tensor(0.0632, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8022  batch loss:  tensor(0.1152, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8023  batch loss:  tensor(0.0950, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8024  batch loss:  tensor(0.0740, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8025  batch loss:  tensor(0.1182, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8026  batch loss:  tensor(0.1308, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8027  batch loss:  tensor(0.1223, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8028  batch loss:  tensor(0.0903, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8029  batch loss:  tensor(0.0812, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8030  batch loss:  tensor(0.1103, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8031  batch loss:  tensor(0.0761, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8032  batch loss:  tensor(0.1664, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8033  batch loss:  tensor(0.1247, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8034  batch loss:  tensor(0.1155, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8035  batch loss:  tensor(0.0833, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8036  batch loss:  tensor(0.1059, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8037  batch loss:  tensor(0.0573, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8038  batch loss:  tensor(0.1272, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8039  batch loss:  tensor(0.0902, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8040  batch loss:  tensor(0.2899, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8041  batch loss:  tensor(0.1525, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8042  batch loss:  tensor(0.0816, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8043  batch loss:  tensor(0.0941, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8044  batch loss:  tensor(0.1137, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8045  batch loss:  tensor(0.0600, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8046  batch loss:  tensor(0.0580, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8047  batch loss:  tensor(0.1441, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8048  batch loss:  tensor(0.0998, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8049  batch loss:  tensor(0.1196, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8050  batch loss:  tensor(0.0956, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8051  batch loss:  tensor(0.2996, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8052  batch loss:  tensor(0.2004, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8053  batch loss:  tensor(0.1039, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8054  batch loss:  tensor(0.0873, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8055  batch loss:  tensor(0.1017, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8056  batch loss:  tensor(0.1434, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8057  batch loss:  tensor(0.1082, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8058  batch loss:  tensor(0.0572, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8059  batch loss:  tensor(0.0660, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8060  batch loss:  tensor(0.1536, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8061  batch loss:  tensor(0.0496, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8062  batch loss:  tensor(0.0862, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8063  batch loss:  tensor(0.0651, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8064  batch loss:  tensor(0.0932, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8065  batch loss:  tensor(0.0591, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8066  batch loss:  tensor(0.0706, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8067  batch loss:  tensor(0.0560, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8068  batch loss:  tensor(0.0620, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8069  batch loss:  tensor(0.1008, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8070  batch loss:  tensor(0.0516, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8071  batch loss:  tensor(0.1445, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8072  batch loss:  tensor(0.0945, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8073  batch loss:  tensor(0.0565, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8074  batch loss:  tensor(0.1817, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8075  batch loss:  tensor(0.2022, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8076  batch loss:  tensor(0.1948, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8077  batch loss:  tensor(0.1158, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8078  batch loss:  tensor(0.0889, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8079  batch loss:  tensor(0.0901, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8080  batch loss:  tensor(0.0651, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8081  batch loss:  tensor(0.0612, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8082  batch loss:  tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8083  batch loss:  tensor(0.1169, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8084  batch loss:  tensor(0.0879, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8085  batch loss:  tensor(0.1486, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8086  batch loss:  tensor(0.0683, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8087  batch loss:  tensor(0.1083, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8088  batch loss:  tensor(0.0911, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8089  batch loss:  tensor(0.0899, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8090  batch loss:  tensor(0.0732, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8091  batch loss:  tensor(0.1110, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8092  batch loss:  tensor(0.0721, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8093  batch loss:  tensor(0.1013, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8094  batch loss:  tensor(0.0965, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8095  batch loss:  tensor(0.0709, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8096  batch loss:  tensor(0.0550, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8097  batch loss:  tensor(0.0518, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8098  batch loss:  tensor(0.2757, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8099  batch loss:  tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8100  batch loss:  tensor(0.0534, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8101  batch loss:  tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8102  batch loss:  tensor(0.0514, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8103  batch loss:  tensor(0.0674, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8104  batch loss:  tensor(0.2517, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8105  batch loss:  tensor(0.1044, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8106  batch loss:  tensor(0.1488, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8107  batch loss:  tensor(0.0617, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8108  batch loss:  tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8109  batch loss:  tensor(0.0778, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8110  batch loss:  tensor(0.1716, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8111  batch loss:  tensor(0.0818, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8112  batch loss:  tensor(0.0831, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8113  batch loss:  tensor(0.1085, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8114  batch loss:  tensor(0.0912, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8115  batch loss:  tensor(0.0834, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8116  batch loss:  tensor(0.1212, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8117  batch loss:  tensor(0.1090, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8118  batch loss:  tensor(0.1835, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8119  batch loss:  tensor(0.0721, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8120  batch loss:  tensor(0.0741, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8121  batch loss:  tensor(0.1519, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8122  batch loss:  tensor(0.0623, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8123  batch loss:  tensor(0.0598, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8124  batch loss:  tensor(0.0694, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8125  batch loss:  tensor(0.1160, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8126  batch loss:  tensor(0.0521, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8127  batch loss:  tensor(0.1423, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8128  batch loss:  tensor(0.0594, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8129  batch loss:  tensor(0.0662, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8130  batch loss:  tensor(0.0763, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8131  batch loss:  tensor(0.0712, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8132  batch loss:  tensor(0.0869, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8133  batch loss:  tensor(0.0583, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8134  batch loss:  tensor(0.0572, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8135  batch loss:  tensor(0.0851, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8136  batch loss:  tensor(0.0779, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8137  batch loss:  tensor(0.1833, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8138  batch loss:  tensor(0.1029, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8139  batch loss:  tensor(0.1346, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8140  batch loss:  tensor(0.0619, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8141  batch loss:  tensor(0.0663, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8142  batch loss:  tensor(0.0881, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8143  batch loss:  tensor(0.0816, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8144  batch loss:  tensor(0.0931, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8145  batch loss:  tensor(0.1064, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8146  batch loss:  tensor(0.1240, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8147  batch loss:  tensor(0.1386, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8148  batch loss:  tensor(0.1072, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8149  batch loss:  tensor(0.0817, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8150  batch loss:  tensor(0.0609, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8151  batch loss:  tensor(0.1303, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8152  batch loss:  tensor(0.1126, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8153  batch loss:  tensor(0.0785, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8154  batch loss:  tensor(0.0694, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8155  batch loss:  tensor(0.0959, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8156  batch loss:  tensor(0.1461, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8157  batch loss:  tensor(0.0749, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8158  batch loss:  tensor(0.1177, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8159  batch loss:  tensor(0.1251, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8160  batch loss:  tensor(0.0807, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8161  batch loss:  tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8162  batch loss:  tensor(0.0489, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8163  batch loss:  tensor(0.1911, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8164  batch loss:  tensor(0.0609, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8165  batch loss:  tensor(0.1024, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8166  batch loss:  tensor(0.1531, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8167  batch loss:  tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8168  batch loss:  tensor(0.0630, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8169  batch loss:  tensor(0.1445, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8170  batch loss:  tensor(0.0852, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8171  batch loss:  tensor(0.1304, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8172  batch loss:  tensor(0.0587, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8173  batch loss:  tensor(0.0815, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8174  batch loss:  tensor(0.0435, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8175  batch loss:  tensor(0.0647, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8176  batch loss:  tensor(0.1484, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8177  batch loss:  tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8178  batch loss:  tensor(0.0897, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8179  batch loss:  tensor(0.0591, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8180  batch loss:  tensor(0.0990, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8181  batch loss:  tensor(0.1505, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8182  batch loss:  tensor(0.0459, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8183  batch loss:  tensor(0.0662, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8184  batch loss:  tensor(0.0913, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8185  batch loss:  tensor(0.0575, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8186  batch loss:  tensor(0.0875, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8187  batch loss:  tensor(0.0876, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8188  batch loss:  tensor(0.0628, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8189  batch loss:  tensor(0.0792, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8190  batch loss:  tensor(0.1105, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8191  batch loss:  tensor(0.1533, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8192  batch loss:  tensor(0.0825, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8193  batch loss:  tensor(0.0658, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8194  batch loss:  tensor(0.0851, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8195  batch loss:  tensor(0.0510, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8196  batch loss:  tensor(0.0873, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8197  batch loss:  tensor(0.1080, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8198  batch loss:  tensor(0.0740, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8199  batch loss:  tensor(0.1127, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8200  batch loss:  tensor(0.0817, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8201  batch loss:  tensor(0.0756, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8202  batch loss:  tensor(0.1130, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8203  batch loss:  tensor(0.0578, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8204  batch loss:  tensor(0.0798, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8205  batch loss:  tensor(0.0480, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8206  batch loss:  tensor(0.0545, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8207  batch loss:  tensor(0.0539, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8208  batch loss:  tensor(0.0661, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8209  batch loss:  tensor(0.0985, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8210  batch loss:  tensor(0.0817, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8211  batch loss:  tensor(0.1130, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8212  batch loss:  tensor(0.1061, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8213  batch loss:  tensor(0.1199, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8214  batch loss:  tensor(0.0869, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8215  batch loss:  tensor(0.0964, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8216  batch loss:  tensor(0.1436, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8217  batch loss:  tensor(0.2085, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8218  batch loss:  tensor(0.1818, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8219  batch loss:  tensor(0.0929, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8220  batch loss:  tensor(0.1050, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8221  batch loss:  tensor(0.0949, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8222  batch loss:  tensor(0.1335, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8223  batch loss:  tensor(0.1461, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8224  batch loss:  tensor(0.1181, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8225  batch loss:  tensor(0.1023, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8226  batch loss:  tensor(0.1686, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8227  batch loss:  tensor(0.1470, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8228  batch loss:  tensor(0.1772, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8229  batch loss:  tensor(0.0989, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8230  batch loss:  tensor(0.1587, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8231  batch loss:  tensor(0.0867, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8232  batch loss:  tensor(0.1204, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8233  batch loss:  tensor(0.1401, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8234  batch loss:  tensor(0.0835, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8235  batch loss:  tensor(0.0876, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8236  batch loss:  tensor(0.1603, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8237  batch loss:  tensor(0.0716, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8238  batch loss:  tensor(0.1077, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8239  batch loss:  tensor(0.1095, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8240  batch loss:  tensor(0.0893, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8241  batch loss:  tensor(0.1024, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8242  batch loss:  tensor(0.1267, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8243  batch loss:  tensor(0.1259, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8244  batch loss:  tensor(0.0880, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8245  batch loss:  tensor(0.1206, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8246  batch loss:  tensor(0.0827, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8247  batch loss:  tensor(0.0647, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8248  batch loss:  tensor(0.1427, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8249  batch loss:  tensor(0.1217, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8250  batch loss:  tensor(0.0980, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8251  batch loss:  tensor(0.0649, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8252  batch loss:  tensor(0.2691, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8253  batch loss:  tensor(0.0832, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8254  batch loss:  tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8255  batch loss:  tensor(0.1197, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8256  batch loss:  tensor(0.0903, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8257  batch loss:  tensor(0.0929, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8258  batch loss:  tensor(0.0596, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8259  batch loss:  tensor(0.1654, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8260  batch loss:  tensor(0.0631, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8261  batch loss:  tensor(0.0617, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8262  batch loss:  tensor(0.0932, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8263  batch loss:  tensor(0.0849, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8264  batch loss:  tensor(0.1150, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8265  batch loss:  tensor(0.2184, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8266  batch loss:  tensor(0.0583, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8267  batch loss:  tensor(0.0641, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8268  batch loss:  tensor(0.0829, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8269  batch loss:  tensor(0.1114, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8270  batch loss:  tensor(0.0962, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8271  batch loss:  tensor(0.1034, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8272  batch loss:  tensor(0.0862, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8273  batch loss:  tensor(0.0885, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8274  batch loss:  tensor(0.0895, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8275  batch loss:  tensor(0.0906, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8276  batch loss:  tensor(0.0938, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8277  batch loss:  tensor(0.0840, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8278  batch loss:  tensor(0.1327, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8279  batch loss:  tensor(0.0779, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8280  batch loss:  tensor(0.0614, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8281  batch loss:  tensor(0.0977, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8282  batch loss:  tensor(0.0617, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8283  batch loss:  tensor(0.0736, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8284  batch loss:  tensor(0.0720, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8285  batch loss:  tensor(0.0672, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8286  batch loss:  tensor(0.1424, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8287  batch loss:  tensor(0.0908, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8288  batch loss:  tensor(0.4220, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8289  batch loss:  tensor(0.1361, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8290  batch loss:  tensor(0.0769, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8291  batch loss:  tensor(0.2204, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8292  batch loss:  tensor(0.1072, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8293  batch loss:  tensor(0.0954, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8294  batch loss:  tensor(0.0639, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8295  batch loss:  tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8296  batch loss:  tensor(0.1157, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8297  batch loss:  tensor(0.0704, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8298  batch loss:  tensor(0.0798, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8299  batch loss:  tensor(0.0831, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8300  batch loss:  tensor(0.0765, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8301  batch loss:  tensor(0.0801, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8302  batch loss:  tensor(0.0821, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8303  batch loss:  tensor(0.0518, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8304  batch loss:  tensor(0.0427, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8305  batch loss:  tensor(0.0947, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8306  batch loss:  tensor(0.1405, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8307  batch loss:  tensor(0.1215, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8308  batch loss:  tensor(0.1009, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8309  batch loss:  tensor(0.0954, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8310  batch loss:  tensor(0.0961, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8311  batch loss:  tensor(0.1033, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8312  batch loss:  tensor(0.0693, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8313  batch loss:  tensor(0.0672, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8314  batch loss:  tensor(0.2187, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8315  batch loss:  tensor(0.0881, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8316  batch loss:  tensor(0.0592, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8317  batch loss:  tensor(0.1032, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8318  batch loss:  tensor(0.0760, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8319  batch loss:  tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8320  batch loss:  tensor(0.0806, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8321  batch loss:  tensor(0.1016, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8322  batch loss:  tensor(0.0451, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8323  batch loss:  tensor(0.0922, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8324  batch loss:  tensor(0.1312, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8325  batch loss:  tensor(0.2165, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8326  batch loss:  tensor(0.0559, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8327  batch loss:  tensor(0.1061, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8328  batch loss:  tensor(0.0951, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8329  batch loss:  tensor(0.0495, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8330  batch loss:  tensor(0.0918, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8331  batch loss:  tensor(0.0807, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8332  batch loss:  tensor(0.0837, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8333  batch loss:  tensor(0.1283, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8334  batch loss:  tensor(0.1183, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8335  batch loss:  tensor(0.0757, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8336  batch loss:  tensor(0.1053, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8337  batch loss:  tensor(0.1554, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8338  batch loss:  tensor(0.0880, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8339  batch loss:  tensor(0.1217, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8340  batch loss:  tensor(0.1616, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8341  batch loss:  tensor(0.1221, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8342  batch loss:  tensor(0.1499, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8343  batch loss:  tensor(0.1140, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8344  batch loss:  tensor(0.0628, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8345  batch loss:  tensor(0.0646, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8346  batch loss:  tensor(0.1299, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8347  batch loss:  tensor(0.0976, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8348  batch loss:  tensor(0.1285, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8349  batch loss:  tensor(0.1714, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8350  batch loss:  tensor(0.1006, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8351  batch loss:  tensor(0.0766, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8352  batch loss:  tensor(0.1459, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8353  batch loss:  tensor(0.0888, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8354  batch loss:  tensor(0.1055, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8355  batch loss:  tensor(0.0886, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8356  batch loss:  tensor(0.0667, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8357  batch loss:  tensor(0.0701, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8358  batch loss:  tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8359  batch loss:  tensor(0.0798, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8360  batch loss:  tensor(0.0586, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8361  batch loss:  tensor(0.0875, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8362  batch loss:  tensor(0.0809, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8363  batch loss:  tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8364  batch loss:  tensor(0.0559, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8365  batch loss:  tensor(0.0988, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8366  batch loss:  tensor(0.1089, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8367  batch loss:  tensor(0.1517, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8368  batch loss:  tensor(0.0968, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8369  batch loss:  tensor(0.0617, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8370  batch loss:  tensor(0.0650, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8371  batch loss:  tensor(0.1284, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8372  batch loss:  tensor(0.0786, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8373  batch loss:  tensor(0.0886, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8374  batch loss:  tensor(0.1467, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8375  batch loss:  tensor(0.1173, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8376  batch loss:  tensor(0.1689, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8377  batch loss:  tensor(0.0922, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8378  batch loss:  tensor(0.1343, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8379  batch loss:  tensor(0.1020, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8380  batch loss:  tensor(0.0751, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8381  batch loss:  tensor(0.0799, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8382  batch loss:  tensor(0.0448, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8383  batch loss:  tensor(0.0843, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8384  batch loss:  tensor(0.0652, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8385  batch loss:  tensor(0.1116, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8386  batch loss:  tensor(0.0746, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8387  batch loss:  tensor(0.1175, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8388  batch loss:  tensor(0.0795, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8389  batch loss:  tensor(0.0911, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8390  batch loss:  tensor(0.0635, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8391  batch loss:  tensor(0.1203, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8392  batch loss:  tensor(0.0844, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8393  batch loss:  tensor(0.0407, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8394  batch loss:  tensor(0.1019, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8395  batch loss:  tensor(0.1052, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8396  batch loss:  tensor(0.0534, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8397  batch loss:  tensor(0.0690, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8398  batch loss:  tensor(0.0818, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8399  batch loss:  tensor(0.0919, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8400  batch loss:  tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8401  batch loss:  tensor(0.0586, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8402  batch loss:  tensor(0.0363, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8403  batch loss:  tensor(0.1365, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8404  batch loss:  tensor(0.1055, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8405  batch loss:  tensor(0.0869, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8406  batch loss:  tensor(0.1131, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8407  batch loss:  tensor(0.0435, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8408  batch loss:  tensor(0.0636, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8409  batch loss:  tensor(0.0907, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8410  batch loss:  tensor(0.1201, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8411  batch loss:  tensor(0.1230, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8412  batch loss:  tensor(0.1623, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8413  batch loss:  tensor(0.2102, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8414  batch loss:  tensor(0.1516, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8415  batch loss:  tensor(0.0647, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8416  batch loss:  tensor(0.1410, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8417  batch loss:  tensor(0.1087, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8418  batch loss:  tensor(0.1467, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8419  batch loss:  tensor(0.1120, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8420  batch loss:  tensor(0.0862, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8421  batch loss:  tensor(0.0714, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8422  batch loss:  tensor(0.0827, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8423  batch loss:  tensor(0.0580, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8424  batch loss:  tensor(0.0939, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8425  batch loss:  tensor(0.1118, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8426  batch loss:  tensor(0.0592, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8427  batch loss:  tensor(0.1429, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8428  batch loss:  tensor(0.1461, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8429  batch loss:  tensor(0.0660, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8430  batch loss:  tensor(0.0852, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8431  batch loss:  tensor(0.1783, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8432  batch loss:  tensor(0.1347, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8433  batch loss:  tensor(0.0610, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8434  batch loss:  tensor(0.1134, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8435  batch loss:  tensor(0.1351, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8436  batch loss:  tensor(0.0883, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8437  batch loss:  tensor(0.0932, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8438  batch loss:  tensor(0.0617, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8439  batch loss:  tensor(0.0582, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8440  batch loss:  tensor(0.0638, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8441  batch loss:  tensor(0.0516, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8442  batch loss:  tensor(0.1526, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8443  batch loss:  tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8444  batch loss:  tensor(0.0702, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8445  batch loss:  tensor(0.1408, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8446  batch loss:  tensor(0.1070, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8447  batch loss:  tensor(0.0755, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8448  batch loss:  tensor(0.1212, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8449  batch loss:  tensor(0.0637, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8450  batch loss:  tensor(0.0483, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8451  batch loss:  tensor(0.1559, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8452  batch loss:  tensor(0.1397, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8453  batch loss:  tensor(0.0833, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8454  batch loss:  tensor(0.1038, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8455  batch loss:  tensor(0.1860, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8456  batch loss:  tensor(0.1024, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8457  batch loss:  tensor(0.1349, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8458  batch loss:  tensor(0.1234, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8459  batch loss:  tensor(0.0937, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8460  batch loss:  tensor(0.0922, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8461  batch loss:  tensor(0.0569, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8462  batch loss:  tensor(0.0543, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8463  batch loss:  tensor(0.0522, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8464  batch loss:  tensor(0.1141, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8465  batch loss:  tensor(0.0505, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8466  batch loss:  tensor(0.0603, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8467  batch loss:  tensor(0.1193, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8468  batch loss:  tensor(0.0942, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8469  batch loss:  tensor(0.0668, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8470  batch loss:  tensor(0.0518, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8471  batch loss:  tensor(0.1042, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8472  batch loss:  tensor(0.0538, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8473  batch loss:  tensor(0.1216, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8474  batch loss:  tensor(0.0929, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8475  batch loss:  tensor(0.1945, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8476  batch loss:  tensor(0.1059, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8477  batch loss:  tensor(0.0683, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8478  batch loss:  tensor(0.1259, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8479  batch loss:  tensor(0.1034, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8480  batch loss:  tensor(0.1042, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8481  batch loss:  tensor(0.1453, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8482  batch loss:  tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8483  batch loss:  tensor(0.0507, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8484  batch loss:  tensor(0.0949, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8485  batch loss:  tensor(0.0843, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8486  batch loss:  tensor(0.0981, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8487  batch loss:  tensor(0.2023, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8488  batch loss:  tensor(0.1212, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8489  batch loss:  tensor(0.1195, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8490  batch loss:  tensor(0.0936, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8491  batch loss:  tensor(0.1252, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8492  batch loss:  tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8493  batch loss:  tensor(0.0669, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8494  batch loss:  tensor(0.0742, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8495  batch loss:  tensor(0.0978, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8496  batch loss:  tensor(0.1354, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8497  batch loss:  tensor(0.0710, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8498  batch loss:  tensor(0.0825, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8499  batch loss:  tensor(0.0877, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8500  batch loss:  tensor(0.0510, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8501  batch loss:  tensor(0.0543, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8502  batch loss:  tensor(0.0758, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8503  batch loss:  tensor(0.0897, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8504  batch loss:  tensor(0.0664, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8505  batch loss:  tensor(0.1201, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8506  batch loss:  tensor(0.0586, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8507  batch loss:  tensor(0.0670, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8508  batch loss:  tensor(0.0888, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8509  batch loss:  tensor(0.0738, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8510  batch loss:  tensor(0.1050, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8511  batch loss:  tensor(0.1459, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8512  batch loss:  tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8513  batch loss:  tensor(0.0841, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8514  batch loss:  tensor(0.1252, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8515  batch loss:  tensor(0.1228, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8516  batch loss:  tensor(0.0785, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8517  batch loss:  tensor(0.0756, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8518  batch loss:  tensor(0.1196, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8519  batch loss:  tensor(0.1035, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8520  batch loss:  tensor(0.0613, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8521  batch loss:  tensor(0.1046, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8522  batch loss:  tensor(0.0870, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8523  batch loss:  tensor(0.0543, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8524  batch loss:  tensor(0.1061, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8525  batch loss:  tensor(0.0994, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8526  batch loss:  tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8527  batch loss:  tensor(0.0884, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8528  batch loss:  tensor(0.1000, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8529  batch loss:  tensor(0.0906, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8530  batch loss:  tensor(0.0670, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8531  batch loss:  tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8532  batch loss:  tensor(0.1496, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8533  batch loss:  tensor(0.1318, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8534  batch loss:  tensor(0.1014, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8535  batch loss:  tensor(0.0755, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8536  batch loss:  tensor(0.1314, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8537  batch loss:  tensor(0.0646, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8538  batch loss:  tensor(0.0792, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8539  batch loss:  tensor(0.0426, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8540  batch loss:  tensor(0.0956, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8541  batch loss:  tensor(0.0481, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8542  batch loss:  tensor(0.1777, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8543  batch loss:  tensor(0.0938, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8544  batch loss:  tensor(0.0548, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8545  batch loss:  tensor(0.1435, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8546  batch loss:  tensor(0.1230, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8547  batch loss:  tensor(0.0336, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8548  batch loss:  tensor(0.1355, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8549  batch loss:  tensor(0.0826, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8550  batch loss:  tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8551  batch loss:  tensor(0.0772, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8552  batch loss:  tensor(0.0600, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8553  batch loss:  tensor(0.0907, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8554  batch loss:  tensor(0.0552, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8555  batch loss:  tensor(0.1114, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8556  batch loss:  tensor(0.1286, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8557  batch loss:  tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8558  batch loss:  tensor(0.0919, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8559  batch loss:  tensor(0.0553, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8560  batch loss:  tensor(0.1170, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8561  batch loss:  tensor(0.0899, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8562  batch loss:  tensor(0.0729, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8563  batch loss:  tensor(0.0871, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8564  batch loss:  tensor(0.0835, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8565  batch loss:  tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8566  batch loss:  tensor(0.0835, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8567  batch loss:  tensor(0.1127, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8568  batch loss:  tensor(0.0841, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8569  batch loss:  tensor(0.0681, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8570  batch loss:  tensor(0.0989, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8571  batch loss:  tensor(0.0905, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8572  batch loss:  tensor(0.1083, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8573  batch loss:  tensor(0.0911, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8574  batch loss:  tensor(0.1073, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8575  batch loss:  tensor(0.1917, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8576  batch loss:  tensor(0.0928, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8577  batch loss:  tensor(0.1042, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8578  batch loss:  tensor(0.0479, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8579  batch loss:  tensor(0.0707, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8580  batch loss:  tensor(0.1020, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8581  batch loss:  tensor(0.0919, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8582  batch loss:  tensor(0.1488, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8583  batch loss:  tensor(0.0789, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8584  batch loss:  tensor(0.1449, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8585  batch loss:  tensor(0.0619, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8586  batch loss:  tensor(0.0486, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8587  batch loss:  tensor(0.0840, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8588  batch loss:  tensor(0.0601, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8589  batch loss:  tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8590  batch loss:  tensor(0.0660, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8591  batch loss:  tensor(0.0899, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8592  batch loss:  tensor(0.1181, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8593  batch loss:  tensor(0.0592, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8594  batch loss:  tensor(0.0627, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8595  batch loss:  tensor(0.1119, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8596  batch loss:  tensor(0.0565, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8597  batch loss:  tensor(0.1035, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8598  batch loss:  tensor(0.0670, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8599  batch loss:  tensor(0.0744, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8600  batch loss:  tensor(0.0833, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8601  batch loss:  tensor(0.0733, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8602  batch loss:  tensor(0.0989, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8603  batch loss:  tensor(0.0476, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8604  batch loss:  tensor(0.0734, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8605  batch loss:  tensor(0.0641, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8606  batch loss:  tensor(0.1033, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8607  batch loss:  tensor(0.1919, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8608  batch loss:  tensor(0.0887, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8609  batch loss:  tensor(0.1717, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8610  batch loss:  tensor(0.0551, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8611  batch loss:  tensor(0.1292, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8612  batch loss:  tensor(0.1043, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8613  batch loss:  tensor(0.1175, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8614  batch loss:  tensor(0.1206, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8615  batch loss:  tensor(0.0421, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8616  batch loss:  tensor(0.0717, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8617  batch loss:  tensor(0.1096, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8618  batch loss:  tensor(0.0627, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8619  batch loss:  tensor(0.0550, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8620  batch loss:  tensor(0.0755, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8621  batch loss:  tensor(0.0852, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8622  batch loss:  tensor(0.1130, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8623  batch loss:  tensor(0.1628, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8624  batch loss:  tensor(0.1038, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8625  batch loss:  tensor(0.0601, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8626  batch loss:  tensor(0.1453, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8627  batch loss:  tensor(0.0858, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8628  batch loss:  tensor(0.0553, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8629  batch loss:  tensor(0.0935, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8630  batch loss:  tensor(0.0694, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8631  batch loss:  tensor(0.1589, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8632  batch loss:  tensor(0.0732, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8633  batch loss:  tensor(0.2299, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8634  batch loss:  tensor(0.1115, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8635  batch loss:  tensor(0.0562, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8636  batch loss:  tensor(0.0520, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8637  batch loss:  tensor(0.1289, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8638  batch loss:  tensor(0.1046, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8639  batch loss:  tensor(0.0663, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8640  batch loss:  tensor(0.0669, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8641  batch loss:  tensor(0.0790, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8642  batch loss:  tensor(0.0528, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8643  batch loss:  tensor(0.0979, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8644  batch loss:  tensor(0.1968, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8645  batch loss:  tensor(0.0950, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8646  batch loss:  tensor(0.0872, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8647  batch loss:  tensor(0.1470, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8648  batch loss:  tensor(0.0806, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8649  batch loss:  tensor(0.0531, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8650  batch loss:  tensor(0.1483, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8651  batch loss:  tensor(0.0682, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8652  batch loss:  tensor(0.0452, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8653  batch loss:  tensor(0.1099, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8654  batch loss:  tensor(0.0760, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8655  batch loss:  tensor(0.1000, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8656  batch loss:  tensor(0.0957, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8657  batch loss:  tensor(0.1094, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8658  batch loss:  tensor(0.1673, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8659  batch loss:  tensor(0.2264, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8660  batch loss:  tensor(0.0962, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8661  batch loss:  tensor(0.1125, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8662  batch loss:  tensor(0.0675, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8663  batch loss:  tensor(0.0755, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8664  batch loss:  tensor(0.1413, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8665  batch loss:  tensor(0.0802, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8666  batch loss:  tensor(0.0468, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8667  batch loss:  tensor(0.1625, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8668  batch loss:  tensor(0.0780, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8669  batch loss:  tensor(0.0593, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8670  batch loss:  tensor(0.1349, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8671  batch loss:  tensor(0.3277, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8672  batch loss:  tensor(0.1399, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8673  batch loss:  tensor(0.0730, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8674  batch loss:  tensor(0.0880, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8675  batch loss:  tensor(0.0484, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8676  batch loss:  tensor(0.0480, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8677  batch loss:  tensor(0.1137, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8678  batch loss:  tensor(0.0663, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8679  batch loss:  tensor(0.0566, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8680  batch loss:  tensor(0.2503, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8681  batch loss:  tensor(0.0623, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8682  batch loss:  tensor(0.1305, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8683  batch loss:  tensor(0.0563, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8684  batch loss:  tensor(0.0460, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8685  batch loss:  tensor(0.0948, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8686  batch loss:  tensor(0.0975, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8687  batch loss:  tensor(0.1068, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8688  batch loss:  tensor(0.1237, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8689  batch loss:  tensor(0.1304, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8690  batch loss:  tensor(0.1201, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8691  batch loss:  tensor(0.0548, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8692  batch loss:  tensor(0.1112, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8693  batch loss:  tensor(0.1548, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8694  batch loss:  tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8695  batch loss:  tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8696  batch loss:  tensor(0.0597, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8697  batch loss:  tensor(0.1237, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8698  batch loss:  tensor(0.0844, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8699  batch loss:  tensor(0.1104, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8700  batch loss:  tensor(0.1075, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8701  batch loss:  tensor(0.0911, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8702  batch loss:  tensor(0.0599, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8703  batch loss:  tensor(0.2434, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8704  batch loss:  tensor(0.0772, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8705  batch loss:  tensor(0.1561, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8706  batch loss:  tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8707  batch loss:  tensor(0.1622, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8708  batch loss:  tensor(0.1071, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8709  batch loss:  tensor(0.0555, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8710  batch loss:  tensor(0.1041, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8711  batch loss:  tensor(0.0705, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8712  batch loss:  tensor(0.0721, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8713  batch loss:  tensor(0.0701, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8714  batch loss:  tensor(0.0852, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8715  batch loss:  tensor(0.0709, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8716  batch loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8717  batch loss:  tensor(0.0354, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8718  batch loss:  tensor(0.1180, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8719  batch loss:  tensor(0.0671, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8720  batch loss:  tensor(0.0624, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8721  batch loss:  tensor(0.1456, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8722  batch loss:  tensor(0.0787, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8723  batch loss:  tensor(0.0962, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8724  batch loss:  tensor(0.0764, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8725  batch loss:  tensor(0.0945, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8726  batch loss:  tensor(0.1792, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8727  batch loss:  tensor(0.0683, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8728  batch loss:  tensor(0.0526, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8729  batch loss:  tensor(0.0885, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8730  batch loss:  tensor(0.0954, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8731  batch loss:  tensor(0.0939, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8732  batch loss:  tensor(0.0441, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8733  batch loss:  tensor(0.1625, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8734  batch loss:  tensor(0.0848, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8735  batch loss:  tensor(0.0911, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8736  batch loss:  tensor(0.1395, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8737  batch loss:  tensor(0.0880, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8738  batch loss:  tensor(0.1706, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8739  batch loss:  tensor(0.2384, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8740  batch loss:  tensor(0.1103, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8741  batch loss:  tensor(0.1041, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8742  batch loss:  tensor(0.0578, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8743  batch loss:  tensor(0.0556, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8744  batch loss:  tensor(0.0511, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8745  batch loss:  tensor(0.0518, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8746  batch loss:  tensor(0.0952, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8747  batch loss:  tensor(0.0759, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8748  batch loss:  tensor(0.2967, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8749  batch loss:  tensor(0.1019, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8750  batch loss:  tensor(0.1073, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8751  batch loss:  tensor(0.0959, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8752  batch loss:  tensor(0.1246, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8753  batch loss:  tensor(0.0687, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8754  batch loss:  tensor(0.0692, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8755  batch loss:  tensor(0.1011, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8756  batch loss:  tensor(0.0998, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8757  batch loss:  tensor(0.0826, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8758  batch loss:  tensor(0.1224, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8759  batch loss:  tensor(0.0843, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8760  batch loss:  tensor(0.1129, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8761  batch loss:  tensor(0.0733, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8762  batch loss:  tensor(0.0647, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8763  batch loss:  tensor(0.0761, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8764  batch loss:  tensor(0.3394, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8765  batch loss:  tensor(0.0685, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8766  batch loss:  tensor(0.0720, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8767  batch loss:  tensor(0.2388, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8768  batch loss:  tensor(0.0950, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8769  batch loss:  tensor(0.0725, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8770  batch loss:  tensor(0.0545, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8771  batch loss:  tensor(0.1659, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8772  batch loss:  tensor(0.0963, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8773  batch loss:  tensor(0.1033, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8774  batch loss:  tensor(0.0917, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8775  batch loss:  tensor(0.1204, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8776  batch loss:  tensor(0.0435, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8777  batch loss:  tensor(0.1680, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8778  batch loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8779  batch loss:  tensor(0.0745, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8780  batch loss:  tensor(0.1061, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8781  batch loss:  tensor(0.1006, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8782  batch loss:  tensor(0.0960, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8783  batch loss:  tensor(0.0850, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8784  batch loss:  tensor(0.0883, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8785  batch loss:  tensor(0.0831, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8786  batch loss:  tensor(0.1280, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8787  batch loss:  tensor(0.0619, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8788  batch loss:  tensor(0.1686, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8789  batch loss:  tensor(0.0442, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8790  batch loss:  tensor(0.0987, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8791  batch loss:  tensor(0.0794, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8792  batch loss:  tensor(0.0671, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8793  batch loss:  tensor(0.0773, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8794  batch loss:  tensor(0.0895, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8795  batch loss:  tensor(0.1365, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8796  batch loss:  tensor(0.0916, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8797  batch loss:  tensor(0.0840, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8798  batch loss:  tensor(0.0822, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8799  batch loss:  tensor(0.0346, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8800  batch loss:  tensor(0.1252, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8801  batch loss:  tensor(0.0569, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8802  batch loss:  tensor(0.1431, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8803  batch loss:  tensor(0.1187, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8804  batch loss:  tensor(0.0758, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8805  batch loss:  tensor(0.0334, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8806  batch loss:  tensor(0.0567, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8807  batch loss:  tensor(0.0812, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8808  batch loss:  tensor(0.0588, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8809  batch loss:  tensor(0.0912, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8810  batch loss:  tensor(0.0789, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8811  batch loss:  tensor(0.0955, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8812  batch loss:  tensor(0.0648, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8813  batch loss:  tensor(0.0768, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8814  batch loss:  tensor(0.0873, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8815  batch loss:  tensor(0.1056, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8816  batch loss:  tensor(0.0942, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8817  batch loss:  tensor(0.1529, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8818  batch loss:  tensor(0.0878, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8819  batch loss:  tensor(0.0692, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8820  batch loss:  tensor(0.0676, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8821  batch loss:  tensor(0.1347, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8822  batch loss:  tensor(0.0527, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8823  batch loss:  tensor(0.0772, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8824  batch loss:  tensor(0.1000, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8825  batch loss:  tensor(0.0839, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8826  batch loss:  tensor(0.0601, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8827  batch loss:  tensor(0.0904, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8828  batch loss:  tensor(0.0529, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8829  batch loss:  tensor(0.1249, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8830  batch loss:  tensor(0.0607, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8831  batch loss:  tensor(0.1912, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8832  batch loss:  tensor(0.0525, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8833  batch loss:  tensor(0.1027, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8834  batch loss:  tensor(0.0506, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8835  batch loss:  tensor(0.0872, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8836  batch loss:  tensor(0.0810, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8837  batch loss:  tensor(0.1195, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8838  batch loss:  tensor(0.0927, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8839  batch loss:  tensor(0.0927, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8840  batch loss:  tensor(0.0638, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8841  batch loss:  tensor(0.1007, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8842  batch loss:  tensor(0.1227, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8843  batch loss:  tensor(0.0626, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8844  batch loss:  tensor(0.0867, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8845  batch loss:  tensor(0.0499, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8846  batch loss:  tensor(0.0906, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8847  batch loss:  tensor(0.1338, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8848  batch loss:  tensor(0.1052, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8849  batch loss:  tensor(0.0560, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8850  batch loss:  tensor(0.0906, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8851  batch loss:  tensor(0.0972, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8852  batch loss:  tensor(0.1024, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8853  batch loss:  tensor(0.2120, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8854  batch loss:  tensor(0.0716, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8855  batch loss:  tensor(0.0773, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8856  batch loss:  tensor(0.1483, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8857  batch loss:  tensor(0.1159, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8858  batch loss:  tensor(0.1194, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8859  batch loss:  tensor(0.0619, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8860  batch loss:  tensor(0.0720, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8861  batch loss:  tensor(0.0735, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8862  batch loss:  tensor(0.0868, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8863  batch loss:  tensor(0.0562, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8864  batch loss:  tensor(0.0676, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8865  batch loss:  tensor(0.0797, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8866  batch loss:  tensor(0.0464, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8867  batch loss:  tensor(0.2092, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8868  batch loss:  tensor(0.0770, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8869  batch loss:  tensor(0.0569, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8870  batch loss:  tensor(0.0746, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8871  batch loss:  tensor(0.1190, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8872  batch loss:  tensor(0.0694, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8873  batch loss:  tensor(0.1146, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8874  batch loss:  tensor(0.1286, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8875  batch loss:  tensor(0.1252, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8876  batch loss:  tensor(0.2109, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8877  batch loss:  tensor(0.0531, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8878  batch loss:  tensor(0.0503, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8879  batch loss:  tensor(0.1391, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8880  batch loss:  tensor(0.0569, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8881  batch loss:  tensor(0.1474, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8882  batch loss:  tensor(0.0495, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8883  batch loss:  tensor(0.1068, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8884  batch loss:  tensor(0.0845, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8885  batch loss:  tensor(0.0486, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8886  batch loss:  tensor(0.0629, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8887  batch loss:  tensor(0.1112, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8888  batch loss:  tensor(0.1339, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8889  batch loss:  tensor(0.1228, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8890  batch loss:  tensor(0.0734, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8891  batch loss:  tensor(0.0697, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8892  batch loss:  tensor(0.0506, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8893  batch loss:  tensor(0.0674, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8894  batch loss:  tensor(0.0798, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8895  batch loss:  tensor(0.0986, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8896  batch loss:  tensor(0.0477, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8897  batch loss:  tensor(0.2087, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8898  batch loss:  tensor(0.0479, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8899  batch loss:  tensor(0.0692, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8900  batch loss:  tensor(0.0826, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8901  batch loss:  tensor(0.0990, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8902  batch loss:  tensor(0.0487, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8903  batch loss:  tensor(0.1603, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8904  batch loss:  tensor(0.1511, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8905  batch loss:  tensor(0.1125, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8906  batch loss:  tensor(0.1777, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8907  batch loss:  tensor(0.0880, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8908  batch loss:  tensor(0.1197, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8909  batch loss:  tensor(0.1209, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8910  batch loss:  tensor(0.1001, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8911  batch loss:  tensor(0.0955, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8912  batch loss:  tensor(0.0742, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8913  batch loss:  tensor(0.0750, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8914  batch loss:  tensor(0.0831, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8915  batch loss:  tensor(0.1026, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8916  batch loss:  tensor(0.1557, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8917  batch loss:  tensor(0.0792, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8918  batch loss:  tensor(0.0967, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8919  batch loss:  tensor(0.1895, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8920  batch loss:  tensor(0.0921, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8921  batch loss:  tensor(0.0753, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8922  batch loss:  tensor(0.0938, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8923  batch loss:  tensor(0.0608, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8924  batch loss:  tensor(0.1261, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8925  batch loss:  tensor(0.0757, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8926  batch loss:  tensor(0.0729, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8927  batch loss:  tensor(0.0802, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8928  batch loss:  tensor(0.1563, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8929  batch loss:  tensor(0.0916, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8930  batch loss:  tensor(0.0609, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8931  batch loss:  tensor(0.2731, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8932  batch loss:  tensor(0.1618, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8933  batch loss:  tensor(0.0482, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8934  batch loss:  tensor(0.0663, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8935  batch loss:  tensor(0.1611, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8936  batch loss:  tensor(0.1459, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8937  batch loss:  tensor(0.0665, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8938  batch loss:  tensor(0.0698, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8939  batch loss:  tensor(0.1514, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8940  batch loss:  tensor(0.0371, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8941  batch loss:  tensor(0.0863, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8942  batch loss:  tensor(0.0638, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8943  batch loss:  tensor(0.1202, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8944  batch loss:  tensor(0.0668, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8945  batch loss:  tensor(0.0977, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8946  batch loss:  tensor(0.0541, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8947  batch loss:  tensor(0.1658, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8948  batch loss:  tensor(0.0964, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8949  batch loss:  tensor(0.1289, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8950  batch loss:  tensor(0.1088, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8951  batch loss:  tensor(0.0702, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8952  batch loss:  tensor(0.0906, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8953  batch loss:  tensor(0.1010, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8954  batch loss:  tensor(0.1387, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8955  batch loss:  tensor(0.1082, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8956  batch loss:  tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8957  batch loss:  tensor(0.0997, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8958  batch loss:  tensor(0.0801, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8959  batch loss:  tensor(0.1016, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8960  batch loss:  tensor(0.0728, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8961  batch loss:  tensor(0.0821, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8962  batch loss:  tensor(0.0927, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8963  batch loss:  tensor(0.0694, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8964  batch loss:  tensor(0.0948, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8965  batch loss:  tensor(0.0712, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8966  batch loss:  tensor(0.1020, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8967  batch loss:  tensor(0.0653, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8968  batch loss:  tensor(0.0867, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8969  batch loss:  tensor(0.0744, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8970  batch loss:  tensor(0.1262, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8971  batch loss:  tensor(0.1011, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8972  batch loss:  tensor(0.0568, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8973  batch loss:  tensor(0.0978, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8974  batch loss:  tensor(0.0520, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8975  batch loss:  tensor(0.1402, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8976  batch loss:  tensor(0.1620, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8977  batch loss:  tensor(0.1992, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8978  batch loss:  tensor(0.1491, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8979  batch loss:  tensor(0.0745, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8980  batch loss:  tensor(0.1031, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8981  batch loss:  tensor(0.0472, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8982  batch loss:  tensor(0.0668, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8983  batch loss:  tensor(0.0653, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8984  batch loss:  tensor(0.1252, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8985  batch loss:  tensor(0.1112, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8986  batch loss:  tensor(0.0715, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8987  batch loss:  tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8988  batch loss:  tensor(0.2715, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8989  batch loss:  tensor(0.0764, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8990  batch loss:  tensor(0.0932, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8991  batch loss:  tensor(0.1286, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8992  batch loss:  tensor(0.1966, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8993  batch loss:  tensor(0.1956, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8994  batch loss:  tensor(0.0728, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8995  batch loss:  tensor(0.1043, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8996  batch loss:  tensor(0.1037, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8997  batch loss:  tensor(0.0832, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8998  batch loss:  tensor(0.0653, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  8999  batch loss:  tensor(0.1073, grad_fn=<MseLossBackward0>)\n",
            "epoch:  0 of batch:  9000  batch loss:  tensor(0.1222, grad_fn=<MseLossBackward0>)\n",
            "The test Loss is:  0.0986980769969523\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform2 = transforms.Compose([transforms.Resize(3), transforms.CenterCrop(224) ])\n",
        "class Fc8_Decoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    self.learning_rate = 0.03\n",
        "    self.test_loss_epoch = []\n",
        "    self.train_loss_epoch = []\n",
        "    super(Fc8_Decoder, self).__init__()\n",
        "    # self.AlexNet_FC8 = AlexNet.classifier[6]\n",
        "    self.AlexNet_conv = nn.ModuleList(list(AlexNet.features)[:]).eval()\n",
        "    self.avg_pool =  AlexNet.avgpool.eval()\n",
        "    self.AlexNet_FC8 = nn.ModuleList(list(AlexNet.classifier)[:]).eval()\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=(1000), out_features=4096)\n",
        "    self.fc2 = nn.Linear(in_features=(4096), out_features=4096)\n",
        "    self.fc3 = nn.Linear(in_features=(4096), out_features=4096)\n",
        "\n",
        "    self.upconv1 = nn.ConvTranspose2d(in_channels=256, out_channels=256, kernel_size=5, stride=2 ,padding=0)\n",
        "    self.upconv2 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=5, stride=2 ,padding=0)\n",
        "    self.upconv3 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=5, stride=2 ,padding=0)\n",
        "    self.upconv4 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=5, stride=2 ,padding=0)\n",
        "    self.upconv5 = nn.ConvTranspose2d(in_channels=32, out_channels=3, kernel_size=5, stride=2 ,padding=0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    for i,layer in enumerate(self.AlexNet_conv):\n",
        "      x = layer(x)\n",
        "    x = self.avg_pool(x)\n",
        "    x = x.view(-1,  1 * 1 * 9216)\n",
        "    x = x.squeeze(0)\n",
        "    for i,layer in enumerate(self.AlexNet_FC8):  \n",
        "      x = layer(x)\n",
        "      if i == 6:\n",
        "        break\n",
        "\n",
        "    x = F.leaky_relu(self.fc1(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.fc2(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.fc3(x), negative_slope=0.2)\n",
        "    #reshaping 4096 --> 4x4x256\n",
        "    x = x.view(1,256, 4 ,4)\n",
        "\n",
        "    x = F.leaky_relu(self.upconv1(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.upconv2(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.upconv3(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.upconv4(x), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.upconv5(x), negative_slope=0.2)\n",
        "    x = transform2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def calculate_test_loss(self, paths):\n",
        "    self.eval()\n",
        "    loss_function = nn.MSELoss()\n",
        "    test_loss = []\n",
        "    with torch.no_grad():\n",
        "      for i in range(len(paths)):\n",
        "        image = read_image(paths[i])\n",
        "        output = self(image) \n",
        "        loss = loss_function(output, image)\n",
        "        test_loss.append(loss.item())\n",
        "    total_loss = np.mean(test_loss)\n",
        "    print(\"The test Loss is: \", total_loss)\n",
        "    return total_loss\n",
        "\n",
        "  def train_net(self, learning_rate, epochs, train_data):\n",
        "    self.train()\n",
        "    optimizer = optim.Adam(self.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
        "    loss_function = nn.MSELoss()\n",
        "    EPOCHS = epochs\n",
        "    self.test_loss_epoch.append(self.calculate_test_loss(test_imagepaths))\n",
        "    for epoch in range(EPOCHS):\n",
        "      num = 0\n",
        "      for image_path in train_data:\n",
        "        data = read_image(image_path)\n",
        "        optimizer.zero_grad()\n",
        "        output = self(data)\n",
        "        loss = loss_function(output, data)\n",
        "        self.train_loss_epoch.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        num += 1\n",
        "        print(\"epoch: \", epoch, \"of batch: \", num, \" batch loss: \", loss)\n",
        "      self.test_loss_epoch.append(self.calculate_test_loss(test_imagepaths))\n",
        "  \n",
        "  def test_model(self, image_path):\n",
        "    self.eval()\n",
        "    with torch.no_grad():\n",
        "      image = read_image(image_path)\n",
        "      reconstructed_image = self.forward(image)\n",
        "      return reconstructed_image\n",
        "\n",
        "  def plot_results(self):\n",
        "    plt.plot(np.arange(1,len(self.train_loss_epoch)+1), self.train_loss_epoch)\n",
        "    plt.ylabel('loss of Train')\n",
        "    plt.xlabel(\"number of steps\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    \n",
        "    plt.plot(np.arange(1,len(self.test_loss_epoch)+1), self.test_loss_epoch)\n",
        "    plt.ylabel('loss of test')\n",
        "    plt.xlabel(\"number of epochs\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "fc8_decoder = Fc8_Decoder()\n",
        "fc8_decoder.train_net(0.001, 1, train_imagepaths)\n",
        "fc8_decoder.plot_results()\n",
        "torch.save(fc8_decoder.state_dict(), '/content/drive/My Drive/DL/HW2/fc8_decoder.pt')"
      ],
      "metadata": {
        "id": "uQi8T68jzgyG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}