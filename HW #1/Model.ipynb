{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUUFUOGmy_CS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Normalization(df):\n",
        "  df_std = df.copy()\n",
        "  for column in df_std.columns:\n",
        "      df_std[column] = (df_std[column] - df_std[column].mean()) / df_std[column].std()\n",
        "  return df_std\n",
        "\n",
        "def Relu(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def Sigmoid_Derivative(x):\n",
        "  return sigmoid(x) * (1 - sigmoid(x))\n",
        "    \n",
        "def Softmax(X):\n",
        "  exps = np.exp(X - np.max(X))\n",
        "  return exps / np.sum(exps, axis=0)\n",
        "    \n",
        "def Y_one_hot(Y, len_last_layer):\n",
        "  one_hot = np.zeros((len(Y), len_last_layer))\n",
        "  one_hot[np.arange(Y.size), Y] = 1\n",
        "  return one_hot.T"
      ],
      "metadata": {
        "id": "L2D7MxW0zcEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_params(layers, std):\n",
        "  params = {}\n",
        "  Delta = {}\n",
        "  for i in range(1, len(layers)):\n",
        "      mu = 0\n",
        "      params['W' + str(i)] = np.random.normal(mu, std, [layers[i], layers[i-1]])\n",
        "      params['B' + str(i)] = np.random.normal(mu, std, [layers[i], 1]) * 0\n",
        "      \n",
        "      Delta['W' + str(i)] = np.zeros([layers[i], layers[i-1]])\n",
        "      Delta['B' + str(i)] = np.zeros([layers[i], 1])\n",
        "  return params, Delta"
      ],
      "metadata": {
        "id": "rQG58z9HzZtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(X, params, Gaussian_RBF):\n",
        "  Number_of_layers = len(params)//2\n",
        "  values = {}\n",
        "  for i in range(1, Number_of_layers + 1):\n",
        "      # S is the Score = W * X + B\n",
        "      # A is the output of layer, A = f(S)\n",
        "      if i==1:                                                                      # Check is it firt layers or not          \n",
        "          values['S' + str(i)] = np.dot(params['W' + str(i)], X) + params['B' + str(i)]\n",
        "          if Gaussian_RBF:\n",
        "              values['A' + str(i)] = sigmoid(values['S' + str(i)])\n",
        "          else:\n",
        "              values['A' + str(i)] = Relu(values['S' + str(i)])\n",
        "          \n",
        "      else:\n",
        "          values['S' + str(i)] = np.dot(params['W' + str(i)], values['A' + str(i-1)]) + params['B' + str(i)]\n",
        "          if i == Number_of_layers:                                                 # Check is it last layers or not   \n",
        "              if Gaussian_RBF:\n",
        "                  values['A' + str(i)] = sigmoid(values['S' + str(i)])\n",
        "              else:\n",
        "                  values['A' + str(i)] = values['S' + str(i)]\n",
        "          else:\n",
        "              if Gaussian_RBF:\n",
        "                  values['A' + str(i)] = sigmoid(values['S' + str(i)])\n",
        "              else:\n",
        "                  values['A' + str(i)] = Relu(values['S' + str(i)])\n",
        "  return values\n",
        "\n",
        "                                                                                   #dim = (100,)\n"
      ],
      "metadata": {
        "id": "4WkvHMWKzWUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Compute_Loss(values, Y, params, Gaussian_RBF, len_last_layer):\n",
        "  Number_of_layers = len(params)//2\n",
        "  if Gaussian_RBF:\n",
        "      y_hat = values['A' + str(Number_of_layers)]\n",
        "      y_true_ohe_hot = Y_one_hot(Y, len_last_layer)\n",
        "      loss = np.apply_along_axis(np.linalg.norm, 0, y_hat - y_true_ohe_hot)\n",
        "      loss = np.power(loss, 2)\n",
        "  else:\n",
        "      Score_last_layer = values['A' + str(Number_of_layers)]                                         #dim = (7,100)\n",
        "      loss = np.log(np.sum(np.exp(Score_last_layer), axis=0)) - Score_last_layer[Y, range(len(Y))]\n",
        "\n",
        "  return loss     "
      ],
      "metadata": {
        "id": "ZDo70wDEzQGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation(params, values, X, Y, len_last_layer, Gaussian_RBF):\n",
        "  Number_of_layers = len(params)//2\n",
        "  m = len(Y)                                                                    #Number_of_samples_in_this_batch\n",
        "  grads = {}\n",
        "  for i in range(Number_of_layers, 0, -1):\n",
        "      if i == Number_of_layers:                                                 #Check is it last layer or not\n",
        "          if Gaussian_RBF:\n",
        "              y_hat = values['A' + str(Number_of_layers)]\n",
        "              y_true_ohe_hot = Y_one_hot(Y, len_last_layer)\n",
        "              Score_last_layer = values['S' + str(Number_of_layers)] \n",
        "              dS = np.multiply(2 * (y_hat - y_true_ohe_hot), Sigmoid_Derivative(Score_last_layer))\n",
        "          else:\n",
        "              Score_last_layer = values['A' + str(Number_of_layers)]            #(7,100)\n",
        "              dS = Softmax(Score_last_layer) - Y_one_hot(Y, len_last_layer)     #(7,100) = (7,100) - (7, 100)\n",
        "      else:\n",
        "          dA = np.dot(params['W' + str(i+1)].T, dS)\n",
        "          if Gaussian_RBF:\n",
        "              dS = np.multiply(dA, Sigmoid_Derivative(values['A' + str(i)]))    #Sigmoid Derivative\n",
        "          else:\n",
        "              dS = np.multiply(dA, np.where(values['A' + str(i)]>=0, 1, 0))     #Relu Derivative\n",
        "                      \n",
        "      if i == 1:\n",
        "          grads['W' + str(i)] = 1/m * np.dot(dS, X.T)\n",
        "          grads['B' + str(i)] = 1/m * np.sum(dS, axis=1, keepdims=True)\n",
        "      else:\n",
        "          grads['W' + str(i)] = 1/m * np.dot(dS,values['A' + str(i-1)].T)\n",
        "          grads['B' + str(i)] = 1/m * np.sum(dS, axis=1, keepdims=True)\n",
        "  return grads\n",
        "\n",
        "def update_params(params, grads, learning_rate, alfa_momentum, Delta):\n",
        "  Number_of_layers = len(params)//2\n",
        "  params_updated = {}\n",
        "  for i in range(1, Number_of_layers + 1):\n",
        "      Delta['W' + str(i)] = Delta['W' + str(i)] * alfa_momentum - learning_rate * grads['W' + str(i)]   \n",
        "      Delta['B' + str(i)] = Delta['B' + str(i)] * alfa_momentum - learning_rate * grads['B' + str(i)]\n",
        "      params_updated['W' + str(i)] = params['W' + str(i)] + Delta['W' + str(i)]\n",
        "      params_updated['B' + str(i)] = params['B' + str(i)] + Delta['B' + str(i)]\n",
        "  return params_updated, Delta\n",
        "\n"
      ],
      "metadata": {
        "id": "4UdhI49uzObp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(_Data, params, Gaussian_RBF):\n",
        "  Number_of_layers = len(params)//2\n",
        "  values = forward_propagation(_Data.T, params, Gaussian_RBF)\n",
        "  predictions = values['A' + str(Number_of_layers)]\n",
        "  y_pred = np.argmax(predictions, axis=0)\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "4cm4yLlGzJMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, layers, Number_of_Epochs, learning_rate, std, alfa_momentum, Batch_size, Gaussian_RBF = False):\n",
        "  params, Delta = initialize_params(layers, std)\n",
        "  _Loss_test = []\n",
        "  _Loss_train = []\n",
        "  _acc_test = []\n",
        "  _acc_train = []\n",
        "  for i in range(Number_of_Epochs):\n",
        "      Each_Epoch_Loss = np.array([])\n",
        "      for j in range(int(len(X_train) / Batch_size) + 1):\n",
        "          X = X_train[j * Batch_size: (j+1)*Batch_size]\n",
        "          Y = Y_train[j * Batch_size: (j+1)*Batch_size]\n",
        "          values = forward_propagation(X.T, params, Gaussian_RBF)\n",
        "          Loss_of_one_batch = Compute_Loss(values, Y.T, params, Gaussian_RBF, layers[-1])\n",
        "          Each_Epoch_Loss = np.append(Each_Epoch_Loss, Loss_of_one_batch)\n",
        "          grads = backward_propagation(params, values, X.T, Y.T, layers[-1], Gaussian_RBF)\n",
        "          params, Delta = update_params(params, grads, learning_rate, alfa_momentum, Delta)\n",
        "      \n",
        "      _Loss_train.append(np.mean(Each_Epoch_Loss))\n",
        "      values = forward_propagation(X_test.T, params, Gaussian_RBF)\n",
        "      _Loss_test.append(np.mean(Compute_Loss(values, Y_test.T, params, Gaussian_RBF, layers[-1])))\n",
        "      y_pred_train = predict(X_train, params, Gaussian_RBF)\n",
        "      y_pred_test = predict(X_test, params, Gaussian_RBF) \n",
        "      _acc_train.append(accuracy_score(Y_train, y_pred_train))\n",
        "      _acc_test.append(accuracy_score(Y_test, y_pred_test))     \n",
        "  return params, _Loss_train, _Loss_test, _acc_test, _acc_train, y_pred_test"
      ],
      "metadata": {
        "id": "IQJH0KJszJQv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}